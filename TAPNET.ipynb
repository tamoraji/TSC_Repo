{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Environment_name = Dl_DANET\n",
    "#Python = 3.6\n",
    "#PyTorch version 1.8.2+cu111\n",
    "#einops\n",
    "#seaborn\n",
    "#sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 1.8.1\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages\n",
      "Requires: numpy, typing-extensions\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from TAPNet.models import TapNet\n",
    "from TAPNet.utils import *\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "import sklearn.preprocessing\n",
    "\n",
    "import time\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from joblib import parallel_backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# dataset settings\n",
    "parser.add_argument('--data_path', type=str, default=\"../datasets\",\n",
    "                    help='the path of data.')\n",
    "# parser.add_argument('--dataset', type=str, default=\"NATOPS\", #NATOPS\n",
    "#                     help='time series dataset. Options: See the datasets list')\n",
    "\n",
    "# cuda settings\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "#parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
    "\n",
    "# Training parameter settings\n",
    "parser.add_argument('--epochs', type=int, default=3000,\n",
    "                    help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=1e-5,\n",
    "                    help='Initial learning rate. default:[0.00001]')\n",
    "parser.add_argument('--wd', type=float, default=1e-3,\n",
    "                    help='Weight decay (L2 loss on parameters). default: 5e-3')\n",
    "parser.add_argument('--stop_thres', type=float, default=1e-9,\n",
    "                    help='The stop threshold for the training error. If the difference between training losses '\n",
    "                         'between epoches are less than the threshold, the training will be stopped. Default:1e-9')\n",
    "\n",
    "# Model parameters\n",
    "parser.add_argument('--use_cnn', type=boolean_string, default=True,\n",
    "                    help='whether to use CNN for feature extraction. Default:False')\n",
    "parser.add_argument('--use_lstm', type=boolean_string, default=True,\n",
    "                    help='whether to use LSTM for feature extraction. Default:False')\n",
    "parser.add_argument('--use_rp', type=boolean_string, default=True,\n",
    "                    help='Whether to use random projection')\n",
    "parser.add_argument('--rp_params', type=str, default='-1,3',\n",
    "                    help='Parameters for random projection: number of random projection, '\n",
    "                         'sub-dimension for each random projection')\n",
    "parser.add_argument('--use_metric', action='store_true', default=False,\n",
    "                    help='whether to use the metric learning for class representation. Default:False')\n",
    "parser.add_argument('--metric_param', type=float, default=0.01,\n",
    "                    help='Metric parameter for prototype distances between classes. Default:0.000001')\n",
    "parser.add_argument('--filters', type=str, default=\"256,256,128\",\n",
    "                    help='filters used for convolutional network. Default:256,256,128')\n",
    "parser.add_argument('--kernels', type=str, default=\"8,5,3\",\n",
    "                    help='kernels used for convolutional network. Default:8,5,3')\n",
    "parser.add_argument('--dilation', type=int, default=1,\n",
    "                    help='the dilation used for the first convolutional layer. '\n",
    "                         'If set to -1, use the automatic number. Default:-1')\n",
    "parser.add_argument('--layers', type=str, default=\"500,300\",\n",
    "                    help='layer settings of mapping function. [Default]: 500,300')\n",
    "parser.add_argument('--dropout', type=float, default=0,\n",
    "                    help='Dropout rate (1 - keep probability). Default:0.5')\n",
    "parser.add_argument('--lstm_dim', type=int, default=128,\n",
    "                    help='Dimension of LSTM Embedding.')\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "\tCUDA=False\n",
      "\tDATA_PATH=../datasets\n",
      "\tDILATION=1\n",
      "\tDROPOUT=0\n",
      "\tEPOCHS=3000\n",
      "\tFILTERS=[256, 256, 128]\n",
      "\tKERNELS=[8, 5, 3]\n",
      "\tLAYERS=[500, 300]\n",
      "\tLR=1e-05\n",
      "\tLSTM_DIM=128\n",
      "\tMETRIC_PARAM=0.01\n",
      "\tNO_CUDA=False\n",
      "\tRP_PARAMS=[-1.0, 3.0]\n",
      "\tSPARSE=True\n",
      "\tSTOP_THRES=1e-09\n",
      "\tUSE_CNN=True\n",
      "\tUSE_LSTM=True\n",
      "\tUSE_METRIC=False\n",
      "\tUSE_RP=True\n",
      "\tWD=0.001\n"
     ]
    }
   ],
   "source": [
    "# np.random.seed(args.seed)\n",
    "# torch.manual_seed(args.seed)\n",
    "# if args.cuda:\n",
    "#     torch.cuda.manual_seed(args.seed)\n",
    "args.sparse = True\n",
    "args.layers = [int(l) for l in args.layers.split(\",\")]\n",
    "args.kernels = [int(l) for l in args.kernels.split(\",\")]\n",
    "args.filters = [int(l) for l in args.filters.split(\",\")]\n",
    "args.rp_params = [float(l) for l in args.rp_params.split(\",\")]\n",
    "\n",
    "if not args.use_lstm and not args.use_cnn:\n",
    "    print(\"Must specify one encoding method: --use_lstm or --use_cnn\")\n",
    "    print(\"Program Exiting.\")\n",
    "    exit(-1)\n",
    "\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(args.__dict__.items()):\n",
    "    print(\"\\t{}={}\".format(attr.upper(), value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function\n",
    "def train():\n",
    "    loss_list = [sys.maxsize]\n",
    "    test_best_possible, best_so_far = 0.0, sys.maxsize\n",
    "    for epoch in range(args.epochs):\n",
    "\n",
    "        t = time.time()\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output, proto_dist = model(input)\n",
    "\n",
    "        loss_train = F.cross_entropy(output[idx_train], torch.squeeze(labels[idx_train]))\n",
    "        if args.use_metric:\n",
    "            loss_train = loss_train + args.metric_param * proto_dist\n",
    "\n",
    "        if abs(loss_train.item() - loss_list[-1]) < args.stop_thres \\\n",
    "                or loss_train.item() > loss_list[-1]:\n",
    "            break\n",
    "        else:\n",
    "            loss_list.append(loss_train.item())\n",
    "\n",
    "        acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val = F.cross_entropy(output[idx_val], torch.squeeze(labels[idx_val]))\n",
    "        acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "\n",
    "        print('Epoch: {:04d}'.format(epoch + 1),\n",
    "              'loss_train: {:.8f}'.format(loss_train.item()),\n",
    "              'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "              'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "              'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "              'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "        if acc_val.item() > test_best_possible:\n",
    "            test_best_possible = acc_val.item()\n",
    "        if best_so_far > loss_train.item():\n",
    "            best_so_far = loss_train.item()\n",
    "            test_acc = acc_val.item()\n",
    "    print(\"test_acc: \" + str(test_acc))\n",
    "    print(\"best possible: \" + str(test_best_possible))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function\n",
    "def test():\n",
    "    output, proto_dist = model(input)\n",
    "    loss_test = F.cross_entropy(output[idx_test], torch.squeeze(labels[idx_test]))\n",
    "    if args.use_metric:\n",
    "        loss_test = loss_test - args.metric_param * proto_dist\n",
    "\n",
    "    acc_test, f1 = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(acc_test)\n",
    "    \n",
    "    print(Dataset_name, \"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()),\n",
    "          \"F1-score= {:.4f}\".format(f1.item()))\n",
    "    \n",
    "    accuracy_scores.append(acc_test.item())\n",
    "    f1_scores.append(f1.item())\n",
    "\n",
    "    # save the output to a text file\n",
    "    with open(f'{results_path}/dataset_{dataset_name}_TAPNet_fold_{fold+1}.txt', 'w') as f:\n",
    "        f.write(f'Accuracy: {acc_test.item()}\\n')\n",
    "        f.write(f'F1 Score: {f1.item()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1670796275068,
     "user": {
      "displayName": "Mojtaba Askarzadeh Farahani",
      "userId": "02005001605152753874"
     },
     "user_tz": 300
    },
    "id": "nYPPgDj2e7oy",
    "outputId": "c71b2a21-7bf6-46ac-c958-794ccd9e5531"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53785, 749, 3)\n",
      "(53785, 749, 1)\n"
     ]
    }
   ],
   "source": [
    "# define a list of datasets\n",
    "datasets = [\"PHM2022_Multivar\", \"PHM2022_Univar_PDIN\"]\n",
    "datasets_path = \"../datasets\"\n",
    "\n",
    "for dataset in datasets:\n",
    "    Dataset_name = dataset + \"_Dataset\"\n",
    "    Dataset = np.load(datasets_path + \"/\" + Dataset_name + \".npy\")\n",
    "    print(Dataset.shape)\n",
    "    \n",
    "\n",
    "    Labels_name = dataset + \"_Labels\"\n",
    "    Labels = np.load(datasets_path + \"/\"  + Labels_name + \".npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset PHM2022_Multivar_Dataset ...\n",
      "\n",
      " The dataset shape is:(53785, 749, 3)\n",
      "\n",
      " The number of data samples (N) is:53785\n",
      "\n",
      " The number of TS length (T) is:749\n",
      "\n",
      " The number of TS dimention (M) is:3\n",
      "rp_params: [3, 2]\n",
      "Data shape: torch.Size([53785, 3, 749])\n",
      "Layers [512, 500, 300]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# change this directory for your machine\n",
    "root_dir = './'\n",
    "\n",
    "# define the number of folds\n",
    "n_folds = 5\n",
    "\n",
    "# perform cross-validation for each dataset and algorithm combination\n",
    "for dataset in datasets:\n",
    "    Dataset_name = dataset + \"_Dataset\"\n",
    "    Dataset = np.load(datasets_path + \"/\" + Dataset_name + \".npy\")\n",
    "    \n",
    "\n",
    "    Labels_name = dataset + \"_Labels\"\n",
    "    Labels = np.load(datasets_path + \"/\"  + Labels_name + \".npy\")\n",
    "    \n",
    "\n",
    "\n",
    "    # Create a folder for results\n",
    "    results_path = root_dir + \"Results/\" + Dataset_name\n",
    "    if os.path.exists(results_path):\n",
    "        pass\n",
    "    else:\n",
    "        try:\n",
    "            os.makedirs(results_path)\n",
    "        except:\n",
    "            # in case another machine created the path meanwhile !:(\n",
    "            pass\n",
    "\n",
    "        \n",
    "    t_total = time.time() ##Start timing\n",
    "    \n",
    "\n",
    "    print(\"Loading dataset\", Dataset_name, \"...\")\n",
    "    print(f\"\\n The dataset shape is:{Dataset.shape}\")\n",
    "    print(f\"\\n The number of data samples (N) is:{Dataset.shape[0]}\")\n",
    "    print(f\"\\n The number of TS length (T) is:{Dataset.shape[1]}\")\n",
    "    print(f\"\\n The number of TS dimention (M) is:{Dataset.shape[2]}\")\n",
    "\n",
    "    \n",
    "    # Model and optimizer\n",
    "    model_type = \"TapNet\" \n",
    "\n",
    "\n",
    "\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True)\n",
    "    accuracy_scores = []\n",
    "    f1_scores = []\n",
    "    confusion_matrices = []\n",
    "    report_list = []\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(Dataset)):\n",
    "        # split the data into training and testing sets\n",
    "        X_train, X_test = Dataset[train_idx], Dataset[test_idx]\n",
    "        y_train, y_test = Labels[train_idx], Labels[test_idx]\n",
    "\n",
    "        if model_type == \"TapNet\":\n",
    "\n",
    "            features, labels, idx_train, idx_val, idx_test, nclass = load_raw_ts(X_train,X_test,y_train,y_test)\n",
    "\n",
    "\n",
    "            # update random permutation parameter\n",
    "            if args.rp_params[0] < 0:\n",
    "                dim = features.shape[1]\n",
    "                args.rp_params = [3, math.floor(dim / (3 / 2))]\n",
    "            else:\n",
    "                dim = features.shape[1]\n",
    "                args.rp_params[1] = math.floor(dim / args.rp_params[1])\n",
    "        \n",
    "            args.rp_params = [int(l) for l in args.rp_params]\n",
    "            print(\"rp_params:\", args.rp_params)\n",
    "\n",
    "            # update dilation parameter\n",
    "            if args.dilation == -1:\n",
    "                args.dilation = math.floor(features.shape[2] / 64)\n",
    "\n",
    "            print(\"Data shape:\", features.size())\n",
    "            model = TapNet(nfeat=features.shape[1],\n",
    "                        len_ts=features.shape[2],\n",
    "                        layers=args.layers,\n",
    "                        nclass=nclass,\n",
    "                        dropout=args.dropout,\n",
    "                        use_lstm=args.use_lstm,\n",
    "                        use_cnn=args.use_cnn,\n",
    "                        filters=args.filters,\n",
    "                        dilation=args.dilation,\n",
    "                        kernels=args.kernels,\n",
    "                        use_metric=args.use_metric,\n",
    "                        use_rp=args.use_rp,\n",
    "                        rp_params=args.rp_params,\n",
    "                        lstm_dim=args.lstm_dim\n",
    "                        )\n",
    "        \n",
    "            # cuda\n",
    "            if args.cuda:\n",
    "                #model = nn.DataParallel(model) Used when you have more than one GPU. Sometimes work but not stable\n",
    "                model.cuda()\n",
    "                features, labels, idx_train = features.cuda(), labels.cuda(), idx_train.cuda()\n",
    "            input = (features, labels, idx_train, idx_val, idx_test)\n",
    "\n",
    "        # init the optimizer\n",
    "        optimizer = optim.Adam(model.parameters(),\n",
    "                            lr=args.lr, weight_decay=args.wd)\n",
    "        \n",
    "        train()\n",
    "        print(\"Optimization Finished!\")\n",
    "        # Testing\n",
    "        test()\n",
    "            \n",
    "\n",
    "        \n",
    "    with open(f'{results_path}/dataset_{dataset_name}_TAPNet.txt', 'w') as f:\n",
    "        f.write(\"Mean accuracy: {:.3f} (std={:.3f})\\n\".format(np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "        f.write(\"Mean F1 score: {:.3f} (std={:.3f})\\n\".format(np.mean(f1_scores), np.std(f1_scores)))\n",
    "        f.write(\"Mean confusion matrix:\\n{}\\n\".format(np.array2string(np.mean(confusion_matrices, axis=0))))\n",
    "        f.write(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "    print(\" Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1670797391439,
     "user": {
      "displayName": "Mojtaba Askarzadeh Farahani",
      "userId": "02005001605152753874"
     },
     "user_tz": 300
    },
    "id": "o0hfsmobe7o6"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, duration, y_true_val=None, y_pred_val=None):\n",
    "    res = pd.DataFrame(data=np.zeros((1, 4), dtype=np.float), index=[0],\n",
    "                       columns=['precision', 'accuracy', 'recall', 'duration'])\n",
    "    res['precision'] = precision_score(y_true, y_pred, average='macro')\n",
    "    res['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    if not y_true_val is None:\n",
    "        # this is useful when transfer learning is used with cross validation\n",
    "        res['accuracy_val'] = accuracy_score(y_true_val, y_pred_val)\n",
    "\n",
    "    res['recall'] = recall_score(y_true, y_pred, average='macro')\n",
    "    res['duration'] = duration\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = calculate_metrics(y_true, y_pred, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 99
    },
    "executionInfo": {
     "elapsed": 1221,
     "status": "ok",
     "timestamp": 1670797392656,
     "user": {
      "displayName": "Mojtaba Askarzadeh Farahani",
      "userId": "02005001605152753874"
     },
     "user_tz": 300
    },
    "id": "BIeCbOs6e7o6",
    "outputId": "973650ee-67f0-43ae-cf19-3b4bf69641fc"
   },
   "outputs": [],
   "source": [
    "classifier.predict(X_test, y_true,X_train,y_train,y_test,return_df_metrics = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "f = h5py.File('./Results/PHM2022_Multivar_Datasetbest_model.hdf5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
