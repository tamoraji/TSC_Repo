{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10ee3c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyts\n",
    "from pyts.metrics import dtw\n",
    "import sklearn\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress FutureWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4a9ae28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 60, 1)\n",
      "(600,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "datasets = [\n",
    "\"Control_charts\",\n",
    "]\n",
    "\n",
    "\n",
    "datasets_path = \"../datasets\"\n",
    "\n",
    "for dataset in datasets:\n",
    "    Dataset_name = dataset + \"_Dataset\"\n",
    "    Dataset = np.load(datasets_path + \"/\" + Dataset_name + \".npy\")\n",
    "    Dataset = Dataset\n",
    "    print(Dataset.shape)\n",
    "    \n",
    "    Labels_name = dataset + \"_Labels\"\n",
    "    Labels = np.load(datasets_path + \"/\"  + Labels_name + \".npy\")\n",
    "    Labels = Labels.squeeze()\n",
    "    print(Labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79c46c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyts.classification import LearningShapelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37a85b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 60)\n",
      "(600,)\n"
     ]
    }
   ],
   "source": [
    "Dataset = Dataset[:,:,0]\n",
    "print(Dataset.shape)\n",
    "Labels = Labels.reshape((-1,))\n",
    "print(Labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "131d4c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((480, 60), (120, 60), (480,), (120,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Generate the initial training and test sets to feed to the model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Dataset, Labels, train_size=0.8 ,shuffle=True)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b0e5bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Step 1: Define the parameter grid\n",
    "param_grid = {'n_shapelets_per_size': [0.1, 0.2], 'min_shapelet_length': [0.05, 0.1], 'C':[10,100,1000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b17a795",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LearningShapelets(verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e326ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(classifier, param_grid,n_jobs=10, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdbaa64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss = 0.686118\n",
      "Iteration 1: loss = 0.473086\n",
      "Iteration 2: loss = 0.468276\n",
      "Iteration 3: loss = 0.467645\n",
      "Iteration 4: loss = 0.467286\n",
      "Iteration 0: loss = 0.666840\n",
      "Iteration 1: loss = 0.435603\n",
      "Iteration 2: loss = 0.423614\n",
      "Iteration 3: loss = 0.421161\n",
      "Iteration 4: loss = 0.420341\n",
      "Iteration 5: loss = 0.419898\n",
      "Iteration 6: loss = 0.419565\n",
      "Iteration 0: loss = 0.716987\n",
      "Iteration 1: loss = 0.442911\n",
      "Iteration 2: loss = 0.427230\n",
      "Iteration 3: loss = 0.423830\n",
      "Iteration 4: loss = 0.422688\n",
      "Iteration 5: loss = 0.422102\n",
      "Iteration 6: loss = 0.421674\n",
      "Iteration 7: loss = 0.421295\n",
      "Iteration 0: loss = 0.694746\n",
      "Iteration 1: loss = 0.448969\n",
      "Iteration 2: loss = 0.411177\n",
      "Iteration 3: loss = 0.402683\n",
      "Iteration 4: loss = 0.399241\n",
      "Iteration 5: loss = 0.396083\n",
      "Iteration 6: loss = 0.393021\n",
      "Iteration 7: loss = 0.390052\n",
      "Iteration 8: loss = 0.387174\n",
      "Iteration 9: loss = 0.384383\n",
      "Iteration 10: loss = 0.381679\n",
      "Iteration 11: loss = 0.379058\n",
      "Iteration 12: loss = 0.376518\n",
      "Iteration 13: loss = 0.374057\n",
      "Iteration 14: loss = 0.371671\n",
      "Iteration 15: loss = 0.369358\n",
      "Iteration 16: loss = 0.367116\n",
      "Iteration 17: loss = 0.364941\n",
      "Iteration 18: loss = 0.362831\n",
      "Iteration 19: loss = 0.360784\n",
      "Iteration 20: loss = 0.358797\n",
      "Iteration 21: loss = 0.356867\n",
      "Iteration 22: loss = 0.354993\n",
      "Iteration 23: loss = 0.353171\n",
      "Iteration 24: loss = 0.351399\n",
      "Iteration 25: loss = 0.349676\n",
      "Iteration 26: loss = 0.348000\n",
      "Iteration 27: loss = 0.346368\n",
      "Iteration 28: loss = 0.344778\n",
      "Iteration 29: loss = 0.343230\n",
      "Iteration 30: loss = 0.341720\n",
      "Iteration 31: loss = 0.340249\n",
      "Iteration 32: loss = 0.338813\n",
      "Iteration 33: loss = 0.337412\n",
      "Iteration 34: loss = 0.336044\n",
      "Iteration 35: loss = 0.334709\n",
      "Iteration 36: loss = 0.333404\n",
      "Iteration 37: loss = 0.332129\n",
      "Iteration 38: loss = 0.330882\n",
      "Iteration 39: loss = 0.329664\n",
      "Iteration 40: loss = 0.328472\n",
      "Iteration 41: loss = 0.327305\n",
      "Iteration 42: loss = 0.326164\n",
      "Iteration 43: loss = 0.325046\n",
      "Iteration 44: loss = 0.323952\n",
      "Iteration 45: loss = 0.322880\n",
      "Iteration 46: loss = 0.321830\n",
      "Iteration 47: loss = 0.320801\n",
      "Iteration 48: loss = 0.319792\n",
      "Iteration 49: loss = 0.318803\n",
      "Iteration 50: loss = 0.317833\n",
      "Iteration 51: loss = 0.316882\n",
      "Iteration 52: loss = 0.315949\n",
      "Iteration 53: loss = 0.315034\n",
      "Iteration 54: loss = 0.314135\n",
      "Iteration 55: loss = 0.313253\n",
      "Iteration 56: loss = 0.312387\n",
      "Iteration 57: loss = 0.311537\n",
      "Iteration 58: loss = 0.310702\n",
      "Iteration 59: loss = 0.309882\n",
      "Iteration 60: loss = 0.309076\n",
      "Iteration 61: loss = 0.308284\n",
      "Iteration 62: loss = 0.307506\n",
      "Iteration 63: loss = 0.306741\n",
      "Iteration 64: loss = 0.305989\n",
      "Iteration 65: loss = 0.305250\n",
      "Iteration 66: loss = 0.304522\n",
      "Iteration 67: loss = 0.303807\n",
      "Iteration 68: loss = 0.303104\n",
      "Iteration 69: loss = 0.302412\n",
      "Iteration 70: loss = 0.301731\n",
      "Iteration 71: loss = 0.301061\n",
      "Iteration 72: loss = 0.300402\n",
      "Iteration 73: loss = 0.299753\n",
      "Iteration 74: loss = 0.299114\n",
      "Iteration 75: loss = 0.298485\n",
      "Iteration 76: loss = 0.297866\n",
      "Iteration 77: loss = 0.297256\n",
      "Iteration 78: loss = 0.296655\n",
      "Iteration 79: loss = 0.296063\n",
      "Iteration 80: loss = 0.295480\n",
      "Iteration 81: loss = 0.294906\n",
      "Iteration 82: loss = 0.294340\n",
      "Iteration 83: loss = 0.293782\n",
      "Iteration 84: loss = 0.293233\n",
      "Iteration 85: loss = 0.292691\n",
      "Iteration 86: loss = 0.292157\n",
      "Iteration 87: loss = 0.291630\n",
      "Iteration 88: loss = 0.291111\n",
      "Iteration 89: loss = 0.290599\n",
      "Iteration 90: loss = 0.290094\n",
      "Iteration 91: loss = 0.289596\n",
      "Iteration 92: loss = 0.289104\n",
      "Iteration 93: loss = 0.288620\n",
      "Iteration 94: loss = 0.288142\n",
      "Iteration 95: loss = 0.287670\n",
      "Iteration 96: loss = 0.287204\n",
      "Iteration 97: loss = 0.286745\n",
      "Iteration 98: loss = 0.286291\n",
      "Iteration 99: loss = 0.285843\n",
      "Iteration 100: loss = 0.285402\n",
      "Iteration 101: loss = 0.284965\n",
      "Iteration 102: loss = 0.284535\n",
      "Iteration 103: loss = 0.284109\n",
      "Iteration 104: loss = 0.283689\n",
      "Iteration 105: loss = 0.283275\n",
      "Iteration 106: loss = 0.282865\n",
      "Iteration 107: loss = 0.282460\n",
      "Iteration 108: loss = 0.282061\n",
      "Iteration 109: loss = 0.281666\n",
      "Iteration 110: loss = 0.281276\n",
      "Iteration 111: loss = 0.280890\n",
      "Iteration 112: loss = 0.280509\n",
      "Iteration 113: loss = 0.280133\n",
      "Iteration 114: loss = 0.279761\n",
      "Iteration 115: loss = 0.279393\n",
      "Iteration 116: loss = 0.279030\n",
      "Iteration 117: loss = 0.278671\n",
      "Iteration 118: loss = 0.278316\n",
      "Iteration 119: loss = 0.277965\n",
      "Iteration 120: loss = 0.277618\n",
      "Iteration 121: loss = 0.277274\n",
      "Iteration 122: loss = 0.276935\n",
      "Iteration 123: loss = 0.276599\n",
      "Iteration 124: loss = 0.276267\n",
      "Iteration 125: loss = 0.275939\n",
      "Iteration 126: loss = 0.275614\n",
      "Iteration 127: loss = 0.275293\n",
      "Iteration 128: loss = 0.274976\n",
      "Iteration 129: loss = 0.274661\n",
      "Iteration 130: loss = 0.274350\n",
      "Iteration 131: loss = 0.274043\n",
      "Iteration 132: loss = 0.273738\n",
      "Iteration 133: loss = 0.273437\n",
      "Iteration 134: loss = 0.273139\n",
      "Iteration 135: loss = 0.272844\n",
      "Iteration 136: loss = 0.272552\n",
      "Iteration 137: loss = 0.272263\n",
      "Iteration 138: loss = 0.271977\n",
      "Iteration 139: loss = 0.271693\n",
      "Iteration 140: loss = 0.271413\n",
      "Iteration 141: loss = 0.271136\n",
      "Iteration 142: loss = 0.270861\n",
      "Iteration 143: loss = 0.270589\n",
      "Iteration 144: loss = 0.270320\n",
      "Iteration 0: loss = 1.784960\n",
      "Iteration 1: loss = 1.675700\n",
      "Iteration 2: loss = 1.625681\n",
      "Iteration 3: loss = 1.598494\n",
      "Iteration 4: loss = 1.583029\n",
      "Iteration 5: loss = 1.573687\n",
      "Iteration 6: loss = 1.567582\n",
      "Iteration 7: loss = 1.563206\n",
      "Iteration 8: loss = 1.559760\n",
      "Iteration 9: loss = 1.556818\n",
      "Iteration 10: loss = 1.554148\n",
      "Iteration 11: loss = 1.551625\n",
      "Iteration 12: loss = 1.549178\n",
      "Iteration 13: loss = 1.546769\n",
      "Iteration 14: loss = 1.544374\n",
      "Iteration 15: loss = 1.541981\n",
      "Iteration 16: loss = 1.539580\n",
      "Iteration 17: loss = 1.537166\n",
      "Iteration 18: loss = 1.534734\n",
      "Iteration 19: loss = 1.532282\n",
      "Iteration 20: loss = 1.529807\n",
      "Iteration 21: loss = 1.527309\n",
      "Iteration 22: loss = 1.524787\n",
      "Iteration 23: loss = 1.522240\n",
      "Iteration 24: loss = 1.519667\n",
      "Iteration 25: loss = 1.517068\n",
      "Iteration 26: loss = 1.514441\n",
      "Iteration 27: loss = 1.511787\n",
      "Iteration 28: loss = 1.509105\n",
      "Iteration 29: loss = 1.506394\n",
      "Iteration 30: loss = 1.503652\n",
      "Iteration 31: loss = 1.500878\n",
      "Iteration 32: loss = 1.498071\n",
      "Iteration 33: loss = 1.495229\n",
      "Iteration 34: loss = 1.492352\n",
      "Iteration 35: loss = 1.489439\n",
      "Iteration 36: loss = 1.486491\n",
      "Iteration 37: loss = 1.483507\n",
      "Iteration 38: loss = 1.480488\n",
      "Iteration 39: loss = 1.477435\n",
      "Iteration 40: loss = 1.474348\n",
      "Iteration 41: loss = 1.471227\n",
      "Iteration 42: loss = 1.468072\n",
      "Iteration 43: loss = 1.464885\n",
      "Iteration 44: loss = 1.461664\n",
      "Iteration 45: loss = 1.458412\n",
      "Iteration 46: loss = 1.455126\n",
      "Iteration 47: loss = 1.451807\n",
      "Iteration 48: loss = 1.448452\n",
      "Iteration 49: loss = 1.445060\n",
      "Iteration 50: loss = 1.441630\n",
      "Iteration 51: loss = 1.438163\n",
      "Iteration 52: loss = 1.434660\n",
      "Iteration 53: loss = 1.431122\n",
      "Iteration 54: loss = 1.427551\n",
      "Iteration 55: loss = 1.423948\n",
      "Iteration 56: loss = 1.420316\n",
      "Iteration 57: loss = 1.416659\n",
      "Iteration 58: loss = 1.412981\n",
      "Iteration 59: loss = 1.409284\n",
      "Iteration 60: loss = 1.405570\n",
      "Iteration 61: loss = 1.401841\n",
      "Iteration 62: loss = 1.398097\n",
      "Iteration 63: loss = 1.394342\n",
      "Iteration 64: loss = 1.390577\n",
      "Iteration 65: loss = 1.386805\n",
      "Iteration 66: loss = 1.383029\n",
      "Iteration 67: loss = 1.379252\n",
      "Iteration 68: loss = 1.375475\n",
      "Iteration 69: loss = 1.371701\n",
      "Iteration 70: loss = 1.367931\n",
      "Iteration 71: loss = 1.364167\n",
      "Iteration 72: loss = 1.360410\n",
      "Iteration 73: loss = 1.356658\n",
      "Iteration 74: loss = 1.352914\n",
      "Iteration 75: loss = 1.349177\n",
      "Iteration 76: loss = 1.345449\n",
      "Iteration 77: loss = 1.341731\n",
      "Iteration 78: loss = 1.338026\n",
      "Iteration 79: loss = 1.334336\n",
      "Iteration 80: loss = 1.330668\n",
      "Iteration 81: loss = 1.327024\n",
      "Iteration 82: loss = 1.323405\n",
      "Iteration 83: loss = 1.319809\n",
      "Iteration 84: loss = 1.316236\n",
      "Iteration 85: loss = 1.312686\n",
      "Iteration 86: loss = 1.309161\n",
      "Iteration 87: loss = 1.305662\n",
      "Iteration 88: loss = 1.302187\n",
      "Iteration 89: loss = 1.298738\n",
      "Iteration 90: loss = 1.295315\n",
      "Iteration 91: loss = 1.291916\n",
      "Iteration 92: loss = 1.288543\n",
      "Iteration 93: loss = 1.285197\n",
      "Iteration 94: loss = 1.281879\n",
      "Iteration 95: loss = 1.278591\n",
      "Iteration 96: loss = 1.275334\n",
      "Iteration 97: loss = 1.272107\n",
      "Iteration 98: loss = 1.268908\n",
      "Iteration 99: loss = 1.265738\n",
      "Iteration 100: loss = 1.262595\n",
      "Iteration 101: loss = 1.259481\n",
      "Iteration 102: loss = 1.256395\n",
      "Iteration 103: loss = 1.253339\n",
      "Iteration 104: loss = 1.250311\n",
      "Iteration 105: loss = 1.247312\n",
      "Iteration 106: loss = 1.244342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss = 0.675831\n",
      "Iteration 1: loss = 0.535721\n",
      "Iteration 2: loss = 0.440242\n",
      "Iteration 3: loss = 0.438458\n",
      "Iteration 4: loss = 0.437464\n",
      "Iteration 5: loss = 0.436856\n",
      "Iteration 6: loss = 0.436479\n",
      "Iteration 0: loss = 0.688736\n",
      "Iteration 1: loss = 0.465214\n",
      "Iteration 2: loss = 0.426580\n",
      "Iteration 3: loss = 0.417765\n",
      "Iteration 4: loss = 0.412346\n",
      "Iteration 5: loss = 0.407458\n",
      "Iteration 6: loss = 0.403030\n",
      "Iteration 7: loss = 0.399025\n",
      "Iteration 8: loss = 0.395409\n",
      "Iteration 9: loss = 0.392146\n",
      "Iteration 10: loss = 0.389204\n",
      "Iteration 11: loss = 0.386551\n",
      "Iteration 12: loss = 0.384158\n",
      "Iteration 13: loss = 0.381998\n",
      "Iteration 14: loss = 0.380046\n",
      "Iteration 15: loss = 0.378280\n",
      "Iteration 16: loss = 0.376680\n",
      "Iteration 17: loss = 0.375227\n",
      "Iteration 18: loss = 0.373906\n",
      "Iteration 19: loss = 0.372703\n",
      "Iteration 20: loss = 0.371605\n",
      "Iteration 21: loss = 0.370600\n",
      "Iteration 22: loss = 0.369679\n",
      "Iteration 23: loss = 0.368834\n",
      "Iteration 24: loss = 0.368056\n",
      "Iteration 25: loss = 0.367339\n",
      "Iteration 26: loss = 0.366677\n",
      "Iteration 27: loss = 0.366065\n",
      "Iteration 28: loss = 0.365497\n",
      "Iteration 29: loss = 0.364970\n",
      "Iteration 30: loss = 0.364480\n",
      "Iteration 31: loss = 0.364024\n",
      "Iteration 32: loss = 0.363598\n",
      "Iteration 33: loss = 0.363201\n",
      "Iteration 34: loss = 0.362829\n",
      "Iteration 35: loss = 0.362481\n",
      "Iteration 0: loss = 0.698185\n",
      "Iteration 1: loss = 0.484976\n",
      "Iteration 2: loss = 0.414223\n",
      "Iteration 3: loss = 0.397756\n",
      "Iteration 4: loss = 0.396245\n",
      "Iteration 5: loss = 0.395187\n",
      "Iteration 6: loss = 0.394143\n",
      "Iteration 7: loss = 0.393112\n",
      "Iteration 8: loss = 0.392094\n",
      "Iteration 9: loss = 0.391089\n",
      "Iteration 10: loss = 0.390097\n",
      "Iteration 11: loss = 0.389117\n",
      "Iteration 12: loss = 0.388149\n",
      "Iteration 13: loss = 0.387194\n",
      "Iteration 14: loss = 0.386251\n",
      "Iteration 15: loss = 0.385320\n",
      "Iteration 16: loss = 0.384400\n",
      "Iteration 17: loss = 0.383492\n",
      "Iteration 18: loss = 0.382596\n",
      "Iteration 19: loss = 0.381711\n",
      "Iteration 20: loss = 0.380838\n",
      "Iteration 21: loss = 0.379975\n",
      "Iteration 22: loss = 0.379124\n",
      "Iteration 23: loss = 0.378283\n",
      "Iteration 24: loss = 0.377453\n",
      "Iteration 25: loss = 0.376633\n",
      "Iteration 26: loss = 0.375824\n",
      "Iteration 27: loss = 0.375025\n",
      "Iteration 28: loss = 0.374237\n",
      "Iteration 29: loss = 0.373458\n",
      "Iteration 30: loss = 0.372689\n",
      "Iteration 31: loss = 0.371930\n",
      "Iteration 32: loss = 0.371181\n",
      "Iteration 33: loss = 0.370441\n",
      "Iteration 34: loss = 0.369711\n",
      "Iteration 35: loss = 0.368989\n",
      "Iteration 36: loss = 0.368277\n",
      "Iteration 37: loss = 0.367574\n",
      "Iteration 38: loss = 0.366880\n",
      "Iteration 39: loss = 0.366194\n",
      "Iteration 40: loss = 0.365518\n",
      "Iteration 41: loss = 0.364849\n",
      "Iteration 42: loss = 0.364190\n",
      "Iteration 43: loss = 0.363538\n",
      "Iteration 44: loss = 0.362895\n",
      "Iteration 45: loss = 0.362259\n",
      "Iteration 46: loss = 0.361632\n",
      "Iteration 47: loss = 0.361013\n",
      "Iteration 48: loss = 0.360401\n",
      "Iteration 49: loss = 0.359797\n",
      "Iteration 50: loss = 0.359200\n",
      "Iteration 51: loss = 0.358611\n",
      "Iteration 52: loss = 0.358029\n",
      "Iteration 53: loss = 0.357455\n",
      "Iteration 54: loss = 0.356887\n",
      "Iteration 55: loss = 0.356327\n",
      "Iteration 56: loss = 0.355774\n",
      "Iteration 57: loss = 0.355227\n",
      "Iteration 58: loss = 0.354687\n",
      "Iteration 59: loss = 0.354154\n",
      "Iteration 60: loss = 0.353627\n",
      "Iteration 61: loss = 0.353107\n",
      "Iteration 62: loss = 0.352593\n",
      "Iteration 63: loss = 0.352086\n",
      "Iteration 64: loss = 0.351585\n",
      "Iteration 65: loss = 0.351089\n",
      "Iteration 66: loss = 0.350600\n",
      "Iteration 67: loss = 0.350117\n",
      "Iteration 68: loss = 0.349640\n",
      "Iteration 69: loss = 0.349168\n",
      "Iteration 70: loss = 0.348702\n",
      "Iteration 71: loss = 0.348242\n",
      "Iteration 72: loss = 0.347787\n",
      "Iteration 73: loss = 0.347338\n",
      "Iteration 74: loss = 0.346894\n",
      "Iteration 75: loss = 0.346455\n",
      "Iteration 76: loss = 0.346022\n",
      "Iteration 77: loss = 0.345594\n",
      "Iteration 78: loss = 0.345171\n",
      "Iteration 79: loss = 0.344753\n",
      "Iteration 80: loss = 0.344340\n",
      "Iteration 81: loss = 0.343932\n",
      "Iteration 82: loss = 0.343528\n",
      "Iteration 83: loss = 0.343130\n",
      "Iteration 84: loss = 0.342736\n",
      "Iteration 85: loss = 0.342347\n",
      "Iteration 86: loss = 0.341962\n",
      "Iteration 87: loss = 0.341582\n",
      "Iteration 88: loss = 0.341206\n",
      "Iteration 89: loss = 0.340834\n",
      "Iteration 90: loss = 0.340467\n",
      "Iteration 91: loss = 0.340104\n",
      "Iteration 92: loss = 0.339746\n",
      "Iteration 93: loss = 0.339391\n",
      "Iteration 94: loss = 0.339041\n",
      "Iteration 95: loss = 0.338695\n",
      "Iteration 96: loss = 0.338352\n",
      "Iteration 97: loss = 0.338014\n",
      "Iteration 98: loss = 0.337679\n",
      "Iteration 0: loss = 1.796871\n",
      "Iteration 1: loss = 1.676838\n",
      "Iteration 2: loss = 1.623766\n",
      "Iteration 3: loss = 1.595187\n",
      "Iteration 4: loss = 1.579085\n",
      "Iteration 5: loss = 1.569452\n",
      "Iteration 6: loss = 1.563218\n",
      "Iteration 7: loss = 1.558790\n",
      "Iteration 8: loss = 1.555328\n",
      "Iteration 9: loss = 1.552388\n",
      "Iteration 10: loss = 1.549726\n",
      "Iteration 11: loss = 1.547211\n",
      "Iteration 12: loss = 1.544767\n",
      "Iteration 13: loss = 1.542354\n",
      "Iteration 14: loss = 1.539948\n",
      "Iteration 15: loss = 1.537537\n",
      "Iteration 16: loss = 1.535111\n",
      "Iteration 17: loss = 1.532665\n",
      "Iteration 18: loss = 1.530194\n",
      "Iteration 19: loss = 1.527695\n",
      "Iteration 20: loss = 1.525166\n",
      "Iteration 21: loss = 1.522606\n",
      "Iteration 22: loss = 1.520011\n",
      "Iteration 23: loss = 1.517382\n",
      "Iteration 24: loss = 1.514718\n",
      "Iteration 25: loss = 1.512017\n",
      "Iteration 26: loss = 1.509280\n",
      "Iteration 27: loss = 1.506507\n",
      "Iteration 28: loss = 1.503699\n",
      "Iteration 29: loss = 1.500854\n",
      "Iteration 30: loss = 1.497973\n",
      "Iteration 31: loss = 1.495054\n",
      "Iteration 32: loss = 1.492095\n",
      "Iteration 33: loss = 1.489096\n",
      "Iteration 34: loss = 1.486058\n",
      "Iteration 35: loss = 1.482979\n",
      "Iteration 36: loss = 1.479858\n",
      "Iteration 37: loss = 1.476692\n",
      "Iteration 38: loss = 1.473483\n",
      "Iteration 39: loss = 1.470231\n",
      "Iteration 40: loss = 1.466939\n",
      "Iteration 41: loss = 1.463609\n",
      "Iteration 42: loss = 1.460242\n",
      "Iteration 43: loss = 1.456840\n",
      "Iteration 44: loss = 1.453404\n",
      "Iteration 45: loss = 1.449936\n",
      "Iteration 46: loss = 1.446436\n",
      "Iteration 47: loss = 1.442908\n",
      "Iteration 48: loss = 1.439349\n",
      "Iteration 49: loss = 1.435758\n",
      "Iteration 50: loss = 1.432131\n",
      "Iteration 51: loss = 1.428467\n",
      "Iteration 52: loss = 1.424771\n",
      "Iteration 53: loss = 1.421046\n",
      "Iteration 54: loss = 1.417298\n",
      "Iteration 55: loss = 1.413526\n",
      "Iteration 56: loss = 1.409736\n",
      "Iteration 57: loss = 1.405928\n",
      "Iteration 58: loss = 1.402107\n",
      "Iteration 59: loss = 1.398272\n",
      "Iteration 60: loss = 1.394426\n",
      "Iteration 61: loss = 1.390568\n",
      "Iteration 62: loss = 1.386698\n",
      "Iteration 63: loss = 1.382817\n",
      "Iteration 64: loss = 1.378927\n",
      "Iteration 65: loss = 1.375029\n",
      "Iteration 66: loss = 1.371124\n",
      "Iteration 67: loss = 1.367215\n",
      "Iteration 68: loss = 1.363303\n",
      "Iteration 69: loss = 1.359389\n",
      "Iteration 70: loss = 1.355473\n",
      "Iteration 71: loss = 1.351561\n",
      "Iteration 72: loss = 1.347655\n",
      "Iteration 73: loss = 1.343760\n",
      "Iteration 74: loss = 1.339878\n",
      "Iteration 75: loss = 1.336012\n",
      "Iteration 76: loss = 1.332161\n",
      "Iteration 77: loss = 1.328328\n",
      "Iteration 78: loss = 1.324512\n",
      "Iteration 79: loss = 1.320716\n",
      "Iteration 80: loss = 1.316939\n",
      "Iteration 81: loss = 1.313183\n",
      "Iteration 82: loss = 1.309449\n",
      "Iteration 83: loss = 1.305738\n",
      "Iteration 84: loss = 1.302052\n",
      "Iteration 85: loss = 1.298392\n",
      "Iteration 86: loss = 1.294761\n",
      "Iteration 87: loss = 1.291159\n",
      "Iteration 88: loss = 1.287588\n",
      "Iteration 89: loss = 1.284050\n",
      "Iteration 90: loss = 1.280544\n",
      "Iteration 91: loss = 1.277072\n",
      "Iteration 92: loss = 1.273633\n",
      "Iteration 93: loss = 1.270227\n",
      "Iteration 94: loss = 1.266855\n",
      "Iteration 95: loss = 1.263515\n",
      "Iteration 96: loss = 1.260208\n",
      "Iteration 97: loss = 1.256933\n",
      "Iteration 98: loss = 1.253690\n",
      "Iteration 99: loss = 1.250478\n",
      "Iteration 100: loss = 1.247298\n",
      "Iteration 101: loss = 1.244150\n",
      "Iteration 102: loss = 1.241034\n",
      "Iteration 103: loss = 1.237950\n",
      "Iteration 104: loss = 1.234899\n",
      "Iteration 105: loss = 1.231882\n",
      "Iteration 106: loss = 1.228898\n",
      "Iteration 107: loss = 1.225947\n",
      "Iteration 108: loss = 1.223028\n",
      "Iteration 109: loss = 1.220142\n",
      "Iteration 110: loss = 1.217286\n",
      "Iteration 111: loss = 1.214461\n",
      "Iteration 112: loss = 1.211665\n",
      "Iteration 113: loss = 1.208899\n",
      "Iteration 114: loss = 1.206161\n",
      "Iteration 115: loss = 1.203453\n",
      "Iteration 116: loss = 1.200772\n",
      "Iteration 117: loss = 1.198120\n",
      "Iteration 118: loss = 1.195495\n",
      "Iteration 119: loss = 1.192897\n",
      "Iteration 120: loss = 1.190324\n",
      "Iteration 121: loss = 1.187777\n",
      "Iteration 122: loss = 1.185254\n",
      "Iteration 123: loss = 1.182757\n",
      "Iteration 124: loss = 1.180286\n",
      "Iteration 125: loss = 1.177854\n",
      "Iteration 126: loss = 1.175484\n",
      "Iteration 127: loss = 1.173339\n",
      "Iteration 128: loss = 1.171981\n",
      "Iteration 129: loss = 1.170351\n",
      "Iteration 130: loss = 1.169724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss = 0.694302\n",
      "Iteration 1: loss = 0.480445\n",
      "Iteration 2: loss = 0.476646\n",
      "Iteration 3: loss = 0.476077\n",
      "Iteration 4: loss = 0.475747\n",
      "Iteration 0: loss = 0.693775\n",
      "Iteration 1: loss = 0.426157\n",
      "Iteration 2: loss = 0.421464\n",
      "Iteration 3: loss = 0.419382\n",
      "Iteration 4: loss = 0.417756\n",
      "Iteration 5: loss = 0.416277\n",
      "Iteration 6: loss = 0.414889\n",
      "Iteration 7: loss = 0.413577\n",
      "Iteration 8: loss = 0.412337\n",
      "Iteration 9: loss = 0.411164\n",
      "Iteration 10: loss = 0.410055\n",
      "Iteration 11: loss = 0.409006\n",
      "Iteration 12: loss = 0.408014\n",
      "Iteration 13: loss = 0.407076\n",
      "Iteration 14: loss = 0.406189\n",
      "Iteration 15: loss = 0.405351\n",
      "Iteration 16: loss = 0.404559\n",
      "Iteration 17: loss = 0.403810\n",
      "Iteration 18: loss = 0.403102\n",
      "Iteration 19: loss = 0.402433\n",
      "Iteration 20: loss = 0.401800\n",
      "Iteration 21: loss = 0.401203\n",
      "Iteration 22: loss = 0.400637\n",
      "Iteration 23: loss = 0.400103\n",
      "Iteration 24: loss = 0.399598\n",
      "Iteration 25: loss = 0.399121\n",
      "Iteration 26: loss = 0.398670\n",
      "Iteration 27: loss = 0.398243\n",
      "Iteration 28: loss = 0.397839\n",
      "Iteration 29: loss = 0.397457\n",
      "Iteration 0: loss = 0.699994\n",
      "Iteration 1: loss = 0.411103\n",
      "Iteration 2: loss = 0.405433\n",
      "Iteration 3: loss = 0.403203\n",
      "Iteration 4: loss = 0.401588\n",
      "Iteration 5: loss = 0.400125\n",
      "Iteration 6: loss = 0.398721\n",
      "Iteration 7: loss = 0.397353\n",
      "Iteration 8: loss = 0.396017\n",
      "Iteration 9: loss = 0.394709\n",
      "Iteration 10: loss = 0.393431\n",
      "Iteration 11: loss = 0.392180\n",
      "Iteration 12: loss = 0.390956\n",
      "Iteration 13: loss = 0.389759\n",
      "Iteration 14: loss = 0.388589\n",
      "Iteration 15: loss = 0.387444\n",
      "Iteration 16: loss = 0.386323\n",
      "Iteration 17: loss = 0.385228\n",
      "Iteration 18: loss = 0.384156\n",
      "Iteration 19: loss = 0.383108\n",
      "Iteration 20: loss = 0.382083\n",
      "Iteration 21: loss = 0.381080\n",
      "Iteration 22: loss = 0.380099\n",
      "Iteration 23: loss = 0.379140\n",
      "Iteration 24: loss = 0.378201\n",
      "Iteration 25: loss = 0.377283\n",
      "Iteration 26: loss = 0.376385\n",
      "Iteration 27: loss = 0.375506\n",
      "Iteration 28: loss = 0.374647\n",
      "Iteration 29: loss = 0.373806\n",
      "Iteration 30: loss = 0.372983\n",
      "Iteration 31: loss = 0.372178\n",
      "Iteration 32: loss = 0.371390\n",
      "Iteration 33: loss = 0.370619\n",
      "Iteration 34: loss = 0.369865\n",
      "Iteration 35: loss = 0.369127\n",
      "Iteration 36: loss = 0.368404\n",
      "Iteration 37: loss = 0.367697\n",
      "Iteration 38: loss = 0.367005\n",
      "Iteration 39: loss = 0.366327\n",
      "Iteration 40: loss = 0.365663\n",
      "Iteration 41: loss = 0.365014\n",
      "Iteration 42: loss = 0.364377\n",
      "Iteration 43: loss = 0.363754\n",
      "Iteration 44: loss = 0.363144\n",
      "Iteration 45: loss = 0.362547\n",
      "Iteration 46: loss = 0.361961\n",
      "Iteration 47: loss = 0.361388\n",
      "Iteration 48: loss = 0.360826\n",
      "Iteration 49: loss = 0.360275\n",
      "Iteration 50: loss = 0.359736\n",
      "Iteration 51: loss = 0.359207\n",
      "Iteration 52: loss = 0.358689\n",
      "Iteration 53: loss = 0.358181\n",
      "Iteration 54: loss = 0.357683\n",
      "Iteration 55: loss = 0.357195\n",
      "Iteration 56: loss = 0.356717\n",
      "Iteration 57: loss = 0.356247\n",
      "Iteration 58: loss = 0.355787\n",
      "Iteration 59: loss = 0.355335\n",
      "Iteration 60: loss = 0.354893\n",
      "Iteration 61: loss = 0.354458\n",
      "Iteration 62: loss = 0.354032\n",
      "Iteration 63: loss = 0.353613\n",
      "Iteration 64: loss = 0.353203\n",
      "Iteration 65: loss = 0.352800\n",
      "Iteration 66: loss = 0.352405\n",
      "Iteration 67: loss = 0.352016\n",
      "Iteration 68: loss = 0.351635\n",
      "Iteration 69: loss = 0.351261\n",
      "Iteration 70: loss = 0.350893\n",
      "Iteration 71: loss = 0.350532\n",
      "Iteration 72: loss = 0.350178\n",
      "Iteration 73: loss = 0.349830\n",
      "Iteration 0: loss = 1.798995\n",
      "Iteration 1: loss = 1.750451\n",
      "Iteration 2: loss = 1.724418\n",
      "Iteration 3: loss = 1.708741\n",
      "Iteration 4: loss = 1.698675\n",
      "Iteration 5: loss = 1.691832\n",
      "Iteration 6: loss = 1.686868\n",
      "Iteration 7: loss = 1.682993\n",
      "Iteration 8: loss = 1.679737\n",
      "Iteration 9: loss = 1.676818\n",
      "Iteration 10: loss = 1.674067\n",
      "Iteration 11: loss = 1.671380\n",
      "Iteration 12: loss = 1.668694\n",
      "Iteration 13: loss = 1.665973\n",
      "Iteration 14: loss = 1.663200\n",
      "Iteration 15: loss = 1.660360\n",
      "Iteration 16: loss = 1.657442\n",
      "Iteration 17: loss = 1.654438\n",
      "Iteration 18: loss = 1.651342\n",
      "Iteration 19: loss = 1.648158\n",
      "Iteration 20: loss = 1.644886\n",
      "Iteration 21: loss = 1.641525\n",
      "Iteration 22: loss = 1.638074\n",
      "Iteration 23: loss = 1.634529\n",
      "Iteration 24: loss = 1.630886\n",
      "Iteration 25: loss = 1.627141\n",
      "Iteration 26: loss = 1.623294\n",
      "Iteration 27: loss = 1.619345\n",
      "Iteration 28: loss = 1.615293\n",
      "Iteration 29: loss = 1.611135\n",
      "Iteration 30: loss = 1.606871\n",
      "Iteration 31: loss = 1.602505\n",
      "Iteration 32: loss = 1.598041\n",
      "Iteration 33: loss = 1.593476\n",
      "Iteration 34: loss = 1.588806\n",
      "Iteration 35: loss = 1.584032\n",
      "Iteration 36: loss = 1.579152\n",
      "Iteration 37: loss = 1.574160\n",
      "Iteration 38: loss = 1.569058\n",
      "Iteration 39: loss = 1.563865\n",
      "Iteration 40: loss = 1.558596\n",
      "Iteration 41: loss = 1.553259\n",
      "Iteration 42: loss = 1.547868\n",
      "Iteration 43: loss = 1.542437\n",
      "Iteration 44: loss = 1.536961\n",
      "Iteration 45: loss = 1.531446\n",
      "Iteration 46: loss = 1.525902\n",
      "Iteration 47: loss = 1.520336\n",
      "Iteration 48: loss = 1.514749\n",
      "Iteration 49: loss = 1.509148\n",
      "Iteration 50: loss = 1.503542\n",
      "Iteration 51: loss = 1.497939\n",
      "Iteration 52: loss = 1.492339\n",
      "Iteration 53: loss = 1.486756\n",
      "Iteration 54: loss = 1.481216\n",
      "Iteration 55: loss = 1.475714\n",
      "Iteration 56: loss = 1.470238\n",
      "Iteration 57: loss = 1.464794\n",
      "Iteration 58: loss = 1.459388\n",
      "Iteration 59: loss = 1.454021\n",
      "Iteration 60: loss = 1.448696\n",
      "Iteration 61: loss = 1.443411\n",
      "Iteration 62: loss = 1.438170\n",
      "Iteration 63: loss = 1.432976\n",
      "Iteration 64: loss = 1.427826\n",
      "Iteration 65: loss = 1.422722\n",
      "Iteration 66: loss = 1.417667\n",
      "Iteration 67: loss = 1.412663\n",
      "Iteration 68: loss = 1.407712\n",
      "Iteration 69: loss = 1.402813\n",
      "Iteration 70: loss = 1.397963\n",
      "Iteration 71: loss = 1.393159\n",
      "Iteration 72: loss = 1.388402\n",
      "Iteration 73: loss = 1.383691\n",
      "Iteration 74: loss = 1.379022\n",
      "Iteration 75: loss = 1.374394\n",
      "Iteration 76: loss = 1.369804\n",
      "Iteration 77: loss = 1.365253\n",
      "Iteration 78: loss = 1.360743\n",
      "Iteration 79: loss = 1.356278\n",
      "Iteration 80: loss = 1.351858\n",
      "Iteration 81: loss = 1.347484\n",
      "Iteration 82: loss = 1.343157\n",
      "Iteration 83: loss = 1.338878\n",
      "Iteration 84: loss = 1.334648\n",
      "Iteration 85: loss = 1.330467\n",
      "Iteration 86: loss = 1.326335\n",
      "Iteration 87: loss = 1.322250\n",
      "Iteration 88: loss = 1.318211\n",
      "Iteration 89: loss = 1.314215\n",
      "Iteration 90: loss = 1.310261\n",
      "Iteration 91: loss = 1.306348\n",
      "Iteration 92: loss = 1.302474\n",
      "Iteration 93: loss = 1.298639\n",
      "Iteration 94: loss = 1.294842\n",
      "Iteration 95: loss = 1.291082\n",
      "Iteration 96: loss = 1.287357\n",
      "Iteration 97: loss = 1.283669\n",
      "Iteration 98: loss = 1.280017\n",
      "Iteration 99: loss = 1.276402\n",
      "Iteration 100: loss = 1.272825\n",
      "Iteration 101: loss = 1.269286\n",
      "Iteration 102: loss = 1.265784\n",
      "Iteration 103: loss = 1.262317\n",
      "Iteration 104: loss = 1.258883\n",
      "Iteration 105: loss = 1.255480\n",
      "Iteration 106: loss = 1.252108\n",
      "Iteration 107: loss = 1.248768\n",
      "Iteration 108: loss = 1.245457\n",
      "Iteration 109: loss = 1.242175\n",
      "Iteration 110: loss = 1.238921\n",
      "Iteration 111: loss = 1.235693\n",
      "Iteration 112: loss = 1.232489\n",
      "Iteration 113: loss = 1.229309\n",
      "Iteration 114: loss = 1.226151\n",
      "Iteration 115: loss = 1.223016\n",
      "Iteration 116: loss = 1.219904\n",
      "Iteration 117: loss = 1.216818\n",
      "Iteration 118: loss = 1.213753\n",
      "Iteration 119: loss = 1.210733\n",
      "Iteration 120: loss = 1.207803\n",
      "Iteration 121: loss = 1.205757\n",
      "Iteration 122: loss = 1.204197\n",
      "Iteration 123: loss = 1.203568\n",
      "Iteration 0: loss = 1.791444\n",
      "Iteration 1: loss = 1.743641\n",
      "Iteration 2: loss = 1.704182\n",
      "Iteration 3: loss = 1.668998\n",
      "Iteration 4: loss = 1.636297\n",
      "Iteration 5: loss = 1.605237\n",
      "Iteration 6: loss = 1.575352\n",
      "Iteration 7: loss = 1.546340\n",
      "Iteration 8: loss = 1.517982\n",
      "Iteration 9: loss = 1.490118\n",
      "Iteration 10: loss = 1.462618\n",
      "Iteration 11: loss = 1.435376\n",
      "Iteration 12: loss = 1.408327\n",
      "Iteration 13: loss = 1.381461\n",
      "Iteration 14: loss = 1.354825\n",
      "Iteration 15: loss = 1.328468\n",
      "Iteration 16: loss = 1.302435\n",
      "Iteration 17: loss = 1.276785\n",
      "Iteration 18: loss = 1.251599\n",
      "Iteration 19: loss = 1.226931\n",
      "Iteration 20: loss = 1.202828\n",
      "Iteration 21: loss = 1.179357\n",
      "Iteration 22: loss = 1.156608\n",
      "Iteration 23: loss = 1.134587\n",
      "Iteration 24: loss = 1.113306\n",
      "Iteration 25: loss = 1.092788\n",
      "Iteration 26: loss = 1.073061\n",
      "Iteration 27: loss = 1.054180\n",
      "Iteration 28: loss = 1.036196\n",
      "Iteration 29: loss = 1.019072\n",
      "Iteration 30: loss = 1.002795\n",
      "Iteration 31: loss = 0.987356\n",
      "Iteration 32: loss = 0.972675\n",
      "Iteration 33: loss = 0.958680\n",
      "Iteration 34: loss = 0.945371\n",
      "Iteration 35: loss = 0.932720\n",
      "Iteration 36: loss = 0.920682\n",
      "Iteration 37: loss = 0.909226\n",
      "Iteration 38: loss = 0.898324\n",
      "Iteration 39: loss = 0.887956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss = 0.688323\n",
      "Iteration 1: loss = 0.624825\n",
      "Iteration 2: loss = 0.448056\n",
      "Iteration 3: loss = 0.447354\n",
      "Iteration 4: loss = 0.447048\n",
      "Iteration 0: loss = 0.692053\n",
      "Iteration 1: loss = 0.432880\n",
      "Iteration 2: loss = 0.429119\n",
      "Iteration 3: loss = 0.427763\n",
      "Iteration 4: loss = 0.426798\n",
      "Iteration 5: loss = 0.425943\n",
      "Iteration 6: loss = 0.425151\n",
      "Iteration 7: loss = 0.424409\n",
      "Iteration 8: loss = 0.423713\n",
      "Iteration 9: loss = 0.423059\n",
      "Iteration 10: loss = 0.422445\n",
      "Iteration 11: loss = 0.421868\n",
      "Iteration 12: loss = 0.421325\n",
      "Iteration 13: loss = 0.420815\n",
      "Iteration 14: loss = 0.420335\n",
      "Iteration 15: loss = 0.419884\n",
      "Iteration 16: loss = 0.419459\n",
      "Iteration 17: loss = 0.419058\n",
      "Iteration 0: loss = 0.714008\n",
      "Iteration 1: loss = 0.422565\n",
      "Iteration 2: loss = 0.402876\n",
      "Iteration 3: loss = 0.398197\n",
      "Iteration 4: loss = 0.396593\n",
      "Iteration 5: loss = 0.395843\n",
      "Iteration 6: loss = 0.395364\n",
      "Iteration 7: loss = 0.394979\n",
      "Iteration 0: loss = 0.703051\n",
      "Iteration 1: loss = 0.473794\n",
      "Iteration 2: loss = 0.422279\n",
      "Iteration 3: loss = 0.409670\n",
      "Iteration 4: loss = 0.402645\n",
      "Iteration 5: loss = 0.396102\n",
      "Iteration 6: loss = 0.389966\n",
      "Iteration 7: loss = 0.384227\n",
      "Iteration 8: loss = 0.378870\n",
      "Iteration 9: loss = 0.373877\n",
      "Iteration 10: loss = 0.369225\n",
      "Iteration 11: loss = 0.364892\n",
      "Iteration 12: loss = 0.360853\n",
      "Iteration 13: loss = 0.357083\n",
      "Iteration 14: loss = 0.353560\n",
      "Iteration 15: loss = 0.350260\n",
      "Iteration 16: loss = 0.347163\n",
      "Iteration 17: loss = 0.344249\n",
      "Iteration 18: loss = 0.341501\n",
      "Iteration 19: loss = 0.338904\n",
      "Iteration 20: loss = 0.336442\n",
      "Iteration 21: loss = 0.334104\n",
      "Iteration 22: loss = 0.331877\n",
      "Iteration 23: loss = 0.329752\n",
      "Iteration 24: loss = 0.327720\n",
      "Iteration 25: loss = 0.325773\n",
      "Iteration 26: loss = 0.323904\n",
      "Iteration 27: loss = 0.322106\n",
      "Iteration 28: loss = 0.320373\n",
      "Iteration 29: loss = 0.318702\n",
      "Iteration 30: loss = 0.317087\n",
      "Iteration 31: loss = 0.315525\n",
      "Iteration 32: loss = 0.314011\n",
      "Iteration 33: loss = 0.312543\n",
      "Iteration 34: loss = 0.311118\n",
      "Iteration 35: loss = 0.309733\n",
      "Iteration 36: loss = 0.308386\n",
      "Iteration 37: loss = 0.307074\n",
      "Iteration 38: loss = 0.305796\n",
      "Iteration 39: loss = 0.304550\n",
      "Iteration 40: loss = 0.303334\n",
      "Iteration 41: loss = 0.302147\n",
      "Iteration 42: loss = 0.300987\n",
      "Iteration 43: loss = 0.299853\n",
      "Iteration 44: loss = 0.298745\n",
      "Iteration 45: loss = 0.297660\n",
      "Iteration 46: loss = 0.296597\n",
      "Iteration 47: loss = 0.295557\n",
      "Iteration 48: loss = 0.294538\n",
      "Iteration 49: loss = 0.293539\n",
      "Iteration 50: loss = 0.292559\n",
      "Iteration 51: loss = 0.291598\n",
      "Iteration 52: loss = 0.290655\n",
      "Iteration 53: loss = 0.289730\n",
      "Iteration 54: loss = 0.288821\n",
      "Iteration 55: loss = 0.287929\n",
      "Iteration 56: loss = 0.287052\n",
      "Iteration 57: loss = 0.286190\n",
      "Iteration 58: loss = 0.285344\n",
      "Iteration 59: loss = 0.284511\n",
      "Iteration 60: loss = 0.283693\n",
      "Iteration 61: loss = 0.282888\n",
      "Iteration 62: loss = 0.282096\n",
      "Iteration 63: loss = 0.281316\n",
      "Iteration 64: loss = 0.280549\n",
      "Iteration 65: loss = 0.279795\n",
      "Iteration 66: loss = 0.279051\n",
      "Iteration 67: loss = 0.278320\n",
      "Iteration 68: loss = 0.277599\n",
      "Iteration 69: loss = 0.276889\n",
      "Iteration 70: loss = 0.276190\n",
      "Iteration 71: loss = 0.275501\n",
      "Iteration 72: loss = 0.274821\n",
      "Iteration 73: loss = 0.274152\n",
      "Iteration 74: loss = 0.273493\n",
      "Iteration 75: loss = 0.272842\n",
      "Iteration 76: loss = 0.272201\n",
      "Iteration 77: loss = 0.271569\n",
      "Iteration 78: loss = 0.270945\n",
      "Iteration 79: loss = 0.270330\n",
      "Iteration 80: loss = 0.269723\n",
      "Iteration 81: loss = 0.269125\n",
      "Iteration 82: loss = 0.268534\n",
      "Iteration 83: loss = 0.267952\n",
      "Iteration 84: loss = 0.267377\n",
      "Iteration 85: loss = 0.266809\n",
      "Iteration 86: loss = 0.266249\n",
      "Iteration 87: loss = 0.265696\n",
      "Iteration 88: loss = 0.265150\n",
      "Iteration 89: loss = 0.264611\n",
      "Iteration 90: loss = 0.264079\n",
      "Iteration 91: loss = 0.263554\n",
      "Iteration 92: loss = 0.263035\n",
      "Iteration 93: loss = 0.262522\n",
      "Iteration 94: loss = 0.262016\n",
      "Iteration 95: loss = 0.261516\n",
      "Iteration 96: loss = 0.261022\n",
      "Iteration 97: loss = 0.260534\n",
      "Iteration 98: loss = 0.260052\n",
      "Iteration 99: loss = 0.259575\n",
      "Iteration 100: loss = 0.259105\n",
      "Iteration 101: loss = 0.258640\n",
      "Iteration 102: loss = 0.258180\n",
      "Iteration 103: loss = 0.257725\n",
      "Iteration 104: loss = 0.257276\n",
      "Iteration 105: loss = 0.256832\n",
      "Iteration 106: loss = 0.256394\n",
      "Iteration 107: loss = 0.255960\n",
      "Iteration 108: loss = 0.255531\n",
      "Iteration 109: loss = 0.255107\n",
      "Iteration 110: loss = 0.254688\n",
      "Iteration 111: loss = 0.254273\n",
      "Iteration 112: loss = 0.253864\n",
      "Iteration 113: loss = 0.253458\n",
      "Iteration 114: loss = 0.253057\n",
      "Iteration 115: loss = 0.252661\n",
      "Iteration 116: loss = 0.252269\n",
      "Iteration 117: loss = 0.251881\n",
      "Iteration 118: loss = 0.251497\n",
      "Iteration 119: loss = 0.251118\n",
      "Iteration 120: loss = 0.250743\n",
      "Iteration 121: loss = 0.250371\n",
      "Iteration 122: loss = 0.250004\n",
      "Iteration 123: loss = 0.249640\n",
      "Iteration 124: loss = 0.249281\n",
      "Iteration 125: loss = 0.248925\n",
      "Iteration 126: loss = 0.248573\n",
      "Iteration 127: loss = 0.248225\n",
      "Iteration 128: loss = 0.247880\n",
      "Iteration 129: loss = 0.247539\n",
      "Iteration 130: loss = 0.247201\n",
      "Iteration 131: loss = 0.246867\n",
      "Iteration 132: loss = 0.246536\n",
      "Iteration 133: loss = 0.246209\n",
      "Iteration 134: loss = 0.245885\n",
      "Iteration 135: loss = 0.245564\n",
      "Iteration 136: loss = 0.245246\n",
      "Iteration 137: loss = 0.244932\n",
      "Iteration 138: loss = 0.244621\n",
      "Iteration 139: loss = 0.244313\n",
      "Iteration 140: loss = 0.244008\n",
      "Iteration 141: loss = 0.243706\n",
      "Iteration 142: loss = 0.243407\n",
      "Iteration 143: loss = 0.243111\n",
      "Iteration 144: loss = 0.242818\n",
      "Iteration 145: loss = 0.242528\n",
      "Iteration 146: loss = 0.242241\n",
      "Iteration 147: loss = 0.241956\n",
      "Iteration 148: loss = 0.241675\n",
      "Iteration 149: loss = 0.241396\n",
      "Iteration 150: loss = 0.241119\n",
      "Iteration 151: loss = 0.240846\n",
      "Iteration 152: loss = 0.240575\n",
      "Iteration 153: loss = 0.240306\n",
      "Iteration 154: loss = 0.240040\n",
      "Iteration 155: loss = 0.239777\n",
      "Iteration 156: loss = 0.239516\n",
      "Iteration 157: loss = 0.239258\n",
      "Iteration 158: loss = 0.239002\n",
      "Iteration 159: loss = 0.238748\n",
      "Iteration 160: loss = 0.238497\n",
      "Iteration 161: loss = 0.238248\n",
      "Iteration 162: loss = 0.238002\n",
      "Iteration 163: loss = 0.237757\n",
      "Iteration 164: loss = 0.237515\n",
      "Iteration 165: loss = 0.237276\n",
      "Iteration 166: loss = 0.237038\n",
      "Iteration 167: loss = 0.236803\n",
      "Iteration 0: loss = 1.789170\n",
      "Iteration 1: loss = 1.559043\n",
      "Iteration 2: loss = 1.481208\n",
      "Iteration 3: loss = 1.449891\n",
      "Iteration 4: loss = 1.428155\n",
      "Iteration 5: loss = 1.420563\n",
      "Iteration 6: loss = 1.416351\n",
      "Iteration 7: loss = 1.413224\n",
      "Iteration 8: loss = 1.410583\n",
      "Iteration 9: loss = 1.408244\n",
      "Iteration 10: loss = 1.406133\n",
      "Iteration 11: loss = 1.404211\n",
      "Iteration 12: loss = 1.402452\n",
      "Iteration 13: loss = 1.400835\n",
      "Iteration 14: loss = 1.399343\n",
      "Iteration 15: loss = 1.397962\n",
      "Iteration 0: loss = 1.791919\n",
      "Iteration 1: loss = 1.557968\n",
      "Iteration 2: loss = 1.479450\n",
      "Iteration 3: loss = 1.446786\n",
      "Iteration 4: loss = 1.420575\n",
      "Iteration 5: loss = 1.411981\n",
      "Iteration 6: loss = 1.407425\n",
      "Iteration 7: loss = 1.404096\n",
      "Iteration 8: loss = 1.401296\n",
      "Iteration 9: loss = 1.398818\n",
      "Iteration 10: loss = 1.396584\n",
      "Iteration 11: loss = 1.394554\n",
      "Iteration 12: loss = 1.392699\n",
      "Iteration 13: loss = 1.390999\n",
      "Iteration 14: loss = 1.389433\n",
      "Iteration 15: loss = 1.387987\n",
      "Iteration 16: loss = 1.386645\n",
      "Iteration 0: loss = 1.791383\n",
      "Iteration 1: loss = 1.562013\n",
      "Iteration 2: loss = 1.485408\n",
      "Iteration 3: loss = 1.455571\n",
      "Iteration 4: loss = 1.428894\n",
      "Iteration 5: loss = 1.420431\n",
      "Iteration 6: loss = 1.416003\n",
      "Iteration 7: loss = 1.412793\n",
      "Iteration 8: loss = 1.410104\n",
      "Iteration 9: loss = 1.407728\n",
      "Iteration 10: loss = 1.405586\n",
      "Iteration 11: loss = 1.403640\n",
      "Iteration 12: loss = 1.401860\n",
      "Iteration 13: loss = 1.400228\n",
      "Iteration 14: loss = 1.398723\n",
      "Iteration 15: loss = 1.397332\n",
      "Iteration 0: loss = 1.794880\n",
      "Iteration 1: loss = 1.680554\n",
      "Iteration 2: loss = 1.590865\n",
      "Iteration 3: loss = 1.513331\n",
      "Iteration 4: loss = 1.445143\n",
      "Iteration 5: loss = 1.384712\n",
      "Iteration 6: loss = 1.330807\n",
      "Iteration 7: loss = 1.282426\n",
      "Iteration 8: loss = 1.238754\n",
      "Iteration 9: loss = 1.199125\n",
      "Iteration 10: loss = 1.162994\n",
      "Iteration 11: loss = 1.129907\n",
      "Iteration 12: loss = 1.099491\n",
      "Iteration 13: loss = 1.071437\n",
      "Iteration 14: loss = 1.045483\n",
      "Iteration 15: loss = 1.021424\n",
      "Iteration 16: loss = 0.999095\n",
      "Iteration 17: loss = 0.978317\n",
      "Iteration 18: loss = 0.958935\n",
      "Iteration 19: loss = 0.940834\n",
      "Iteration 20: loss = 0.923903\n",
      "Iteration 21: loss = 0.908052\n",
      "Iteration 22: loss = 0.893200\n",
      "Iteration 23: loss = 0.879243\n",
      "Iteration 24: loss = 0.866121\n",
      "Iteration 0: loss = 0.714829\n",
      "Iteration 1: loss = 0.610718\n",
      "Iteration 2: loss = 0.434415\n",
      "Iteration 3: loss = 0.430957\n",
      "Iteration 4: loss = 0.430331\n",
      "Iteration 5: loss = 0.430081\n",
      "Iteration 0: loss = 0.690862\n",
      "Iteration 1: loss = 0.417059\n",
      "Iteration 2: loss = 0.412205\n",
      "Iteration 3: loss = 0.410432\n",
      "Iteration 4: loss = 0.409193\n",
      "Iteration 5: loss = 0.408105\n",
      "Iteration 6: loss = 0.407097\n",
      "Iteration 7: loss = 0.406150\n",
      "Iteration 8: loss = 0.405258\n",
      "Iteration 9: loss = 0.404418\n",
      "Iteration 10: loss = 0.403627\n",
      "Iteration 11: loss = 0.402881\n",
      "Iteration 12: loss = 0.402178\n",
      "Iteration 13: loss = 0.401515\n",
      "Iteration 14: loss = 0.400891\n",
      "Iteration 15: loss = 0.400302\n",
      "Iteration 16: loss = 0.399747\n",
      "Iteration 17: loss = 0.399224\n",
      "Iteration 18: loss = 0.398731\n",
      "Iteration 19: loss = 0.398266\n",
      "Iteration 20: loss = 0.397827\n",
      "Iteration 21: loss = 0.397414\n",
      "Iteration 22: loss = 0.397024\n",
      "Iteration 0: loss = 0.684245\n",
      "Iteration 1: loss = 0.427500\n",
      "Iteration 2: loss = 0.409732\n",
      "Iteration 3: loss = 0.405422\n",
      "Iteration 4: loss = 0.403827\n",
      "Iteration 5: loss = 0.402967\n",
      "Iteration 6: loss = 0.402335\n",
      "Iteration 7: loss = 0.401780\n",
      "Iteration 8: loss = 0.401257\n",
      "Iteration 9: loss = 0.400747\n",
      "Iteration 10: loss = 0.400245\n",
      "Iteration 11: loss = 0.399751\n",
      "Iteration 12: loss = 0.399262\n",
      "Iteration 13: loss = 0.398780\n",
      "Iteration 14: loss = 0.398302\n",
      "Iteration 15: loss = 0.397830\n",
      "Iteration 16: loss = 0.397364\n",
      "Iteration 17: loss = 0.396903\n",
      "Iteration 18: loss = 0.396447\n",
      "Iteration 19: loss = 0.395996\n",
      "Iteration 20: loss = 0.395551\n",
      "Iteration 21: loss = 0.395110\n",
      "Iteration 22: loss = 0.394675\n",
      "Iteration 23: loss = 0.394244\n",
      "Iteration 24: loss = 0.393819\n",
      "Iteration 25: loss = 0.393398\n",
      "Iteration 26: loss = 0.392982\n",
      "Iteration 27: loss = 0.392571\n",
      "Iteration 28: loss = 0.392165\n",
      "Iteration 29: loss = 0.391763\n",
      "Iteration 30: loss = 0.391366\n",
      "Iteration 31: loss = 0.390973\n",
      "Iteration 32: loss = 0.390585\n",
      "Iteration 0: loss = 0.714416\n",
      "Iteration 1: loss = 0.419559\n",
      "Iteration 2: loss = 0.399250\n",
      "Iteration 3: loss = 0.394356\n",
      "Iteration 4: loss = 0.392677\n",
      "Iteration 5: loss = 0.391908\n",
      "Iteration 6: loss = 0.391434\n",
      "Iteration 7: loss = 0.391062\n",
      "Iteration 0: loss = 1.793276\n",
      "Iteration 1: loss = 1.748478\n",
      "Iteration 2: loss = 1.724116\n",
      "Iteration 3: loss = 1.709386\n",
      "Iteration 4: loss = 1.699921\n",
      "Iteration 5: loss = 1.693494\n",
      "Iteration 6: loss = 1.688840\n",
      "Iteration 7: loss = 1.685217\n",
      "Iteration 8: loss = 1.682182\n",
      "Iteration 9: loss = 1.679470\n",
      "Iteration 10: loss = 1.676922\n",
      "Iteration 11: loss = 1.674438\n",
      "Iteration 12: loss = 1.671961\n",
      "Iteration 13: loss = 1.669454\n",
      "Iteration 14: loss = 1.666894\n",
      "Iteration 15: loss = 1.664269\n",
      "Iteration 16: loss = 1.661572\n",
      "Iteration 17: loss = 1.658799\n",
      "Iteration 18: loss = 1.655944\n",
      "Iteration 19: loss = 1.653000\n",
      "Iteration 20: loss = 1.649965\n",
      "Iteration 21: loss = 1.646839\n",
      "Iteration 22: loss = 1.643625\n",
      "Iteration 23: loss = 1.640320\n",
      "Iteration 24: loss = 1.636924\n",
      "Iteration 25: loss = 1.633435\n",
      "Iteration 26: loss = 1.629852\n",
      "Iteration 27: loss = 1.626171\n",
      "Iteration 28: loss = 1.622393\n",
      "Iteration 29: loss = 1.618520\n",
      "Iteration 30: loss = 1.614558\n",
      "Iteration 31: loss = 1.610508\n",
      "Iteration 32: loss = 1.606367\n",
      "Iteration 33: loss = 1.602127\n",
      "Iteration 34: loss = 1.597783\n",
      "Iteration 35: loss = 1.593341\n",
      "Iteration 36: loss = 1.588811\n",
      "Iteration 37: loss = 1.584198\n",
      "Iteration 38: loss = 1.579497\n",
      "Iteration 39: loss = 1.574708\n",
      "Iteration 40: loss = 1.569833\n",
      "Iteration 41: loss = 1.564873\n",
      "Iteration 42: loss = 1.559832\n",
      "Iteration 43: loss = 1.554714\n",
      "Iteration 44: loss = 1.549527\n",
      "Iteration 45: loss = 1.544275\n",
      "Iteration 46: loss = 1.538967\n",
      "Iteration 47: loss = 1.533616\n",
      "Iteration 48: loss = 1.528236\n",
      "Iteration 49: loss = 1.522835\n",
      "Iteration 50: loss = 1.517422\n",
      "Iteration 51: loss = 1.512001\n",
      "Iteration 52: loss = 1.506579\n",
      "Iteration 53: loss = 1.501162\n",
      "Iteration 54: loss = 1.495753\n",
      "Iteration 55: loss = 1.490352\n",
      "Iteration 56: loss = 1.484968\n",
      "Iteration 57: loss = 1.479614\n",
      "Iteration 58: loss = 1.474287\n",
      "Iteration 59: loss = 1.468982\n",
      "Iteration 60: loss = 1.463700\n",
      "Iteration 61: loss = 1.458443\n",
      "Iteration 62: loss = 1.453214\n",
      "Iteration 63: loss = 1.448018\n",
      "Iteration 64: loss = 1.442858\n",
      "Iteration 65: loss = 1.437736\n",
      "Iteration 66: loss = 1.432655\n",
      "Iteration 67: loss = 1.427616\n",
      "Iteration 68: loss = 1.422621\n",
      "Iteration 69: loss = 1.417668\n",
      "Iteration 70: loss = 1.412757\n",
      "Iteration 71: loss = 1.407885\n",
      "Iteration 72: loss = 1.403050\n",
      "Iteration 73: loss = 1.398255\n",
      "Iteration 74: loss = 1.393506\n",
      "Iteration 75: loss = 1.388806\n",
      "Iteration 76: loss = 1.384156\n",
      "Iteration 77: loss = 1.379555\n",
      "Iteration 78: loss = 1.375002\n",
      "Iteration 79: loss = 1.370494\n",
      "Iteration 80: loss = 1.366031\n",
      "Iteration 81: loss = 1.361613\n",
      "Iteration 82: loss = 1.357242\n",
      "Iteration 83: loss = 1.352917\n",
      "Iteration 84: loss = 1.348640\n",
      "Iteration 85: loss = 1.344408\n",
      "Iteration 86: loss = 1.340221\n",
      "Iteration 87: loss = 1.336078\n",
      "Iteration 88: loss = 1.331978\n",
      "Iteration 89: loss = 1.327920\n",
      "Iteration 90: loss = 1.323905\n",
      "Iteration 91: loss = 1.319931\n",
      "Iteration 92: loss = 1.315999\n",
      "Iteration 93: loss = 1.312108\n",
      "Iteration 94: loss = 1.308256\n",
      "Iteration 95: loss = 1.304440\n",
      "Iteration 96: loss = 1.300658\n",
      "Iteration 97: loss = 1.296911\n",
      "Iteration 98: loss = 1.293197\n",
      "Iteration 99: loss = 1.289516\n",
      "Iteration 100: loss = 1.285867\n",
      "Iteration 101: loss = 1.282248\n",
      "Iteration 102: loss = 1.278661\n",
      "Iteration 103: loss = 1.275106\n",
      "Iteration 104: loss = 1.271583\n",
      "Iteration 105: loss = 1.268092\n",
      "Iteration 106: loss = 1.264631\n",
      "Iteration 107: loss = 1.261202\n",
      "Iteration 108: loss = 1.257804\n",
      "Iteration 109: loss = 1.254438\n",
      "Iteration 110: loss = 1.251102\n",
      "Iteration 111: loss = 1.247799\n",
      "Iteration 112: loss = 1.244526\n",
      "Iteration 113: loss = 1.241283\n",
      "Iteration 114: loss = 1.238070\n",
      "Iteration 115: loss = 1.234886\n",
      "Iteration 116: loss = 1.231729\n",
      "Iteration 117: loss = 1.228600\n",
      "Iteration 118: loss = 1.225496\n",
      "Iteration 119: loss = 1.222463\n",
      "Iteration 120: loss = 1.219688\n",
      "Iteration 121: loss = 1.218953\n",
      "Iteration 0: loss = 1.787404\n",
      "Iteration 1: loss = 1.739221\n",
      "Iteration 2: loss = 1.699007\n",
      "Iteration 3: loss = 1.663000\n",
      "Iteration 4: loss = 1.629547\n",
      "Iteration 5: loss = 1.597844\n",
      "Iteration 6: loss = 1.567430\n",
      "Iteration 7: loss = 1.537995\n",
      "Iteration 8: loss = 1.509305\n",
      "Iteration 9: loss = 1.481182\n",
      "Iteration 10: loss = 1.453491\n",
      "Iteration 11: loss = 1.426145\n",
      "Iteration 12: loss = 1.399107\n",
      "Iteration 13: loss = 1.372347\n",
      "Iteration 14: loss = 1.345862\n",
      "Iteration 15: loss = 1.319720\n",
      "Iteration 16: loss = 1.293974\n",
      "Iteration 17: loss = 1.268680\n",
      "Iteration 18: loss = 1.243892\n",
      "Iteration 19: loss = 1.219668\n",
      "Iteration 20: loss = 1.196084\n",
      "Iteration 21: loss = 1.173183\n",
      "Iteration 22: loss = 1.150978\n",
      "Iteration 23: loss = 1.129487\n",
      "Iteration 24: loss = 1.108744\n",
      "Iteration 25: loss = 1.088755\n",
      "Iteration 26: loss = 1.069524\n",
      "Iteration 27: loss = 1.051067\n",
      "Iteration 28: loss = 1.033409\n",
      "Iteration 29: loss = 1.016570\n",
      "Iteration 30: loss = 1.000519\n",
      "Iteration 31: loss = 0.985203\n",
      "Iteration 32: loss = 0.970598\n",
      "Iteration 33: loss = 0.956719\n",
      "Iteration 34: loss = 0.943563\n",
      "Iteration 35: loss = 0.931081\n",
      "Iteration 36: loss = 0.919214\n",
      "Iteration 37: loss = 0.907919\n",
      "Iteration 38: loss = 0.897169\n",
      "Iteration 39: loss = 0.886935\n",
      "Iteration 40: loss = 0.877188\n",
      "Iteration 41: loss = 0.867900\n",
      "Iteration 42: loss = 0.859038\n",
      "Iteration 43: loss = 0.850558\n",
      "Iteration 44: loss = 0.842425\n",
      "Iteration 45: loss = 0.834617\n",
      "Iteration 46: loss = 0.827119\n",
      "Iteration 47: loss = 0.819922\n",
      "Iteration 48: loss = 0.813012\n",
      "Iteration 49: loss = 0.806371\n",
      "Iteration 50: loss = 0.799988\n",
      "Iteration 51: loss = 0.793853\n",
      "Iteration 52: loss = 0.787952\n",
      "Iteration 53: loss = 0.782268\n",
      "Iteration 54: loss = 0.776788\n",
      "Iteration 55: loss = 0.771496\n",
      "Iteration 56: loss = 0.766374\n",
      "Iteration 57: loss = 0.761412\n",
      "Iteration 58: loss = 0.756596\n",
      "Iteration 59: loss = 0.751914\n",
      "Iteration 60: loss = 0.747354\n",
      "Iteration 61: loss = 0.742913\n",
      "Iteration 62: loss = 0.738595\n",
      "Iteration 63: loss = 0.734403\n",
      "Iteration 64: loss = 0.730330\n",
      "Iteration 65: loss = 0.726368\n",
      "Iteration 66: loss = 0.722505\n",
      "Iteration 67: loss = 0.718728\n",
      "Iteration 68: loss = 0.715030\n",
      "Iteration 69: loss = 0.711407\n",
      "Iteration 70: loss = 0.707869\n",
      "Iteration 71: loss = 0.704418\n",
      "Iteration 72: loss = 0.701040\n",
      "Iteration 73: loss = 0.697723\n",
      "Iteration 74: loss = 0.694459\n",
      "Iteration 75: loss = 0.691246\n",
      "Iteration 76: loss = 0.688084\n",
      "Iteration 77: loss = 0.684972\n",
      "Iteration 78: loss = 0.681911\n",
      "Iteration 79: loss = 0.678899\n",
      "Iteration 80: loss = 0.675929\n",
      "Iteration 81: loss = 0.672995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss = 0.701134\n",
      "Iteration 1: loss = 0.566971\n",
      "Iteration 2: loss = 0.455947\n",
      "Iteration 3: loss = 0.454676\n",
      "Iteration 4: loss = 0.453960\n",
      "Iteration 5: loss = 0.453526\n",
      "Iteration 0: loss = 0.695993\n",
      "Iteration 1: loss = 0.471281\n",
      "Iteration 2: loss = 0.411637\n",
      "Iteration 3: loss = 0.399192\n",
      "Iteration 4: loss = 0.394091\n",
      "Iteration 5: loss = 0.389494\n",
      "Iteration 6: loss = 0.385290\n",
      "Iteration 7: loss = 0.381448\n",
      "Iteration 8: loss = 0.377938\n",
      "Iteration 9: loss = 0.374730\n",
      "Iteration 10: loss = 0.371799\n",
      "Iteration 11: loss = 0.369121\n",
      "Iteration 12: loss = 0.366672\n",
      "Iteration 13: loss = 0.364432\n",
      "Iteration 14: loss = 0.362382\n",
      "Iteration 15: loss = 0.360505\n",
      "Iteration 16: loss = 0.358784\n",
      "Iteration 17: loss = 0.357206\n",
      "Iteration 18: loss = 0.355757\n",
      "Iteration 19: loss = 0.354425\n",
      "Iteration 20: loss = 0.353200\n",
      "Iteration 21: loss = 0.352073\n",
      "Iteration 22: loss = 0.351035\n",
      "Iteration 23: loss = 0.350077\n",
      "Iteration 24: loss = 0.349193\n",
      "Iteration 25: loss = 0.348377\n",
      "Iteration 26: loss = 0.347622\n",
      "Iteration 27: loss = 0.346924\n",
      "Iteration 28: loss = 0.346277\n",
      "Iteration 29: loss = 0.345678\n",
      "Iteration 30: loss = 0.345122\n",
      "Iteration 31: loss = 0.344606\n",
      "Iteration 32: loss = 0.344128\n",
      "Iteration 33: loss = 0.343683\n",
      "Iteration 34: loss = 0.343269\n",
      "Iteration 35: loss = 0.342885\n",
      "Iteration 36: loss = 0.342527\n",
      "Iteration 37: loss = 0.342193\n",
      "Iteration 0: loss = 0.706480\n",
      "Iteration 1: loss = 0.438710\n",
      "Iteration 2: loss = 0.423410\n",
      "Iteration 3: loss = 0.419991\n",
      "Iteration 4: loss = 0.418794\n",
      "Iteration 5: loss = 0.418151\n",
      "Iteration 6: loss = 0.417667\n",
      "Iteration 7: loss = 0.417231\n",
      "Iteration 8: loss = 0.416812\n",
      "Iteration 9: loss = 0.416399\n",
      "Iteration 0: loss = 0.699900\n",
      "Iteration 1: loss = 0.423477\n",
      "Iteration 2: loss = 0.406375\n",
      "Iteration 3: loss = 0.402340\n",
      "Iteration 4: loss = 0.400924\n",
      "Iteration 5: loss = 0.400214\n",
      "Iteration 6: loss = 0.399719\n",
      "Iteration 7: loss = 0.399295\n",
      "Iteration 8: loss = 0.398896\n",
      "Iteration 0: loss = 1.792679\n",
      "Iteration 1: loss = 1.748889\n",
      "Iteration 2: loss = 1.725092\n",
      "Iteration 3: loss = 1.710647\n",
      "Iteration 4: loss = 1.701304\n",
      "Iteration 5: loss = 1.694917\n",
      "Iteration 6: loss = 1.690265\n",
      "Iteration 7: loss = 1.686628\n",
      "Iteration 8: loss = 1.683576\n",
      "Iteration 9: loss = 1.680849\n",
      "Iteration 10: loss = 1.678293\n",
      "Iteration 11: loss = 1.675811\n",
      "Iteration 12: loss = 1.673344\n",
      "Iteration 13: loss = 1.670851\n",
      "Iteration 14: loss = 1.668306\n",
      "Iteration 15: loss = 1.665692\n",
      "Iteration 16: loss = 1.662999\n",
      "Iteration 17: loss = 1.660221\n",
      "Iteration 18: loss = 1.657359\n",
      "Iteration 19: loss = 1.654412\n",
      "Iteration 20: loss = 1.651382\n",
      "Iteration 21: loss = 1.648267\n",
      "Iteration 22: loss = 1.645065\n",
      "Iteration 23: loss = 1.641775\n",
      "Iteration 24: loss = 1.638398\n",
      "Iteration 25: loss = 1.634932\n",
      "Iteration 26: loss = 1.631377\n",
      "Iteration 27: loss = 1.627727\n",
      "Iteration 28: loss = 1.623978\n",
      "Iteration 29: loss = 1.620128\n",
      "Iteration 30: loss = 1.616175\n",
      "Iteration 31: loss = 1.612116\n",
      "Iteration 32: loss = 1.607955\n",
      "Iteration 33: loss = 1.603697\n",
      "Iteration 34: loss = 1.599347\n",
      "Iteration 35: loss = 1.594901\n",
      "Iteration 36: loss = 1.590352\n",
      "Iteration 37: loss = 1.585699\n",
      "Iteration 38: loss = 1.580948\n",
      "Iteration 39: loss = 1.576103\n",
      "Iteration 40: loss = 1.571163\n",
      "Iteration 41: loss = 1.566125\n",
      "Iteration 42: loss = 1.560991\n",
      "Iteration 43: loss = 1.555781\n",
      "Iteration 44: loss = 1.550520\n",
      "Iteration 45: loss = 1.545221\n",
      "Iteration 46: loss = 1.539889\n",
      "Iteration 47: loss = 1.534533\n",
      "Iteration 48: loss = 1.529161\n",
      "Iteration 49: loss = 1.523782\n",
      "Iteration 50: loss = 1.518407\n",
      "Iteration 51: loss = 1.513045\n",
      "Iteration 52: loss = 1.507696\n",
      "Iteration 53: loss = 1.502354\n",
      "Iteration 54: loss = 1.497016\n",
      "Iteration 55: loss = 1.491683\n",
      "Iteration 56: loss = 1.486355\n",
      "Iteration 57: loss = 1.481036\n",
      "Iteration 58: loss = 1.475726\n",
      "Iteration 59: loss = 1.470425\n",
      "Iteration 60: loss = 1.465137\n",
      "Iteration 61: loss = 1.459873\n",
      "Iteration 62: loss = 1.454640\n",
      "Iteration 63: loss = 1.449444\n",
      "Iteration 64: loss = 1.444288\n",
      "Iteration 65: loss = 1.439174\n",
      "Iteration 66: loss = 1.434099\n",
      "Iteration 67: loss = 1.429065\n",
      "Iteration 68: loss = 1.424076\n",
      "Iteration 69: loss = 1.419138\n",
      "Iteration 70: loss = 1.414255\n",
      "Iteration 71: loss = 1.409419\n",
      "Iteration 72: loss = 1.404628\n",
      "Iteration 73: loss = 1.399879\n",
      "Iteration 74: loss = 1.395171\n",
      "Iteration 75: loss = 1.390504\n",
      "Iteration 76: loss = 1.385879\n",
      "Iteration 77: loss = 1.381298\n",
      "Iteration 78: loss = 1.376761\n",
      "Iteration 79: loss = 1.372268\n",
      "Iteration 80: loss = 1.367820\n",
      "Iteration 81: loss = 1.363419\n",
      "Iteration 82: loss = 1.359063\n",
      "Iteration 83: loss = 1.354753\n",
      "Iteration 84: loss = 1.350488\n",
      "Iteration 85: loss = 1.346267\n",
      "Iteration 86: loss = 1.342084\n",
      "Iteration 87: loss = 1.337937\n",
      "Iteration 88: loss = 1.333824\n",
      "Iteration 89: loss = 1.329743\n",
      "Iteration 90: loss = 1.325693\n",
      "Iteration 91: loss = 1.321676\n",
      "Iteration 92: loss = 1.317696\n",
      "Iteration 93: loss = 1.313751\n",
      "Iteration 94: loss = 1.309840\n",
      "Iteration 95: loss = 1.305960\n",
      "Iteration 96: loss = 1.302115\n",
      "Iteration 97: loss = 1.298304\n",
      "Iteration 98: loss = 1.294528\n",
      "Iteration 99: loss = 1.290783\n",
      "Iteration 100: loss = 1.287062\n",
      "Iteration 101: loss = 1.283361\n",
      "Iteration 102: loss = 1.279678\n",
      "Iteration 103: loss = 1.276014\n",
      "Iteration 104: loss = 1.272366\n",
      "Iteration 105: loss = 1.268730\n",
      "Iteration 106: loss = 1.265096\n",
      "Iteration 107: loss = 1.261462\n",
      "Iteration 108: loss = 1.257837\n",
      "Iteration 109: loss = 1.254223\n",
      "Iteration 110: loss = 1.250616\n",
      "Iteration 111: loss = 1.247008\n",
      "Iteration 112: loss = 1.243399\n",
      "Iteration 113: loss = 1.239768\n",
      "Iteration 114: loss = 1.236108\n",
      "Iteration 115: loss = 1.232456\n",
      "Iteration 116: loss = 1.229034\n",
      "Iteration 117: loss = 1.227095\n",
      "Iteration 118: loss = 1.224438\n",
      "Iteration 119: loss = 1.223512\n",
      "Iteration 0: loss = 1.791704\n",
      "Iteration 1: loss = 1.744449\n",
      "Iteration 2: loss = 1.704944\n",
      "Iteration 3: loss = 1.669471\n",
      "Iteration 4: loss = 1.636382\n",
      "Iteration 5: loss = 1.604875\n",
      "Iteration 6: loss = 1.574485\n",
      "Iteration 7: loss = 1.544895\n",
      "Iteration 8: loss = 1.515867\n",
      "Iteration 9: loss = 1.487214\n",
      "Iteration 10: loss = 1.458806\n",
      "Iteration 11: loss = 1.430584\n",
      "Iteration 12: loss = 1.402500\n",
      "Iteration 13: loss = 1.374533\n",
      "Iteration 14: loss = 1.346743\n",
      "Iteration 15: loss = 1.319202\n",
      "Iteration 16: loss = 1.291967\n",
      "Iteration 17: loss = 1.265103\n",
      "Iteration 18: loss = 1.238695\n",
      "Iteration 19: loss = 1.212810\n",
      "Iteration 20: loss = 1.187492\n",
      "Iteration 21: loss = 1.162817\n",
      "Iteration 22: loss = 1.138795\n",
      "Iteration 23: loss = 1.115412\n",
      "Iteration 24: loss = 1.092779\n",
      "Iteration 25: loss = 1.071030\n",
      "Iteration 26: loss = 1.050220\n",
      "Iteration 27: loss = 1.030418\n",
      "Iteration 28: loss = 1.011596\n",
      "Iteration 29: loss = 0.993731\n",
      "Iteration 30: loss = 0.976796\n",
      "Iteration 31: loss = 0.960762\n",
      "Iteration 32: loss = 0.945597\n",
      "Iteration 33: loss = 0.931277\n",
      "Iteration 34: loss = 0.917770\n",
      "Iteration 35: loss = 0.905033\n",
      "Iteration 36: loss = 0.893013\n",
      "Iteration 37: loss = 0.881638\n",
      "Iteration 38: loss = 0.870849\n",
      "Iteration 39: loss = 0.860623\n",
      "Iteration 40: loss = 0.850939\n",
      "Iteration 41: loss = 0.841770\n",
      "Iteration 42: loss = 0.833081\n",
      "Iteration 43: loss = 0.824830\n",
      "Iteration 44: loss = 0.816984\n",
      "Iteration 45: loss = 0.809511\n",
      "Iteration 46: loss = 0.802379\n",
      "Iteration 47: loss = 0.795559\n",
      "Iteration 48: loss = 0.789023\n",
      "Iteration 49: loss = 0.782749\n",
      "Iteration 50: loss = 0.776721\n",
      "Iteration 51: loss = 0.770933\n",
      "Iteration 52: loss = 0.765369\n",
      "Iteration 53: loss = 0.760012\n",
      "Iteration 54: loss = 0.754845\n",
      "Iteration 55: loss = 0.749853\n",
      "Iteration 56: loss = 0.745023\n",
      "Iteration 57: loss = 0.740342\n",
      "Iteration 58: loss = 0.735803\n",
      "Iteration 59: loss = 0.731399\n",
      "Iteration 60: loss = 0.727131\n",
      "Iteration 61: loss = 0.722996\n",
      "Iteration 62: loss = 0.718987\n",
      "Iteration 63: loss = 0.715093\n",
      "Iteration 64: loss = 0.711302\n",
      "Iteration 65: loss = 0.707606\n",
      "Iteration 66: loss = 0.703997\n",
      "Iteration 67: loss = 0.700475\n",
      "Iteration 68: loss = 0.697035\n",
      "Iteration 69: loss = 0.693676\n",
      "Iteration 70: loss = 0.690393\n",
      "Iteration 71: loss = 0.687183\n",
      "Iteration 72: loss = 0.684043\n",
      "Iteration 73: loss = 0.680973\n",
      "Iteration 74: loss = 0.677968\n",
      "Iteration 75: loss = 0.675027\n",
      "Iteration 76: loss = 0.672148\n",
      "Iteration 77: loss = 0.669328\n",
      "Iteration 78: loss = 0.666564\n",
      "Iteration 79: loss = 0.663853\n",
      "Iteration 80: loss = 0.661192\n",
      "Iteration 81: loss = 0.658577\n",
      "Iteration 82: loss = 0.656007\n",
      "Iteration 83: loss = 0.653478\n",
      "Iteration 84: loss = 0.650988\n",
      "Iteration 85: loss = 0.648536\n",
      "Iteration 86: loss = 0.646122\n",
      "Iteration 87: loss = 0.643744\n",
      "Iteration 88: loss = 0.641401\n",
      "Iteration 89: loss = 0.639095\n",
      "Iteration 90: loss = 0.636825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss = 0.685293\n",
      "Iteration 1: loss = 0.464300\n",
      "Iteration 2: loss = 0.463015\n",
      "Iteration 3: loss = 0.462588\n",
      "Iteration 0: loss = 0.677852\n",
      "Iteration 1: loss = 0.427902\n",
      "Iteration 2: loss = 0.413893\n",
      "Iteration 3: loss = 0.411049\n",
      "Iteration 4: loss = 0.410192\n",
      "Iteration 5: loss = 0.409808\n",
      "Iteration 0: loss = 0.709547\n",
      "Iteration 1: loss = 0.493075\n",
      "Iteration 2: loss = 0.416256\n",
      "Iteration 3: loss = 0.399908\n",
      "Iteration 4: loss = 0.398704\n",
      "Iteration 5: loss = 0.397857\n",
      "Iteration 6: loss = 0.397050\n",
      "Iteration 7: loss = 0.396283\n",
      "Iteration 8: loss = 0.395552\n",
      "Iteration 9: loss = 0.394856\n",
      "Iteration 10: loss = 0.394193\n",
      "Iteration 11: loss = 0.393562\n",
      "Iteration 12: loss = 0.392962\n",
      "Iteration 13: loss = 0.392390\n",
      "Iteration 14: loss = 0.391846\n",
      "Iteration 15: loss = 0.391328\n",
      "Iteration 16: loss = 0.390834\n",
      "Iteration 17: loss = 0.390364\n",
      "Iteration 18: loss = 0.389917\n",
      "Iteration 19: loss = 0.389491\n",
      "Iteration 20: loss = 0.389086\n",
      "Iteration 21: loss = 0.388700\n",
      "Iteration 0: loss = 0.696908\n",
      "Iteration 1: loss = 0.488458\n",
      "Iteration 2: loss = 0.428763\n",
      "Iteration 3: loss = 0.419065\n",
      "Iteration 4: loss = 0.416753\n",
      "Iteration 5: loss = 0.414595\n",
      "Iteration 6: loss = 0.412537\n",
      "Iteration 7: loss = 0.410574\n",
      "Iteration 8: loss = 0.408703\n",
      "Iteration 9: loss = 0.406918\n",
      "Iteration 10: loss = 0.405215\n",
      "Iteration 11: loss = 0.403590\n",
      "Iteration 12: loss = 0.402039\n",
      "Iteration 13: loss = 0.400557\n",
      "Iteration 14: loss = 0.399141\n",
      "Iteration 15: loss = 0.397786\n",
      "Iteration 16: loss = 0.396489\n",
      "Iteration 17: loss = 0.395246\n",
      "Iteration 18: loss = 0.394053\n",
      "Iteration 19: loss = 0.392909\n",
      "Iteration 20: loss = 0.391809\n",
      "Iteration 21: loss = 0.390750\n",
      "Iteration 22: loss = 0.389730\n",
      "Iteration 23: loss = 0.388747\n",
      "Iteration 24: loss = 0.387798\n",
      "Iteration 25: loss = 0.386881\n",
      "Iteration 26: loss = 0.385994\n",
      "Iteration 27: loss = 0.385135\n",
      "Iteration 28: loss = 0.384303\n",
      "Iteration 29: loss = 0.383495\n",
      "Iteration 30: loss = 0.382710\n",
      "Iteration 31: loss = 0.381947\n",
      "Iteration 32: loss = 0.381205\n",
      "Iteration 33: loss = 0.380482\n",
      "Iteration 34: loss = 0.379778\n",
      "Iteration 35: loss = 0.379091\n",
      "Iteration 36: loss = 0.378420\n",
      "Iteration 37: loss = 0.377766\n",
      "Iteration 38: loss = 0.377126\n",
      "Iteration 39: loss = 0.376500\n",
      "Iteration 40: loss = 0.375887\n",
      "Iteration 41: loss = 0.375288\n",
      "Iteration 42: loss = 0.374700\n",
      "Iteration 43: loss = 0.374125\n",
      "Iteration 44: loss = 0.373560\n",
      "Iteration 45: loss = 0.373006\n",
      "Iteration 46: loss = 0.372463\n",
      "Iteration 47: loss = 0.371930\n",
      "Iteration 48: loss = 0.371406\n",
      "Iteration 49: loss = 0.370891\n",
      "Iteration 50: loss = 0.370385\n",
      "Iteration 51: loss = 0.369888\n",
      "Iteration 52: loss = 0.369399\n",
      "Iteration 53: loss = 0.368918\n",
      "Iteration 54: loss = 0.368444\n",
      "Iteration 55: loss = 0.367979\n",
      "Iteration 56: loss = 0.367520\n",
      "Iteration 57: loss = 0.367068\n",
      "Iteration 58: loss = 0.366624\n",
      "Iteration 59: loss = 0.366186\n",
      "Iteration 60: loss = 0.365754\n",
      "Iteration 61: loss = 0.365328\n",
      "Iteration 62: loss = 0.364909\n",
      "Iteration 63: loss = 0.364496\n",
      "Iteration 64: loss = 0.364088\n",
      "Iteration 65: loss = 0.363686\n",
      "Iteration 66: loss = 0.363290\n",
      "Iteration 67: loss = 0.362899\n",
      "Iteration 68: loss = 0.362513\n",
      "Iteration 69: loss = 0.362133\n",
      "Iteration 70: loss = 0.361757\n",
      "Iteration 71: loss = 0.361386\n",
      "Iteration 72: loss = 0.361020\n",
      "Iteration 73: loss = 0.360659\n",
      "Iteration 74: loss = 0.360302\n",
      "Iteration 0: loss = 1.793568\n",
      "Iteration 1: loss = 1.677833\n",
      "Iteration 2: loss = 1.627767\n",
      "Iteration 3: loss = 1.600616\n",
      "Iteration 4: loss = 1.585206\n",
      "Iteration 5: loss = 1.575916\n",
      "Iteration 6: loss = 1.569856\n",
      "Iteration 7: loss = 1.565518\n",
      "Iteration 8: loss = 1.562105\n",
      "Iteration 9: loss = 1.559191\n",
      "Iteration 10: loss = 1.556545\n",
      "Iteration 11: loss = 1.554040\n",
      "Iteration 12: loss = 1.551608\n",
      "Iteration 13: loss = 1.549209\n",
      "Iteration 14: loss = 1.546821\n",
      "Iteration 15: loss = 1.544429\n",
      "Iteration 16: loss = 1.542025\n",
      "Iteration 17: loss = 1.539602\n",
      "Iteration 18: loss = 1.537156\n",
      "Iteration 19: loss = 1.534686\n",
      "Iteration 20: loss = 1.532189\n",
      "Iteration 21: loss = 1.529663\n",
      "Iteration 22: loss = 1.527107\n",
      "Iteration 23: loss = 1.524520\n",
      "Iteration 24: loss = 1.521902\n",
      "Iteration 25: loss = 1.519252\n",
      "Iteration 26: loss = 1.516570\n",
      "Iteration 27: loss = 1.513854\n",
      "Iteration 28: loss = 1.511105\n",
      "Iteration 29: loss = 1.508320\n",
      "Iteration 30: loss = 1.505500\n",
      "Iteration 31: loss = 1.502641\n",
      "Iteration 32: loss = 1.499744\n",
      "Iteration 33: loss = 1.496807\n",
      "Iteration 34: loss = 1.493827\n",
      "Iteration 35: loss = 1.490805\n",
      "Iteration 36: loss = 1.487740\n",
      "Iteration 37: loss = 1.484632\n",
      "Iteration 38: loss = 1.481484\n",
      "Iteration 39: loss = 1.478295\n",
      "Iteration 40: loss = 1.475067\n",
      "Iteration 41: loss = 1.471799\n",
      "Iteration 42: loss = 1.468492\n",
      "Iteration 43: loss = 1.465145\n",
      "Iteration 44: loss = 1.461756\n",
      "Iteration 45: loss = 1.458327\n",
      "Iteration 46: loss = 1.454859\n",
      "Iteration 47: loss = 1.451353\n",
      "Iteration 48: loss = 1.447813\n",
      "Iteration 49: loss = 1.444240\n",
      "Iteration 50: loss = 1.440635\n",
      "Iteration 51: loss = 1.436998\n",
      "Iteration 52: loss = 1.433328\n",
      "Iteration 53: loss = 1.429624\n",
      "Iteration 54: loss = 1.425890\n",
      "Iteration 55: loss = 1.422127\n",
      "Iteration 56: loss = 1.418338\n",
      "Iteration 57: loss = 1.414522\n",
      "Iteration 58: loss = 1.410682\n",
      "Iteration 59: loss = 1.406816\n",
      "Iteration 60: loss = 1.402927\n",
      "Iteration 61: loss = 1.399019\n",
      "Iteration 62: loss = 1.395097\n",
      "Iteration 63: loss = 1.391166\n",
      "Iteration 64: loss = 1.387229\n",
      "Iteration 65: loss = 1.383289\n",
      "Iteration 66: loss = 1.379348\n",
      "Iteration 67: loss = 1.375410\n",
      "Iteration 68: loss = 1.371475\n",
      "Iteration 69: loss = 1.367545\n",
      "Iteration 70: loss = 1.363623\n",
      "Iteration 71: loss = 1.359708\n",
      "Iteration 72: loss = 1.355803\n",
      "Iteration 73: loss = 1.351910\n",
      "Iteration 74: loss = 1.348030\n",
      "Iteration 75: loss = 1.344168\n",
      "Iteration 76: loss = 1.340328\n",
      "Iteration 77: loss = 1.336513\n",
      "Iteration 78: loss = 1.332721\n",
      "Iteration 79: loss = 1.328949\n",
      "Iteration 80: loss = 1.325195\n",
      "Iteration 81: loss = 1.321460\n",
      "Iteration 82: loss = 1.317744\n",
      "Iteration 83: loss = 1.314050\n",
      "Iteration 84: loss = 1.310379\n",
      "Iteration 85: loss = 1.306731\n",
      "Iteration 86: loss = 1.303107\n",
      "Iteration 87: loss = 1.299508\n",
      "Iteration 88: loss = 1.295936\n",
      "Iteration 89: loss = 1.292391\n",
      "Iteration 90: loss = 1.288875\n",
      "Iteration 91: loss = 1.285388\n",
      "Iteration 92: loss = 1.281931\n",
      "Iteration 93: loss = 1.278506\n",
      "Iteration 94: loss = 1.275113\n",
      "Iteration 95: loss = 1.271753\n",
      "Iteration 96: loss = 1.268427\n",
      "Iteration 97: loss = 1.265135\n",
      "Iteration 98: loss = 1.261878\n",
      "Iteration 99: loss = 1.258656\n",
      "Iteration 100: loss = 1.255469\n",
      "Iteration 101: loss = 1.252317\n",
      "Iteration 102: loss = 1.249199\n",
      "Iteration 103: loss = 1.246113\n",
      "Iteration 104: loss = 1.243058\n",
      "Iteration 105: loss = 1.240034\n",
      "Iteration 106: loss = 1.237040\n",
      "Iteration 107: loss = 1.234077\n",
      "Iteration 108: loss = 1.231144\n",
      "Iteration 109: loss = 1.228242\n",
      "Iteration 110: loss = 1.225369\n",
      "Iteration 111: loss = 1.222525\n",
      "Iteration 112: loss = 1.219711\n",
      "Iteration 113: loss = 1.216927\n",
      "Iteration 114: loss = 1.214171\n",
      "Iteration 115: loss = 1.211444\n",
      "Iteration 116: loss = 1.208747\n",
      "Iteration 117: loss = 1.206079\n",
      "Iteration 118: loss = 1.203440\n",
      "Iteration 119: loss = 1.200829\n",
      "Iteration 120: loss = 1.198247\n",
      "Iteration 121: loss = 1.195692\n",
      "Iteration 122: loss = 1.193164\n",
      "Iteration 123: loss = 1.190662\n",
      "Iteration 124: loss = 1.188184\n",
      "Iteration 125: loss = 1.185732\n",
      "Iteration 126: loss = 1.183310\n",
      "Iteration 127: loss = 1.180934\n",
      "Iteration 128: loss = 1.178687\n",
      "Iteration 129: loss = 1.176959\n",
      "Iteration 130: loss = 1.175810\n",
      "Iteration 0: loss = 1.793301\n",
      "Iteration 1: loss = 1.680918\n",
      "Iteration 2: loss = 1.592681\n",
      "Iteration 3: loss = 1.516059\n",
      "Iteration 4: loss = 1.448450\n",
      "Iteration 5: loss = 1.388426\n",
      "Iteration 6: loss = 1.334873\n",
      "Iteration 7: loss = 1.286879\n",
      "Iteration 8: loss = 1.243686\n",
      "Iteration 9: loss = 1.204655\n",
      "Iteration 10: loss = 1.169241\n",
      "Iteration 11: loss = 1.136980\n",
      "Iteration 12: loss = 1.107478\n",
      "Iteration 13: loss = 1.080407\n",
      "Iteration 14: loss = 1.055494\n",
      "Iteration 15: loss = 1.032497\n",
      "Iteration 16: loss = 1.011195\n",
      "Iteration 17: loss = 0.991392\n",
      "Iteration 18: loss = 0.972917\n",
      "Iteration 19: loss = 0.955632\n",
      "Iteration 20: loss = 0.939423\n",
      "Iteration 21: loss = 0.924214\n",
      "Iteration 22: loss = 0.909926\n",
      "Iteration 23: loss = 0.896485\n",
      "Iteration 24: loss = 0.883818\n",
      "Iteration 25: loss = 0.871859\n",
      "Iteration 26: loss = 0.860557\n",
      "Iteration 27: loss = 0.849871\n",
      "Iteration 28: loss = 0.839774\n",
      "Iteration 29: loss = 0.830220\n",
      "Iteration 30: loss = 0.821152\n",
      "Iteration 31: loss = 0.812523\n",
      "Iteration 32: loss = 0.804296\n",
      "Iteration 33: loss = 0.796442\n",
      "Iteration 34: loss = 0.788937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss = 0.697851\n",
      "Iteration 1: loss = 0.566097\n",
      "Iteration 2: loss = 0.453172\n",
      "Iteration 3: loss = 0.450571\n",
      "Iteration 4: loss = 0.449055\n",
      "Iteration 5: loss = 0.448161\n",
      "Iteration 6: loss = 0.447632\n",
      "Iteration 7: loss = 0.447318\n",
      "Iteration 0: loss = 0.691184\n",
      "Iteration 1: loss = 0.521911\n",
      "Iteration 2: loss = 0.432839\n",
      "Iteration 3: loss = 0.420718\n",
      "Iteration 4: loss = 0.419941\n",
      "Iteration 5: loss = 0.419204\n",
      "Iteration 6: loss = 0.418503\n",
      "Iteration 7: loss = 0.417838\n",
      "Iteration 8: loss = 0.417206\n",
      "Iteration 9: loss = 0.416605\n",
      "Iteration 10: loss = 0.416035\n",
      "Iteration 11: loss = 0.415493\n",
      "Iteration 12: loss = 0.414978\n",
      "Iteration 13: loss = 0.414489\n",
      "Iteration 14: loss = 0.414024\n",
      "Iteration 15: loss = 0.413582\n",
      "Iteration 16: loss = 0.413162\n",
      "Iteration 17: loss = 0.412763\n",
      "Iteration 0: loss = 0.701385\n",
      "Iteration 1: loss = 0.422807\n",
      "Iteration 2: loss = 0.418165\n",
      "Iteration 3: loss = 0.415416\n",
      "Iteration 4: loss = 0.413005\n",
      "Iteration 5: loss = 0.410696\n",
      "Iteration 6: loss = 0.408446\n",
      "Iteration 7: loss = 0.406247\n",
      "Iteration 8: loss = 0.404096\n",
      "Iteration 9: loss = 0.401993\n",
      "Iteration 10: loss = 0.399937\n",
      "Iteration 11: loss = 0.397928\n",
      "Iteration 12: loss = 0.395964\n",
      "Iteration 13: loss = 0.394046\n",
      "Iteration 14: loss = 0.392172\n",
      "Iteration 15: loss = 0.390341\n",
      "Iteration 16: loss = 0.388554\n",
      "Iteration 17: loss = 0.386809\n",
      "Iteration 18: loss = 0.385105\n",
      "Iteration 19: loss = 0.383442\n",
      "Iteration 20: loss = 0.381819\n",
      "Iteration 21: loss = 0.380235\n",
      "Iteration 22: loss = 0.378690\n",
      "Iteration 23: loss = 0.377181\n",
      "Iteration 24: loss = 0.375709\n",
      "Iteration 25: loss = 0.374273\n",
      "Iteration 26: loss = 0.372872\n",
      "Iteration 27: loss = 0.371505\n",
      "Iteration 28: loss = 0.370171\n",
      "Iteration 29: loss = 0.368870\n",
      "Iteration 30: loss = 0.367600\n",
      "Iteration 31: loss = 0.366360\n",
      "Iteration 32: loss = 0.365151\n",
      "Iteration 33: loss = 0.363971\n",
      "Iteration 34: loss = 0.362819\n",
      "Iteration 35: loss = 0.361695\n",
      "Iteration 36: loss = 0.360597\n",
      "Iteration 37: loss = 0.359526\n",
      "Iteration 38: loss = 0.358479\n",
      "Iteration 39: loss = 0.357458\n",
      "Iteration 40: loss = 0.356460\n",
      "Iteration 41: loss = 0.355485\n",
      "Iteration 42: loss = 0.354533\n",
      "Iteration 43: loss = 0.353603\n",
      "Iteration 44: loss = 0.352693\n",
      "Iteration 45: loss = 0.351805\n",
      "Iteration 46: loss = 0.350936\n",
      "Iteration 47: loss = 0.350087\n",
      "Iteration 48: loss = 0.349256\n",
      "Iteration 49: loss = 0.348444\n",
      "Iteration 50: loss = 0.347649\n",
      "Iteration 51: loss = 0.346872\n",
      "Iteration 52: loss = 0.346111\n",
      "Iteration 53: loss = 0.345366\n",
      "Iteration 54: loss = 0.344637\n",
      "Iteration 55: loss = 0.343923\n",
      "Iteration 56: loss = 0.343224\n",
      "Iteration 57: loss = 0.342539\n",
      "Iteration 58: loss = 0.341868\n",
      "Iteration 59: loss = 0.341210\n",
      "Iteration 60: loss = 0.340566\n",
      "Iteration 61: loss = 0.339934\n",
      "Iteration 62: loss = 0.339315\n",
      "Iteration 63: loss = 0.338707\n",
      "Iteration 64: loss = 0.338111\n",
      "Iteration 65: loss = 0.337527\n",
      "Iteration 66: loss = 0.336953\n",
      "Iteration 67: loss = 0.336390\n",
      "Iteration 68: loss = 0.335837\n",
      "Iteration 69: loss = 0.335294\n",
      "Iteration 70: loss = 0.334761\n",
      "Iteration 71: loss = 0.334238\n",
      "Iteration 72: loss = 0.333724\n",
      "Iteration 73: loss = 0.333219\n",
      "Iteration 74: loss = 0.332722\n",
      "Iteration 75: loss = 0.332234\n",
      "Iteration 76: loss = 0.331754\n",
      "Iteration 77: loss = 0.331282\n",
      "Iteration 78: loss = 0.330818\n",
      "Iteration 79: loss = 0.330362\n",
      "Iteration 80: loss = 0.329913\n",
      "Iteration 81: loss = 0.329471\n",
      "Iteration 82: loss = 0.329036\n",
      "Iteration 83: loss = 0.328609\n",
      "Iteration 84: loss = 0.328187\n",
      "Iteration 85: loss = 0.327773\n",
      "Iteration 86: loss = 0.327364\n",
      "Iteration 87: loss = 0.326962\n",
      "Iteration 88: loss = 0.326566\n",
      "Iteration 89: loss = 0.326176\n",
      "Iteration 90: loss = 0.325792\n",
      "Iteration 91: loss = 0.325413\n",
      "Iteration 92: loss = 0.325040\n",
      "Iteration 93: loss = 0.324672\n",
      "Iteration 94: loss = 0.324309\n",
      "Iteration 95: loss = 0.323951\n",
      "Iteration 96: loss = 0.323599\n",
      "Iteration 97: loss = 0.323251\n",
      "Iteration 98: loss = 0.322909\n",
      "Iteration 99: loss = 0.322570\n",
      "Iteration 100: loss = 0.322237\n",
      "Iteration 101: loss = 0.321908\n",
      "Iteration 102: loss = 0.321583\n",
      "Iteration 103: loss = 0.321263\n",
      "Iteration 0: loss = 1.795488\n",
      "Iteration 1: loss = 1.693634\n",
      "Iteration 2: loss = 1.640516\n",
      "Iteration 3: loss = 1.609515\n",
      "Iteration 4: loss = 1.590699\n",
      "Iteration 5: loss = 1.578622\n",
      "Iteration 6: loss = 1.570259\n",
      "Iteration 7: loss = 1.563931\n",
      "Iteration 8: loss = 1.558705\n",
      "Iteration 9: loss = 1.554063\n",
      "Iteration 10: loss = 1.549721\n",
      "Iteration 11: loss = 1.545516\n",
      "Iteration 12: loss = 1.541356\n",
      "Iteration 13: loss = 1.537188\n",
      "Iteration 14: loss = 1.532981\n",
      "Iteration 15: loss = 1.528720\n",
      "Iteration 16: loss = 1.524400\n",
      "Iteration 17: loss = 1.520017\n",
      "Iteration 18: loss = 1.515560\n",
      "Iteration 19: loss = 1.511020\n",
      "Iteration 20: loss = 1.506387\n",
      "Iteration 21: loss = 1.501657\n",
      "Iteration 22: loss = 1.496846\n",
      "Iteration 23: loss = 1.491967\n",
      "Iteration 24: loss = 1.487028\n",
      "Iteration 25: loss = 1.482034\n",
      "Iteration 26: loss = 1.476983\n",
      "Iteration 27: loss = 1.471872\n",
      "Iteration 28: loss = 1.466695\n",
      "Iteration 29: loss = 1.461440\n",
      "Iteration 30: loss = 1.456083\n",
      "Iteration 31: loss = 1.450654\n",
      "Iteration 32: loss = 1.445179\n",
      "Iteration 33: loss = 1.439640\n",
      "Iteration 34: loss = 1.434020\n",
      "Iteration 35: loss = 1.428318\n",
      "Iteration 36: loss = 1.422546\n",
      "Iteration 37: loss = 1.416730\n",
      "Iteration 38: loss = 1.410880\n",
      "Iteration 39: loss = 1.404974\n",
      "Iteration 40: loss = 1.399003\n",
      "Iteration 41: loss = 1.392973\n",
      "Iteration 42: loss = 1.386883\n",
      "Iteration 43: loss = 1.380721\n",
      "Iteration 44: loss = 1.374481\n",
      "Iteration 45: loss = 1.368227\n",
      "Iteration 46: loss = 1.361920\n",
      "Iteration 47: loss = 1.355513\n",
      "Iteration 48: loss = 1.349103\n",
      "Iteration 49: loss = 1.342722\n",
      "Iteration 50: loss = 1.336356\n",
      "Iteration 51: loss = 1.329967\n",
      "Iteration 52: loss = 1.323592\n",
      "Iteration 53: loss = 1.317251\n",
      "Iteration 54: loss = 1.310944\n",
      "Iteration 55: loss = 1.304693\n",
      "Iteration 56: loss = 1.298525\n",
      "Iteration 57: loss = 1.292349\n",
      "Iteration 58: loss = 1.286216\n",
      "Iteration 59: loss = 1.280152\n",
      "Iteration 60: loss = 1.274166\n",
      "Iteration 61: loss = 1.268277\n",
      "Iteration 62: loss = 1.262502\n",
      "Iteration 63: loss = 1.256843\n",
      "Iteration 64: loss = 1.251297\n",
      "Iteration 65: loss = 1.245858\n",
      "Iteration 66: loss = 1.240524\n",
      "Iteration 67: loss = 1.235299\n",
      "Iteration 68: loss = 1.230181\n",
      "Iteration 69: loss = 1.225168\n",
      "Iteration 70: loss = 1.220252\n",
      "Iteration 71: loss = 1.215434\n",
      "Iteration 72: loss = 1.210719\n",
      "Iteration 73: loss = 1.206099\n",
      "Iteration 74: loss = 1.201596\n",
      "Iteration 75: loss = 1.197220\n",
      "Iteration 76: loss = 1.193322\n",
      "Iteration 77: loss = 1.191341\n",
      "Iteration 78: loss = 1.187994\n",
      "Iteration 79: loss = 1.186845\n",
      "Iteration 0: loss = 1.801083\n",
      "Iteration 1: loss = 1.666056\n",
      "Iteration 2: loss = 1.575529\n",
      "Iteration 3: loss = 1.501578\n",
      "Iteration 4: loss = 1.439834\n",
      "Iteration 5: loss = 1.387413\n",
      "Iteration 6: loss = 1.342215\n",
      "Iteration 7: loss = 1.302687\n",
      "Iteration 8: loss = 1.267673\n",
      "Iteration 9: loss = 1.236299\n",
      "Iteration 10: loss = 1.207891\n",
      "Iteration 11: loss = 1.181928\n",
      "Iteration 12: loss = 1.158003\n",
      "Iteration 13: loss = 1.135794\n",
      "Iteration 14: loss = 1.115043\n",
      "Iteration 15: loss = 1.095534\n",
      "Iteration 16: loss = 1.077091\n",
      "Iteration 17: loss = 1.059567\n",
      "Iteration 18: loss = 1.042844\n",
      "Iteration 19: loss = 1.026827\n",
      "Iteration 20: loss = 1.011435\n",
      "Iteration 21: loss = 0.996604\n",
      "Iteration 22: loss = 0.982283\n",
      "Iteration 23: loss = 0.968432\n",
      "Iteration 24: loss = 0.955019\n",
      "Iteration 25: loss = 0.942026\n",
      "Iteration 26: loss = 0.929442\n",
      "Iteration 27: loss = 0.917243\n",
      "Iteration 28: loss = 0.905407\n",
      "Iteration 29: loss = 0.893924\n",
      "Iteration 30: loss = 0.882796\n",
      "Iteration 31: loss = 0.872017\n",
      "Iteration 32: loss = 0.861567\n",
      "Iteration 33: loss = 0.851431\n",
      "Iteration 34: loss = 0.841598\n",
      "Iteration 35: loss = 0.832062\n",
      "Iteration 36: loss = 0.822820\n",
      "Iteration 37: loss = 0.813880\n",
      "Iteration 38: loss = 0.805243\n",
      "Iteration 39: loss = 0.796900\n",
      "Iteration 40: loss = 0.788840\n",
      "Iteration 41: loss = 0.781053\n",
      "Iteration 42: loss = 0.773532\n",
      "Iteration 43: loss = 0.766272\n",
      "Iteration 44: loss = 0.759262\n",
      "Iteration 45: loss = 0.752493\n",
      "Iteration 46: loss = 0.745956\n",
      "Iteration 47: loss = 0.739644\n",
      "Iteration 48: loss = 0.733551\n",
      "Iteration 49: loss = 0.727671\n",
      "Iteration 50: loss = 0.721996\n",
      "Iteration 51: loss = 0.716519\n",
      "Iteration 52: loss = 0.711231\n",
      "Iteration 53: loss = 0.706124\n",
      "Iteration 54: loss = 0.701190\n",
      "Iteration 55: loss = 0.696421\n",
      "Iteration 56: loss = 0.691809\n",
      "Iteration 57: loss = 0.687347\n",
      "Iteration 58: loss = 0.683027\n",
      "Iteration 59: loss = 0.678843\n",
      "Iteration 60: loss = 0.674787\n",
      "Iteration 61: loss = 0.670853\n",
      "Iteration 62: loss = 0.667037\n",
      "Iteration 63: loss = 0.663334\n",
      "Iteration 0: loss = 0.693826\n",
      "Iteration 1: loss = 0.580275\n",
      "Iteration 2: loss = 0.427249\n",
      "Iteration 3: loss = 0.424492\n",
      "Iteration 4: loss = 0.424063\n",
      "Iteration 5: loss = 0.423905\n",
      "Iteration 0: loss = 0.707740\n",
      "Iteration 1: loss = 0.479498\n",
      "Iteration 2: loss = 0.431599\n",
      "Iteration 3: loss = 0.424083\n",
      "Iteration 4: loss = 0.421387\n",
      "Iteration 5: loss = 0.418928\n",
      "Iteration 6: loss = 0.416650\n",
      "Iteration 7: loss = 0.414541\n",
      "Iteration 8: loss = 0.412587\n",
      "Iteration 9: loss = 0.410779\n",
      "Iteration 10: loss = 0.409103\n",
      "Iteration 11: loss = 0.407551\n",
      "Iteration 12: loss = 0.406112\n",
      "Iteration 13: loss = 0.404778\n",
      "Iteration 14: loss = 0.403540\n",
      "Iteration 15: loss = 0.402392\n",
      "Iteration 16: loss = 0.401325\n",
      "Iteration 17: loss = 0.400335\n",
      "Iteration 18: loss = 0.399413\n",
      "Iteration 19: loss = 0.398556\n",
      "Iteration 20: loss = 0.397757\n",
      "Iteration 21: loss = 0.397014\n",
      "Iteration 22: loss = 0.396320\n",
      "Iteration 23: loss = 0.395672\n",
      "Iteration 24: loss = 0.395068\n",
      "Iteration 25: loss = 0.394503\n",
      "Iteration 26: loss = 0.393974\n",
      "Iteration 27: loss = 0.393479\n",
      "Iteration 28: loss = 0.393015\n",
      "Iteration 29: loss = 0.392581\n",
      "Iteration 30: loss = 0.392173\n",
      "Iteration 31: loss = 0.391790\n",
      "Iteration 0: loss = 0.690024\n",
      "Iteration 1: loss = 0.514237\n",
      "Iteration 2: loss = 0.431923\n",
      "Iteration 3: loss = 0.418777\n",
      "Iteration 4: loss = 0.417848\n",
      "Iteration 5: loss = 0.416955\n",
      "Iteration 6: loss = 0.416077\n",
      "Iteration 7: loss = 0.415215\n",
      "Iteration 8: loss = 0.414367\n",
      "Iteration 9: loss = 0.413534\n",
      "Iteration 10: loss = 0.412716\n",
      "Iteration 11: loss = 0.411911\n",
      "Iteration 12: loss = 0.411119\n",
      "Iteration 13: loss = 0.410341\n",
      "Iteration 14: loss = 0.409576\n",
      "Iteration 15: loss = 0.408824\n",
      "Iteration 16: loss = 0.408084\n",
      "Iteration 17: loss = 0.407356\n",
      "Iteration 18: loss = 0.406640\n",
      "Iteration 19: loss = 0.405936\n",
      "Iteration 20: loss = 0.405243\n",
      "Iteration 21: loss = 0.404561\n",
      "Iteration 22: loss = 0.403890\n",
      "Iteration 23: loss = 0.403230\n",
      "Iteration 24: loss = 0.402581\n",
      "Iteration 25: loss = 0.401941\n",
      "Iteration 26: loss = 0.401311\n",
      "Iteration 27: loss = 0.400692\n",
      "Iteration 28: loss = 0.400081\n",
      "Iteration 29: loss = 0.399480\n",
      "Iteration 30: loss = 0.398889\n",
      "Iteration 31: loss = 0.398306\n",
      "Iteration 32: loss = 0.397732\n",
      "Iteration 33: loss = 0.397166\n",
      "Iteration 34: loss = 0.396609\n",
      "Iteration 35: loss = 0.396060\n",
      "Iteration 36: loss = 0.395519\n",
      "Iteration 37: loss = 0.394986\n",
      "Iteration 38: loss = 0.394460\n",
      "Iteration 39: loss = 0.393942\n",
      "Iteration 40: loss = 0.393432\n",
      "Iteration 41: loss = 0.392929\n",
      "Iteration 42: loss = 0.392432\n",
      "Iteration 43: loss = 0.391943\n",
      "Iteration 44: loss = 0.391461\n",
      "Iteration 45: loss = 0.390985\n",
      "Iteration 46: loss = 0.390515\n",
      "Iteration 47: loss = 0.390052\n",
      "Iteration 48: loss = 0.389595\n",
      "Iteration 49: loss = 0.389145\n",
      "Iteration 50: loss = 0.388700\n",
      "Iteration 51: loss = 0.388261\n",
      "Iteration 52: loss = 0.387828\n",
      "Iteration 53: loss = 0.387400\n",
      "Iteration 54: loss = 0.386978\n",
      "Iteration 55: loss = 0.386561\n",
      "Iteration 56: loss = 0.386150\n",
      "Iteration 57: loss = 0.385744\n",
      "Iteration 58: loss = 0.385343\n",
      "Iteration 59: loss = 0.384947\n",
      "Iteration 60: loss = 0.384556\n",
      "Iteration 61: loss = 0.384169\n",
      "Iteration 62: loss = 0.383788\n",
      "Iteration 0: loss = 1.803327\n",
      "Iteration 1: loss = 1.688197\n",
      "Iteration 2: loss = 1.630220\n",
      "Iteration 3: loss = 1.596676\n",
      "Iteration 4: loss = 1.576477\n",
      "Iteration 5: loss = 1.563604\n",
      "Iteration 6: loss = 1.554744\n",
      "Iteration 7: loss = 1.548074\n",
      "Iteration 8: loss = 1.542591\n",
      "Iteration 9: loss = 1.537740\n",
      "Iteration 10: loss = 1.533214\n",
      "Iteration 11: loss = 1.528844\n",
      "Iteration 12: loss = 1.524536\n",
      "Iteration 13: loss = 1.520236\n",
      "Iteration 14: loss = 1.515912\n",
      "Iteration 15: loss = 1.511542\n",
      "Iteration 16: loss = 1.507112\n",
      "Iteration 17: loss = 1.502612\n",
      "Iteration 18: loss = 1.498040\n",
      "Iteration 19: loss = 1.493395\n",
      "Iteration 20: loss = 1.488674\n",
      "Iteration 21: loss = 1.483871\n",
      "Iteration 22: loss = 1.478983\n",
      "Iteration 23: loss = 1.474006\n",
      "Iteration 24: loss = 1.468936\n",
      "Iteration 25: loss = 1.463770\n",
      "Iteration 26: loss = 1.458510\n",
      "Iteration 27: loss = 1.453173\n",
      "Iteration 28: loss = 1.447777\n",
      "Iteration 29: loss = 1.442327\n",
      "Iteration 30: loss = 1.436822\n",
      "Iteration 31: loss = 1.431275\n",
      "Iteration 32: loss = 1.425700\n",
      "Iteration 33: loss = 1.420095\n",
      "Iteration 34: loss = 1.414453\n",
      "Iteration 35: loss = 1.408772\n",
      "Iteration 36: loss = 1.403063\n",
      "Iteration 37: loss = 1.397328\n",
      "Iteration 38: loss = 1.391572\n",
      "Iteration 39: loss = 1.385803\n",
      "Iteration 40: loss = 1.380017\n",
      "Iteration 41: loss = 1.374207\n",
      "Iteration 42: loss = 1.368362\n",
      "Iteration 43: loss = 1.362474\n",
      "Iteration 44: loss = 1.356541\n",
      "Iteration 45: loss = 1.350586\n",
      "Iteration 46: loss = 1.344599\n",
      "Iteration 47: loss = 1.338610\n",
      "Iteration 48: loss = 1.332622\n",
      "Iteration 49: loss = 1.326637\n",
      "Iteration 50: loss = 1.320610\n",
      "Iteration 51: loss = 1.314509\n",
      "Iteration 52: loss = 1.308427\n",
      "Iteration 53: loss = 1.302369\n",
      "Iteration 54: loss = 1.296324\n",
      "Iteration 55: loss = 1.290315\n",
      "Iteration 56: loss = 1.284339\n",
      "Iteration 57: loss = 1.278381\n",
      "Iteration 58: loss = 1.272459\n",
      "Iteration 59: loss = 1.266587\n",
      "Iteration 60: loss = 1.260734\n",
      "Iteration 61: loss = 1.254921\n",
      "Iteration 62: loss = 1.249183\n",
      "Iteration 63: loss = 1.243509\n",
      "Iteration 64: loss = 1.237817\n",
      "Iteration 65: loss = 1.232204\n",
      "Iteration 66: loss = 1.226687\n",
      "Iteration 67: loss = 1.221273\n",
      "Iteration 68: loss = 1.215967\n",
      "Iteration 69: loss = 1.210766\n",
      "Iteration 70: loss = 1.205668\n",
      "Iteration 71: loss = 1.200667\n",
      "Iteration 72: loss = 1.195758\n",
      "Iteration 73: loss = 1.190930\n",
      "Iteration 74: loss = 1.186172\n",
      "Iteration 75: loss = 1.181508\n",
      "Iteration 76: loss = 1.176958\n",
      "Iteration 77: loss = 1.172727\n",
      "Iteration 78: loss = 1.169540\n",
      "Iteration 79: loss = 1.167327\n",
      "Iteration 80: loss = 1.166292\n",
      "Iteration 0: loss = 1.788320\n",
      "Iteration 1: loss = 1.663834\n",
      "Iteration 2: loss = 1.576452\n",
      "Iteration 3: loss = 1.504429\n",
      "Iteration 4: loss = 1.443915\n",
      "Iteration 5: loss = 1.392269\n",
      "Iteration 6: loss = 1.347543\n",
      "Iteration 7: loss = 1.308287\n",
      "Iteration 8: loss = 1.273414\n",
      "Iteration 9: loss = 1.242102\n",
      "Iteration 10: loss = 1.213718\n",
      "Iteration 11: loss = 1.187763\n",
      "Iteration 12: loss = 1.163843\n",
      "Iteration 13: loss = 1.141640\n",
      "Iteration 14: loss = 1.120903\n",
      "Iteration 15: loss = 1.101423\n",
      "Iteration 16: loss = 1.083028\n",
      "Iteration 17: loss = 1.065578\n",
      "Iteration 18: loss = 1.048960\n",
      "Iteration 19: loss = 1.033080\n",
      "Iteration 20: loss = 1.017867\n",
      "Iteration 21: loss = 1.003263\n",
      "Iteration 22: loss = 0.989222\n",
      "Iteration 23: loss = 0.975704\n",
      "Iteration 24: loss = 0.962668\n",
      "Iteration 25: loss = 0.950078\n",
      "Iteration 26: loss = 0.937896\n",
      "Iteration 27: loss = 0.926097\n",
      "Iteration 28: loss = 0.914660\n",
      "Iteration 29: loss = 0.903565\n",
      "Iteration 30: loss = 0.892796\n",
      "Iteration 31: loss = 0.882341\n",
      "Iteration 32: loss = 0.872192\n",
      "Iteration 33: loss = 0.862347\n",
      "Iteration 34: loss = 0.852800\n",
      "Iteration 35: loss = 0.843538\n",
      "Iteration 36: loss = 0.834552\n",
      "Iteration 37: loss = 0.825837\n",
      "Iteration 38: loss = 0.817392\n",
      "Iteration 39: loss = 0.809210\n",
      "Iteration 40: loss = 0.801280\n",
      "Iteration 41: loss = 0.793594\n",
      "Iteration 42: loss = 0.786144\n",
      "Iteration 43: loss = 0.778927\n",
      "Iteration 44: loss = 0.771937\n",
      "Iteration 45: loss = 0.765169\n",
      "Iteration 46: loss = 0.758621\n",
      "Iteration 47: loss = 0.752290\n",
      "Iteration 48: loss = 0.746169\n",
      "Iteration 49: loss = 0.740253\n",
      "Iteration 50: loss = 0.734534\n",
      "Iteration 51: loss = 0.729003\n",
      "Iteration 52: loss = 0.723653\n",
      "Iteration 53: loss = 0.718478\n",
      "Iteration 54: loss = 0.713470\n",
      "Iteration 55: loss = 0.708629\n",
      "Iteration 56: loss = 0.703956\n",
      "Iteration 57: loss = 0.699442\n",
      "Iteration 58: loss = 0.695073\n",
      "Iteration 59: loss = 0.690838\n",
      "Iteration 60: loss = 0.686731\n",
      "Iteration 61: loss = 0.682746\n",
      "Iteration 62: loss = 0.678878\n",
      "Iteration 63: loss = 0.675123\n",
      "Iteration 64: loss = 0.671476\n",
      "Iteration 65: loss = 0.667931\n",
      "Iteration 66: loss = 0.664484\n",
      "Iteration 67: loss = 0.661131\n",
      "Iteration 68: loss = 0.657866\n",
      "Iteration 69: loss = 0.654685\n",
      "Iteration 70: loss = 0.651586\n",
      "Iteration 71: loss = 0.648565\n",
      "Iteration 72: loss = 0.645621\n",
      "Iteration 73: loss = 0.642751\n",
      "Iteration 74: loss = 0.639954\n",
      "Iteration 75: loss = 0.637228\n",
      "Iteration 76: loss = 0.634572\n",
      "Iteration 77: loss = 0.631982\n",
      "Iteration 78: loss = 0.629455\n",
      "Iteration 79: loss = 0.626988\n",
      "Iteration 80: loss = 0.624578\n",
      "Iteration 81: loss = 0.622223\n",
      "Iteration 82: loss = 0.619918\n",
      "Iteration 83: loss = 0.617664\n",
      "Iteration 84: loss = 0.615457\n",
      "Iteration 85: loss = 0.613296\n",
      "Iteration 86: loss = 0.611181\n",
      "Iteration 87: loss = 0.609109\n",
      "Iteration 88: loss = 0.607080\n",
      "Iteration 89: loss = 0.605092\n",
      "Iteration 90: loss = 0.603144\n",
      "Iteration 91: loss = 0.601235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss = 0.685655\n",
      "Iteration 1: loss = 0.449239\n",
      "Iteration 2: loss = 0.438893\n",
      "Iteration 3: loss = 0.436840\n",
      "Iteration 4: loss = 0.436086\n",
      "Iteration 5: loss = 0.435616\n",
      "Iteration 6: loss = 0.435232\n",
      "Iteration 0: loss = 0.699133\n",
      "Iteration 1: loss = 0.487847\n",
      "Iteration 2: loss = 0.410107\n",
      "Iteration 3: loss = 0.392564\n",
      "Iteration 4: loss = 0.391508\n",
      "Iteration 5: loss = 0.390899\n",
      "Iteration 6: loss = 0.390323\n",
      "Iteration 7: loss = 0.389776\n",
      "Iteration 8: loss = 0.389258\n",
      "Iteration 9: loss = 0.388767\n",
      "Iteration 10: loss = 0.388301\n",
      "Iteration 11: loss = 0.387860\n",
      "Iteration 12: loss = 0.387441\n",
      "Iteration 13: loss = 0.387044\n",
      "Iteration 14: loss = 0.386668\n",
      "Iteration 0: loss = 0.691618\n",
      "Iteration 1: loss = 0.429768\n",
      "Iteration 2: loss = 0.424160\n",
      "Iteration 3: loss = 0.422406\n",
      "Iteration 4: loss = 0.421285\n",
      "Iteration 5: loss = 0.420309\n",
      "Iteration 6: loss = 0.419383\n",
      "Iteration 7: loss = 0.418487\n",
      "Iteration 8: loss = 0.417616\n",
      "Iteration 9: loss = 0.416768\n",
      "Iteration 10: loss = 0.415942\n",
      "Iteration 11: loss = 0.415137\n",
      "Iteration 12: loss = 0.414352\n",
      "Iteration 13: loss = 0.413588\n",
      "Iteration 14: loss = 0.412843\n",
      "Iteration 15: loss = 0.412116\n",
      "Iteration 16: loss = 0.411407\n",
      "Iteration 17: loss = 0.410716\n",
      "Iteration 18: loss = 0.410042\n",
      "Iteration 19: loss = 0.409384\n",
      "Iteration 20: loss = 0.408742\n",
      "Iteration 21: loss = 0.408115\n",
      "Iteration 22: loss = 0.407503\n",
      "Iteration 23: loss = 0.406905\n",
      "Iteration 24: loss = 0.406321\n",
      "Iteration 25: loss = 0.405750\n",
      "Iteration 26: loss = 0.405193\n",
      "Iteration 27: loss = 0.404647\n",
      "Iteration 28: loss = 0.404114\n",
      "Iteration 29: loss = 0.403593\n",
      "Iteration 30: loss = 0.403083\n",
      "Iteration 31: loss = 0.402583\n",
      "Iteration 32: loss = 0.402095\n",
      "Iteration 33: loss = 0.401617\n",
      "Iteration 34: loss = 0.401148\n",
      "Iteration 35: loss = 0.400690\n",
      "Iteration 36: loss = 0.400240\n",
      "Iteration 37: loss = 0.399800\n",
      "Iteration 38: loss = 0.399368\n",
      "Iteration 39: loss = 0.398945\n",
      "Iteration 40: loss = 0.398530\n",
      "Iteration 41: loss = 0.398123\n",
      "Iteration 42: loss = 0.397724\n",
      "Iteration 43: loss = 0.397332\n",
      "Iteration 0: loss = 0.713424\n",
      "Iteration 1: loss = 0.465402\n",
      "Iteration 2: loss = 0.404995\n",
      "Iteration 3: loss = 0.389113\n",
      "Iteration 4: loss = 0.387532\n",
      "Iteration 5: loss = 0.386841\n",
      "Iteration 6: loss = 0.386167\n",
      "Iteration 7: loss = 0.385503\n",
      "Iteration 8: loss = 0.384849\n",
      "Iteration 9: loss = 0.384206\n",
      "Iteration 10: loss = 0.383572\n",
      "Iteration 11: loss = 0.382948\n",
      "Iteration 12: loss = 0.382334\n",
      "Iteration 13: loss = 0.381728\n",
      "Iteration 14: loss = 0.381132\n",
      "Iteration 15: loss = 0.380544\n",
      "Iteration 16: loss = 0.379965\n",
      "Iteration 17: loss = 0.379395\n",
      "Iteration 18: loss = 0.378833\n",
      "Iteration 19: loss = 0.378279\n",
      "Iteration 20: loss = 0.377733\n",
      "Iteration 21: loss = 0.377195\n",
      "Iteration 22: loss = 0.376664\n",
      "Iteration 23: loss = 0.376141\n",
      "Iteration 24: loss = 0.375626\n",
      "Iteration 25: loss = 0.375117\n",
      "Iteration 26: loss = 0.374616\n",
      "Iteration 27: loss = 0.374121\n",
      "Iteration 28: loss = 0.373633\n",
      "Iteration 29: loss = 0.373152\n",
      "Iteration 30: loss = 0.372678\n",
      "Iteration 31: loss = 0.372209\n",
      "Iteration 32: loss = 0.371747\n",
      "Iteration 33: loss = 0.371291\n",
      "Iteration 34: loss = 0.370841\n",
      "Iteration 35: loss = 0.370397\n",
      "Iteration 36: loss = 0.369959\n",
      "Iteration 37: loss = 0.369526\n",
      "Iteration 38: loss = 0.369099\n",
      "Iteration 39: loss = 0.368677\n",
      "Iteration 40: loss = 0.368261\n",
      "Iteration 41: loss = 0.367850\n",
      "Iteration 42: loss = 0.367444\n",
      "Iteration 43: loss = 0.367043\n",
      "Iteration 44: loss = 0.366646\n",
      "Iteration 45: loss = 0.366255\n",
      "Iteration 46: loss = 0.365869\n",
      "Iteration 47: loss = 0.365487\n",
      "Iteration 48: loss = 0.365109\n",
      "Iteration 49: loss = 0.364736\n",
      "Iteration 50: loss = 0.364368\n",
      "Iteration 51: loss = 0.364004\n",
      "Iteration 52: loss = 0.363644\n",
      "Iteration 0: loss = 1.799118\n",
      "Iteration 1: loss = 1.688877\n",
      "Iteration 2: loss = 1.632898\n",
      "Iteration 3: loss = 1.600532\n",
      "Iteration 4: loss = 1.581065\n",
      "Iteration 5: loss = 1.568677\n",
      "Iteration 6: loss = 1.560164\n",
      "Iteration 7: loss = 1.553760\n",
      "Iteration 8: loss = 1.548496\n",
      "Iteration 9: loss = 1.543838\n",
      "Iteration 10: loss = 1.539492\n",
      "Iteration 11: loss = 1.535297\n",
      "Iteration 12: loss = 1.531162\n",
      "Iteration 13: loss = 1.527033\n",
      "Iteration 14: loss = 1.522880\n",
      "Iteration 15: loss = 1.518684\n",
      "Iteration 16: loss = 1.514433\n",
      "Iteration 17: loss = 1.510122\n",
      "Iteration 18: loss = 1.505746\n",
      "Iteration 19: loss = 1.501299\n",
      "Iteration 20: loss = 1.496774\n",
      "Iteration 21: loss = 1.492166\n",
      "Iteration 22: loss = 1.487477\n",
      "Iteration 23: loss = 1.482701\n",
      "Iteration 24: loss = 1.477833\n",
      "Iteration 25: loss = 1.472870\n",
      "Iteration 26: loss = 1.467818\n",
      "Iteration 27: loss = 1.462680\n",
      "Iteration 28: loss = 1.457468\n",
      "Iteration 29: loss = 1.452201\n",
      "Iteration 30: loss = 1.446883\n",
      "Iteration 31: loss = 1.441501\n",
      "Iteration 32: loss = 1.436045\n",
      "Iteration 33: loss = 1.430508\n",
      "Iteration 34: loss = 1.424889\n",
      "Iteration 35: loss = 1.419199\n",
      "Iteration 36: loss = 1.413460\n",
      "Iteration 37: loss = 1.407689\n",
      "Iteration 38: loss = 1.401892\n",
      "Iteration 39: loss = 1.396089\n",
      "Iteration 40: loss = 1.390258\n",
      "Iteration 41: loss = 1.384353\n",
      "Iteration 42: loss = 1.378357\n",
      "Iteration 43: loss = 1.372308\n",
      "Iteration 44: loss = 1.366220\n",
      "Iteration 45: loss = 1.360117\n",
      "Iteration 46: loss = 1.354024\n",
      "Iteration 47: loss = 1.347967\n",
      "Iteration 48: loss = 1.341932\n",
      "Iteration 49: loss = 1.335900\n",
      "Iteration 50: loss = 1.329876\n",
      "Iteration 51: loss = 1.323875\n",
      "Iteration 52: loss = 1.317912\n",
      "Iteration 53: loss = 1.311998\n",
      "Iteration 54: loss = 1.306146\n",
      "Iteration 55: loss = 1.300356\n",
      "Iteration 56: loss = 1.294628\n",
      "Iteration 57: loss = 1.288969\n",
      "Iteration 58: loss = 1.283383\n",
      "Iteration 59: loss = 1.277867\n",
      "Iteration 60: loss = 1.272406\n",
      "Iteration 61: loss = 1.266996\n",
      "Iteration 62: loss = 1.261642\n",
      "Iteration 63: loss = 1.256350\n",
      "Iteration 64: loss = 1.251130\n",
      "Iteration 65: loss = 1.245985\n",
      "Iteration 66: loss = 1.240912\n",
      "Iteration 67: loss = 1.235913\n",
      "Iteration 68: loss = 1.230990\n",
      "Iteration 69: loss = 1.226144\n",
      "Iteration 70: loss = 1.221375\n",
      "Iteration 71: loss = 1.216683\n",
      "Iteration 72: loss = 1.212066\n",
      "Iteration 73: loss = 1.207524\n",
      "Iteration 74: loss = 1.203054\n",
      "Iteration 75: loss = 1.198655\n",
      "Iteration 76: loss = 1.194324\n",
      "Iteration 77: loss = 1.190055\n",
      "Iteration 78: loss = 1.185849\n",
      "Iteration 79: loss = 1.181701\n",
      "Iteration 80: loss = 1.177631\n",
      "Iteration 81: loss = 1.173651\n",
      "Iteration 82: loss = 1.170056\n",
      "Iteration 83: loss = 1.168026\n",
      "Iteration 84: loss = 1.165150\n",
      "Iteration 85: loss = 1.164175\n",
      "Iteration 0: loss = 1.793458\n",
      "Iteration 1: loss = 1.529784\n",
      "Iteration 2: loss = 1.376979\n",
      "Iteration 3: loss = 1.269924\n",
      "Iteration 4: loss = 1.199930\n",
      "Iteration 5: loss = 1.134879\n",
      "Iteration 6: loss = 1.091312\n",
      "Iteration 7: loss = 1.041771\n",
      "Iteration 8: loss = 1.007890\n",
      "Iteration 9: loss = 0.969311\n",
      "Iteration 10: loss = 0.940739\n",
      "Iteration 11: loss = 0.911759\n",
      "Iteration 12: loss = 0.888354\n",
      "Iteration 13: loss = 0.866687\n",
      "Iteration 14: loss = 0.848058\n",
      "Iteration 15: loss = 0.831207\n",
      "Iteration 16: loss = 0.816105\n",
      "Iteration 17: loss = 0.802266\n",
      "Iteration 18: loss = 0.789544\n",
      "Iteration 19: loss = 0.777707\n",
      "Iteration 20: loss = 0.766660\n",
      "Iteration 21: loss = 0.756273\n",
      "Iteration 22: loss = 0.746487\n",
      "Iteration 23: loss = 0.737234\n",
      "Iteration 24: loss = 0.728473\n",
      "Iteration 25: loss = 0.720154\n",
      "Iteration 26: loss = 0.712241\n",
      "Iteration 27: loss = 0.704690\n",
      "Iteration 28: loss = 0.697469\n",
      "Iteration 29: loss = 0.690550\n",
      "Iteration 30: loss = 0.683909\n",
      "Iteration 31: loss = 0.677524\n",
      "Iteration 32: loss = 0.671376\n",
      "Iteration 33: loss = 0.665449\n",
      "Iteration 34: loss = 0.659728\n",
      "Iteration 35: loss = 0.654200\n",
      "Iteration 36: loss = 0.648852\n",
      "Iteration 37: loss = 0.643669\n",
      "Iteration 38: loss = 0.638639\n",
      "Iteration 39: loss = 0.633756\n",
      "Iteration 40: loss = 0.629013\n",
      "Iteration 41: loss = 0.624404\n",
      "Iteration 42: loss = 0.619920\n",
      "Iteration 43: loss = 0.615554\n",
      "Iteration 44: loss = 0.611300\n",
      "Iteration 45: loss = 0.607154\n",
      "Iteration 46: loss = 0.603110\n",
      "Iteration 47: loss = 0.599165\n",
      "Iteration 48: loss = 0.595317\n",
      "Iteration 49: loss = 0.591562\n",
      "Iteration 50: loss = 0.587898\n",
      "Iteration 51: loss = 0.584321\n",
      "Iteration 52: loss = 0.580827\n",
      "Iteration 53: loss = 0.577413\n",
      "Iteration 54: loss = 0.574075\n",
      "Iteration 55: loss = 0.570810\n",
      "Iteration 56: loss = 0.567614\n",
      "Iteration 57: loss = 0.564487\n",
      "Iteration 58: loss = 0.561423\n",
      "Iteration 59: loss = 0.558422\n",
      "Iteration 60: loss = 0.555482\n",
      "Iteration 61: loss = 0.552599\n",
      "Iteration 62: loss = 0.549774\n",
      "Iteration 63: loss = 0.547002\n",
      "Iteration 64: loss = 0.544282\n",
      "Iteration 65: loss = 0.541613\n",
      "Iteration 66: loss = 0.538993\n",
      "Iteration 67: loss = 0.536418\n",
      "Iteration 68: loss = 0.533887\n",
      "Iteration 69: loss = 0.531396"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss = 1.790631\n",
      "Iteration 1: loss = 1.662513\n",
      "Iteration 2: loss = 1.574997\n",
      "Iteration 3: loss = 1.503403\n",
      "Iteration 4: loss = 1.443474\n",
      "Iteration 5: loss = 1.392446\n",
      "Iteration 6: loss = 1.348318\n",
      "Iteration 7: loss = 1.309615\n",
      "Iteration 8: loss = 1.275233\n",
      "Iteration 9: loss = 1.244339\n",
      "Iteration 10: loss = 1.216294\n",
      "Iteration 11: loss = 1.190603\n",
      "Iteration 12: loss = 1.166877\n",
      "Iteration 13: loss = 1.144802\n",
      "Iteration 14: loss = 1.124124\n",
      "Iteration 15: loss = 1.104641\n",
      "Iteration 16: loss = 1.086191\n",
      "Iteration 17: loss = 1.068640\n",
      "Iteration 18: loss = 1.051877\n",
      "Iteration 19: loss = 1.035809\n",
      "Iteration 20: loss = 1.020355\n",
      "Iteration 21: loss = 1.005451\n",
      "Iteration 22: loss = 0.991033\n",
      "Iteration 23: loss = 0.977045\n",
      "Iteration 24: loss = 0.963448\n",
      "Iteration 25: loss = 0.950228\n",
      "Iteration 26: loss = 0.937377\n",
      "Iteration 27: loss = 0.924882\n",
      "Iteration 28: loss = 0.912720\n",
      "Iteration 29: loss = 0.900880\n",
      "Iteration 30: loss = 0.889363\n",
      "Iteration 31: loss = 0.878175\n",
      "Iteration 32: loss = 0.867320\n",
      "Iteration 33: loss = 0.856780\n",
      "Iteration 34: loss = 0.846543\n",
      "Iteration 35: loss = 0.836615\n",
      "Iteration 36: loss = 0.827000\n",
      "Iteration 37: loss = 0.817688\n",
      "Iteration 38: loss = 0.808666\n",
      "Iteration 39: loss = 0.799931\n",
      "Iteration 40: loss = 0.791484\n",
      "Iteration 41: loss = 0.783325\n",
      "Iteration 42: loss = 0.775447\n",
      "Iteration 43: loss = 0.767840\n",
      "Iteration 44: loss = 0.760494\n",
      "Iteration 45: loss = 0.753399\n",
      "Iteration 46: loss = 0.746544\n",
      "Iteration 47: loss = 0.739921\n",
      "Iteration 48: loss = 0.733525\n",
      "Iteration 49: loss = 0.727354\n",
      "Iteration 50: loss = 0.721405\n",
      "Iteration 51: loss = 0.715679\n",
      "Iteration 52: loss = 0.710177\n",
      "Iteration 53: loss = 0.704888\n",
      "Iteration 54: loss = 0.699797\n",
      "Iteration 55: loss = 0.694887\n",
      "Iteration 56: loss = 0.690151\n",
      "Iteration 57: loss = 0.685576\n",
      "Iteration 58: loss = 0.681155\n",
      "Iteration 59: loss = 0.676880\n",
      "Iteration 60: loss = 0.672741\n",
      "Iteration 61: loss = 0.668734\n",
      "Iteration 62: loss = 0.664853\n",
      "Iteration 63: loss = 0.661092\n",
      "Iteration 64: loss = 0.657447\n",
      "Iteration 65: loss = 0.653913\n",
      "Iteration 66: loss = 0.650487\n",
      "Iteration 67: loss = 0.647162\n",
      "Iteration 68: loss = 0.643934\n",
      "Iteration 69: loss = 0.640794\n",
      "Iteration 70: loss = 0.637730\n",
      "Iteration 71: loss = 0.634735\n",
      "Iteration 72: loss = 0.631807\n",
      "Iteration 73: loss = 0.628949\n",
      "Iteration 74: loss = 0.626161\n",
      "Iteration 75: loss = 0.623441\n",
      "Iteration 76: loss = 0.620785\n",
      "Iteration 77: loss = 0.618190\n",
      "Iteration 78: loss = 0.615655\n",
      "Iteration 79: loss = 0.613177\n",
      "Iteration 80: loss = 0.610755\n",
      "Iteration 81: loss = 0.608386\n",
      "Iteration 82: loss = 0.606070\n",
      "Iteration 83: loss = 0.603803\n",
      "Iteration 84: loss = 0.601584\n",
      "Iteration 85: loss = 0.599413\n",
      "Iteration 86: loss = 0.597286\n",
      "Iteration 87: loss = 0.595203\n",
      "Iteration 88: loss = 0.593163\n",
      "Iteration 89: loss = 0.591163\n",
      "Iteration 90: loss = 0.589202\n",
      "Iteration 91: loss = 0.587277\n",
      "Iteration 92: loss = 0.585386\n",
      "Iteration 93: loss = 0.583524\n",
      "Iteration 94: loss = 0.581691\n",
      "Iteration 95: loss = 0.579888\n",
      "Iteration 96: loss = 0.578113\n",
      "Iteration 97: loss = 0.576370\n",
      "Iteration 98: loss = 0.574657\n",
      "Iteration 99: loss = 0.572973\n",
      "Iteration 100: loss = 0.571318\n",
      "Iteration 101: loss = 0.569691\n",
      "Iteration 102: loss = 0.568092\n",
      "Iteration 103: loss = 0.566520\n",
      "Iteration 104: loss = 0.564973\n",
      "Iteration 105: loss = 0.563451\n",
      "Iteration 106: loss = 0.561954\n",
      "Iteration 107: loss = 0.560480\n",
      "Iteration 108: loss = 0.559028\n",
      "Iteration 109: loss = 0.557597\n",
      "Iteration 110: loss = 0.556187\n",
      "Iteration 111: loss = 0.554798\n",
      "Iteration 112: loss = 0.553427\n",
      "Iteration 113: loss = 0.552076\n",
      "Iteration 114: loss = 0.550744\n",
      "Iteration 115: loss = 0.549429\n",
      "Iteration 116: loss = 0.548132\n",
      "Iteration 117: loss = 0.546852\n",
      "Iteration 118: loss = 0.545589\n",
      "Iteration 119: loss = 0.544343\n",
      "Iteration 120: loss = 0.543112\n",
      "Iteration 121: loss = 0.541898\n",
      "Iteration 122: loss = 0.540699\n",
      "Iteration 123: loss = 0.539516\n",
      "Iteration 124: loss = 0.538348\n",
      "Iteration 125: loss = 0.537195\n",
      "Iteration 126: loss = 0.536057\n",
      "Iteration 127: loss = 0.534933\n",
      "Iteration 128: loss = 0.533822\n",
      "Iteration 129: loss = 0.532726\n",
      "Iteration 130: loss = 0.531642\n",
      "Iteration 131: loss = 0.530571\n",
      "Iteration 132: loss = 0.529513\n",
      "Iteration 133: loss = 0.528467\n",
      "Iteration 134: loss = 0.527433\n",
      "Iteration 135: loss = 0.526410\n",
      "Iteration 136: loss = 0.525398\n",
      "Iteration 137: loss = 0.524397\n",
      "Iteration 138: loss = 0.523406\n",
      "Iteration 139: loss = 0.522426\n",
      "Iteration 140: loss = 0.521455\n",
      "Iteration 141: loss = 0.520494\n",
      "Iteration 142: loss = 0.519543\n",
      "Iteration 143: loss = 0.518600\n",
      "Iteration 144: loss = 0.517666\n",
      "Iteration 145: loss = 0.516741\n",
      "Iteration 146: loss = 0.515824\n",
      "Iteration 147: loss = 0.514921\n",
      "Iteration 148: loss = 0.514036\n",
      "Iteration 149: loss = 0.513227\n",
      "Iteration 150: loss = 0.512628\n",
      "Iteration 151: loss = 0.512117\n",
      "Iteration 0: loss = 1.790220\n",
      "Iteration 1: loss = 1.742111\n",
      "Iteration 2: loss = 1.700833\n",
      "Iteration 3: loss = 1.662643\n",
      "Iteration 4: loss = 1.625879\n",
      "Iteration 5: loss = 1.589768\n",
      "Iteration 6: loss = 1.553902\n",
      "Iteration 7: loss = 1.518036\n",
      "Iteration 8: loss = 1.482011\n",
      "Iteration 9: loss = 1.445724\n",
      "Iteration 10: loss = 1.409102\n",
      "Iteration 11: loss = 1.372106\n",
      "Iteration 12: loss = 1.334763\n",
      "Iteration 13: loss = 1.297183\n",
      "Iteration 14: loss = 1.259524\n",
      "Iteration 15: loss = 1.221936\n",
      "Iteration 16: loss = 1.184584\n",
      "Iteration 17: loss = 1.147641\n",
      "Iteration 18: loss = 1.111267\n",
      "Iteration 19: loss = 1.075609\n",
      "Iteration 20: loss = 1.040826\n",
      "Iteration 21: loss = 1.007100\n",
      "Iteration 22: loss = 0.974499\n",
      "Iteration 23: loss = 0.943053\n",
      "Iteration 24: loss = 0.912797\n",
      "Iteration 25: loss = 0.883759\n",
      "Iteration 26: loss = 0.856005\n",
      "Iteration 27: loss = 0.829595\n",
      "Iteration 28: loss = 0.804475\n",
      "Iteration 29: loss = 0.780626\n",
      "Iteration 30: loss = 0.758017\n",
      "Iteration 31: loss = 0.736565\n",
      "Iteration 32: loss = 0.716198\n",
      "Iteration 33: loss = 0.696863\n",
      "Iteration 34: loss = 0.678488\n",
      "Iteration 35: loss = 0.661018\n",
      "Iteration 36: loss = 0.644428\n",
      "Iteration 37: loss = 0.628689\n",
      "Iteration 38: loss = 0.613749\n",
      "Iteration 39: loss = 0.599545\n",
      "Iteration 40: loss = 0.586031\n",
      "Iteration 41: loss = 0.573156\n",
      "Iteration 42: loss = 0.560870\n",
      "Iteration 43: loss = 0.549129\n",
      "Iteration 44: loss = 0.537906\n",
      "Iteration 45: loss = 0.527191\n",
      "Iteration 46: loss = 0.516969\n",
      "Iteration 47: loss = 0.507219\n",
      "Iteration 48: loss = 0.497907\n",
      "Iteration 49: loss = 0.488983\n",
      "Iteration 50: loss = 0.480399\n",
      "Iteration 51: loss = 0.472125\n",
      "Iteration 52: loss = 0.464158\n",
      "Iteration 53: loss = 0.456494\n",
      "Iteration 54: loss = 0.449119\n",
      "Iteration 55: loss = 0.442024\n",
      "Iteration 56: loss = 0.435203\n",
      "Iteration 57: loss = 0.428637\n",
      "Iteration 58: loss = 0.422307\n",
      "Iteration 59: loss = 0.416227\n",
      "Iteration 60: loss = 0.410375\n",
      "Iteration 61: loss = 0.404732\n",
      "Iteration 62: loss = 0.399327\n",
      "Iteration 63: loss = 0.394127\n",
      "Iteration 64: loss = 0.389093\n",
      "Iteration 65: loss = 0.384187\n",
      "Iteration 66: loss = 0.379401\n",
      "Iteration 67: loss = 0.374760\n",
      "Iteration 68: loss = 0.370285\n",
      "Iteration 69: loss = 0.365974\n",
      "Iteration 70: loss = 0.361814\n",
      "Iteration 71: loss = 0.357798\n",
      "Iteration 72: loss = 0.353913\n",
      "Iteration 73: loss = 0.350149\n",
      "Iteration 74: loss = 0.346493\n",
      "Iteration 75: loss = 0.342942\n",
      "Iteration 76: loss = 0.339485\n",
      "Iteration 77: loss = 0.336127\n",
      "Iteration 78: loss = 0.332870\n",
      "Iteration 79: loss = 0.329715\n",
      "Iteration 80: loss = 0.326654\n",
      "Iteration 81: loss = 0.323678\n",
      "Iteration 82: loss = 0.320788\n",
      "Iteration 83: loss = 0.317989\n",
      "Iteration 84: loss = 0.315285\n",
      "Iteration 85: loss = 0.312671\n",
      "Iteration 86: loss = 0.310145\n",
      "Iteration 87: loss = 0.307700\n",
      "Iteration 88: loss = 0.305332\n",
      "Iteration 89: loss = 0.303035\n",
      "Iteration 90: loss = 0.300806\n",
      "Iteration 91: loss = 0.298639\n",
      "Iteration 92: loss = 0.296532\n",
      "Iteration 93: loss = 0.294482\n",
      "Iteration 94: loss = 0.292486\n",
      "Iteration 95: loss = 0.290543\n",
      "Iteration 96: loss = 0.288648\n",
      "Iteration 97: loss = 0.286801\n",
      "Iteration 98: loss = 0.285000\n",
      "Iteration 99: loss = 0.283241\n",
      "Iteration 100: loss = 0.281526\n",
      "Iteration 101: loss = 0.279853\n",
      "Iteration 102: loss = 0.278224\n",
      "Iteration 103: loss = 0.276637\n",
      "Iteration 104: loss = 0.275087\n",
      "Iteration 105: loss = 0.273572\n",
      "Iteration 106: loss = 0.272089\n",
      "Iteration 107: loss = 0.270636\n",
      "Iteration 108: loss = 0.269211\n",
      "Iteration 109: loss = 0.267814\n",
      "Iteration 110: loss = 0.266441\n",
      "Iteration 111: loss = 0.265093\n",
      "Iteration 112: loss = 0.263768\n",
      "Iteration 113: loss = 0.262466\n",
      "Iteration 114: loss = 0.261186\n",
      "Iteration 115: loss = 0.259929\n",
      "Iteration 116: loss = 0.258694\n",
      "Iteration 117: loss = 0.257483\n",
      "Iteration 118: loss = 0.256295\n",
      "Iteration 91: loss = 0.634593\n",
      "Iteration 92: loss = 0.632400\n",
      "Iteration 93: loss = 0.630242\n",
      "Iteration 94: loss = 0.628116\n",
      "Iteration 95: loss = 0.626020\n",
      "Iteration 96: loss = 0.623952\n",
      "Iteration 97: loss = 0.621910\n",
      "Iteration 98: loss = 0.619892\n",
      "Iteration 99: loss = 0.617899\n",
      "Iteration 100: loss = 0.615928\n",
      "Iteration 101: loss = 0.613978\n",
      "Iteration 102: loss = 0.612047\n",
      "Iteration 103: loss = 0.610136\n",
      "Iteration 104: loss = 0.608245\n",
      "Iteration 105: loss = 0.606373\n",
      "Iteration 106: loss = 0.604523\n",
      "Iteration 107: loss = 0.602695\n",
      "Iteration 108: loss = 0.600902\n",
      "Iteration 109: loss = 0.599193\n",
      "Iteration 110: loss = 0.597907\n",
      "Iteration 111: loss = 0.597042\n",
      "Iteration 112: loss = 0.596638\n",
      "Iteration 0: loss = 1.793142\n",
      "Iteration 1: loss = 1.744337\n",
      "Iteration 2: loss = 1.702091\n",
      "Iteration 3: loss = 1.662849\n",
      "Iteration 4: loss = 1.625046\n",
      "Iteration 5: loss = 1.587929\n",
      "Iteration 6: loss = 1.551080\n",
      "Iteration 7: loss = 1.514228\n",
      "Iteration 8: loss = 1.477191\n",
      "Iteration 9: loss = 1.439848\n",
      "Iteration 10: loss = 1.402136\n",
      "Iteration 11: loss = 1.364068\n",
      "Iteration 12: loss = 1.325690\n",
      "Iteration 13: loss = 1.287082\n",
      "Iteration 14: loss = 1.248384\n",
      "Iteration 15: loss = 1.209800\n",
      "Iteration 16: loss = 1.171494\n",
      "Iteration 17: loss = 1.133629\n",
      "Iteration 18: loss = 1.096375\n",
      "Iteration 19: loss = 1.059851\n",
      "Iteration 20: loss = 1.024181\n",
      "Iteration 21: loss = 0.989482\n",
      "Iteration 22: loss = 0.955786\n",
      "Iteration 23: loss = 0.923161\n",
      "Iteration 24: loss = 0.891779\n",
      "Iteration 25: loss = 0.861768\n",
      "Iteration 26: loss = 0.833181\n",
      "Iteration 27: loss = 0.806003\n",
      "Iteration 28: loss = 0.780211\n",
      "Iteration 29: loss = 0.755769\n",
      "Iteration 30: loss = 0.732628\n",
      "Iteration 31: loss = 0.710740\n",
      "Iteration 32: loss = 0.690049\n",
      "Iteration 33: loss = 0.670495\n",
      "Iteration 34: loss = 0.652002\n",
      "Iteration 35: loss = 0.634497\n",
      "Iteration 36: loss = 0.617933\n",
      "Iteration 37: loss = 0.602279\n",
      "Iteration 38: loss = 0.587487\n",
      "Iteration 39: loss = 0.573484\n",
      "Iteration 40: loss = 0.560200\n",
      "Iteration 41: loss = 0.547601\n",
      "Iteration 42: loss = 0.535661\n",
      "Iteration 43: loss = 0.524345\n",
      "Iteration 44: loss = 0.513608\n",
      "Iteration 45: loss = 0.503406\n",
      "Iteration 46: loss = 0.493696\n",
      "Iteration 47: loss = 0.484447\n",
      "Iteration 48: loss = 0.475637\n",
      "Iteration 49: loss = 0.467240\n",
      "Iteration 50: loss = 0.459230\n",
      "Iteration 51: loss = 0.451580\n",
      "Iteration 52: loss = 0.444269\n",
      "Iteration 53: loss = 0.437275\n",
      "Iteration 54: loss = 0.430579\n",
      "Iteration 55: loss = 0.424160\n",
      "Iteration 56: loss = 0.418000\n",
      "Iteration 57: loss = 0.412084\n",
      "Iteration 58: loss = 0.406397\n",
      "Iteration 59: loss = 0.400930\n",
      "Iteration 60: loss = 0.395670\n",
      "Iteration 61: loss = 0.390605\n",
      "Iteration 62: loss = 0.385725\n",
      "Iteration 63: loss = 0.381021\n",
      "Iteration 64: loss = 0.376487\n",
      "Iteration 65: loss = 0.372114\n",
      "Iteration 66: loss = 0.367895\n",
      "Iteration 67: loss = 0.363824\n",
      "Iteration 68: loss = 0.359896\n",
      "Iteration 69: loss = 0.356104\n",
      "Iteration 70: loss = 0.352442\n",
      "Iteration 71: loss = 0.348906\n",
      "Iteration 72: loss = 0.345486\n",
      "Iteration 73: loss = 0.342176\n",
      "Iteration 74: loss = 0.338968\n",
      "Iteration 75: loss = 0.335856\n",
      "Iteration 76: loss = 0.332836\n",
      "Iteration 77: loss = 0.329903\n",
      "Iteration 78: loss = 0.327056\n",
      "Iteration 79: loss = 0.324290\n",
      "Iteration 80: loss = 0.321603\n",
      "Iteration 81: loss = 0.318993\n",
      "Iteration 82: loss = 0.316458\n",
      "Iteration 83: loss = 0.313995\n",
      "Iteration 84: loss = 0.311602\n",
      "Iteration 85: loss = 0.309276\n",
      "Iteration 86: loss = 0.307012\n",
      "Iteration 87: loss = 0.304809\n",
      "Iteration 88: loss = 0.302665\n",
      "Iteration 89: loss = 0.300576\n",
      "Iteration 90: loss = 0.298542\n",
      "Iteration 91: loss = 0.296561\n",
      "Iteration 92: loss = 0.294630\n",
      "Iteration 93: loss = 0.292748\n",
      "Iteration 94: loss = 0.290912\n",
      "Iteration 95: loss = 0.289119\n",
      "Iteration 96: loss = 0.287368\n",
      "Iteration 97: loss = 0.285658\n",
      "Iteration 98: loss = 0.283987\n",
      "Iteration 99: loss = 0.282355\n",
      "Iteration 100: loss = 0.280760\n",
      "Iteration 101: loss = 0.279202\n",
      "Iteration 102: loss = 0.277679\n",
      "Iteration 103: loss = 0.276191\n",
      "Iteration 104: loss = 0.274737\n",
      "Iteration 105: loss = 0.273314\n",
      "Iteration 106: loss = 0.271922\n",
      "Iteration 107: loss = 0.270560\n",
      "Iteration 108: loss = 0.269227\n",
      "Iteration 109: loss = 0.267919\n",
      "Iteration 110: loss = 0.266637\n",
      "Iteration 111: loss = 0.265378\n",
      "Iteration 112: loss = 0.264140\n",
      "Iteration 113: loss = 0.262922\n",
      "Iteration 114: loss = 0.261723\n",
      "Iteration 115: loss = 0.260545\n",
      "Iteration 116: loss = 0.259388\n",
      "Iteration 117: loss = 0.258253\n",
      "Iteration 118: loss = 0.257139\n",
      "Iteration 119: loss = 0.256046\n",
      "Iteration 120: loss = 0.254974\n",
      "Iteration 121: loss = 0.253921\n",
      "Iteration 122: loss = 0.252886\n",
      "Iteration 123: loss = 0.251869\n",
      "Iteration 124: loss = 0.250869\n",
      "Iteration 125: loss = 0.249887\n",
      "Iteration 126: loss = 0.248924\n",
      "Iteration 127: loss = 0.247979\n",
      "Iteration 128: loss = 0.247054\n",
      "Iteration 129: loss = 0.246147\n",
      "Iteration 130: loss = 0.245259\n",
      "Iteration 131: loss = 0.244389\n",
      "Iteration 132: loss = 0.243537\n",
      "Iteration 133: loss = 0.242701\n",
      "Iteration 134: loss = 0.241882\n",
      "Iteration 135: loss = 0.241078\n",
      "Iteration 136: loss = 0.240289\n",
      "Iteration 137: loss = 0.239515\n",
      "Iteration 138: loss = 0.238754\n",
      "Iteration 139: loss = 0.238006\n",
      "Iteration 140: loss = 0.237271\n",
      "Iteration 141: loss = 0.236548\n",
      "Iteration 142: loss = 0.235837\n",
      "Iteration 143: loss = 0.235137\n",
      "Iteration 144: loss = 0.234449\n",
      "Iteration 145: loss = 0.233771\n",
      "Iteration 146: loss = 0.233103\n",
      "Iteration 147: loss = 0.232444\n",
      "Iteration 148: loss = 0.231795\n",
      "Iteration 149: loss = 0.231156\n",
      "Iteration 150: loss = 0.230525\n",
      "Iteration 151: loss = 0.229903\n",
      "Iteration 152: loss = 0.229289\n",
      "Iteration 153: loss = 0.228683\n",
      "Iteration 154: loss = 0.228085\n",
      "Iteration 155: loss = 0.227494\n",
      "Iteration 156: loss = 0.226911\n",
      "Iteration 157: loss = 0.226336\n",
      "Iteration 158: loss = 0.225767\n",
      "Iteration 159: loss = 0.225205\n",
      "Iteration 160: loss = 0.224650\n",
      "Iteration 161: loss = 0.224101\n",
      "Iteration 162: loss = 0.223558\n",
      "Iteration 163: loss = 0.223021\n",
      "Iteration 164: loss = 0.222490\n",
      "Iteration 165: loss = 0.221965\n",
      "Iteration 166: loss = 0.221445\n",
      "Iteration 167: loss = 0.220932\n",
      "Iteration 168: loss = 0.220425\n",
      "Iteration 169: loss = 0.219924\n",
      "Iteration 170: loss = 0.219429\n",
      "Iteration 171: loss = 0.218940\n",
      "Iteration 172: loss = 0.218458\n",
      "Iteration 173: loss = 0.217981\n",
      "Iteration 174: loss = 0.217511\n",
      "Iteration 175: loss = 0.217046\n",
      "Iteration 176: loss = 0.216587\n",
      "Iteration 177: loss = 0.216133\n",
      "Iteration 178: loss = 0.215684\n",
      "Iteration 179: loss = 0.215241\n",
      "Iteration 180: loss = 0.214802\n",
      "Iteration 181: loss = 0.214367\n",
      "Iteration 182: loss = 0.213937\n",
      "Iteration 183: loss = 0.213511\n",
      "Iteration 184: loss = 0.213088\n",
      "Iteration 185: loss = 0.212669\n",
      "Iteration 186: loss = 0.212254\n",
      "Iteration 187: loss = 0.211842\n",
      "Iteration 188: loss = 0.211433\n",
      "Iteration 189: loss = 0.211028\n",
      "Iteration 190: loss = 0.210626\n",
      "Iteration 191: loss = 0.210227\n",
      "Iteration 192: loss = 0.209831\n",
      "Iteration 193: loss = 0.209437\n",
      "Iteration 194: loss = 0.209046\n",
      "Iteration 195: loss = 0.208658\n",
      "Iteration 196: loss = 0.208272\n",
      "Iteration 197: loss = 0.207888\n",
      "Iteration 198: loss = 0.207506\n",
      "Iteration 199: loss = 0.207126\n",
      "Iteration 200: loss = 0.206748\n",
      "Iteration 201: loss = 0.206372\n",
      "Iteration 202: loss = 0.205998\n",
      "Iteration 203: loss = 0.205626\n",
      "Iteration 204: loss = 0.205257\n",
      "Iteration 205: loss = 0.204891\n",
      "Iteration 206: loss = 0.204528\n",
      "Iteration 207: loss = 0.204168\n",
      "Iteration 208: loss = 0.203811\n",
      "Iteration 209: loss = 0.203457\n",
      "Iteration 210: loss = 0.203106\n",
      "Iteration 211: loss = 0.202758\n",
      "Iteration 212: loss = 0.202412\n",
      "Iteration 213: loss = 0.202069\n",
      "Iteration 214: loss = 0.201729\n",
      "Iteration 215: loss = 0.201391\n",
      "Iteration 216: loss = 0.201055\n",
      "Iteration 217: loss = 0.200722\n",
      "Iteration 218: loss = 0.200391\n",
      "Iteration 219: loss = 0.200062\n",
      "Iteration 220: loss = 0.199735\n",
      "Iteration 221: loss = 0.199411\n",
      "Iteration 222: loss = 0.199089\n",
      "Iteration 223: loss = 0.198769\n",
      "Iteration 224: loss = 0.198451\n",
      "Iteration 225: loss = 0.198135\n",
      "Iteration 226: loss = 0.197822\n",
      "Iteration 227: loss = 0.197510\n",
      "Iteration 228: loss = 0.197201\n",
      "Iteration 229: loss = 0.196894\n",
      "Iteration 230: loss = 0.196590\n",
      "Iteration 231: loss = 0.196287\n",
      "Iteration 232: loss = 0.195986\n",
      "Iteration 233: loss = 0.195688\n",
      "Iteration 234: loss = 0.195391\n",
      "Iteration 235: loss = 0.195096\n",
      "Iteration 236: loss = 0.194803\n",
      "Iteration 237: loss = 0.194512\n",
      "Iteration 238: loss = 0.194223\n",
      "Iteration 239: loss = 0.193935\n",
      "Iteration 240: loss = 0.193649\n",
      "Iteration 241: loss = 0.193365\n",
      "Iteration 242: loss = 0.193082\n",
      "Iteration 243: loss = 0.192801\n",
      "Iteration 244: loss = 0.192523\n",
      "Iteration 245: loss = 0.192245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 64: loss = 0.659737\n",
      "Iteration 65: loss = 0.656244\n",
      "Iteration 66: loss = 0.652848\n",
      "Iteration 67: loss = 0.649547\n",
      "Iteration 68: loss = 0.646336\n",
      "Iteration 69: loss = 0.643212\n",
      "Iteration 70: loss = 0.640171\n",
      "Iteration 71: loss = 0.637209\n",
      "Iteration 72: loss = 0.634325\n",
      "Iteration 73: loss = 0.631516\n",
      "Iteration 74: loss = 0.628779\n",
      "Iteration 75: loss = 0.626111\n",
      "Iteration 76: loss = 0.623510\n",
      "Iteration 77: loss = 0.620975\n",
      "Iteration 78: loss = 0.618501\n",
      "Iteration 79: loss = 0.616088\n",
      "Iteration 80: loss = 0.613733\n",
      "Iteration 81: loss = 0.611432\n",
      "Iteration 82: loss = 0.609184\n",
      "Iteration 83: loss = 0.606986\n",
      "Iteration 84: loss = 0.604834\n",
      "Iteration 85: loss = 0.602726\n",
      "Iteration 86: loss = 0.600659\n",
      "Iteration 87: loss = 0.598628\n",
      "Iteration 88: loss = 0.596633\n",
      "Iteration 89: loss = 0.594671\n",
      "Iteration 90: loss = 0.592743\n",
      "Iteration 91: loss = 0.590849\n",
      "Iteration 92: loss = 0.588991\n",
      "Iteration 93: loss = 0.587167\n",
      "Iteration 94: loss = 0.585377\n",
      "Iteration 95: loss = 0.583620\n",
      "Iteration 96: loss = 0.581895\n",
      "Iteration 97: loss = 0.580202\n",
      "Iteration 98: loss = 0.578540\n",
      "Iteration 99: loss = 0.576907\n",
      "Iteration 100: loss = 0.575304\n",
      "Iteration 101: loss = 0.573729\n",
      "Iteration 102: loss = 0.572182\n",
      "Iteration 103: loss = 0.570660\n",
      "Iteration 104: loss = 0.569165\n",
      "Iteration 105: loss = 0.567695\n",
      "Iteration 106: loss = 0.566249\n",
      "Iteration 107: loss = 0.564827\n",
      "Iteration 108: loss = 0.563427\n",
      "Iteration 109: loss = 0.562049\n",
      "Iteration 110: loss = 0.560692\n",
      "Iteration 111: loss = 0.559355\n",
      "Iteration 112: loss = 0.558037\n",
      "Iteration 113: loss = 0.556738\n",
      "Iteration 114: loss = 0.555456\n",
      "Iteration 115: loss = 0.554191\n",
      "Iteration 116: loss = 0.552943\n",
      "Iteration 117: loss = 0.551712\n",
      "Iteration 118: loss = 0.550498\n",
      "Iteration 119: loss = 0.549301\n",
      "Iteration 120: loss = 0.548120\n",
      "Iteration 121: loss = 0.546956\n",
      "Iteration 122: loss = 0.545807\n",
      "Iteration 123: loss = 0.544673\n",
      "Iteration 124: loss = 0.543554\n",
      "Iteration 125: loss = 0.542449\n",
      "Iteration 126: loss = 0.541358\n",
      "Iteration 127: loss = 0.540281\n",
      "Iteration 128: loss = 0.539217\n",
      "Iteration 129: loss = 0.538166\n",
      "Iteration 130: loss = 0.537127\n",
      "Iteration 131: loss = 0.536101\n",
      "Iteration 132: loss = 0.535086\n",
      "Iteration 133: loss = 0.534084\n",
      "Iteration 134: loss = 0.533092\n",
      "Iteration 135: loss = 0.532112\n",
      "Iteration 136: loss = 0.531143\n",
      "Iteration 137: loss = 0.530184\n",
      "Iteration 138: loss = 0.529236\n",
      "Iteration 139: loss = 0.528298\n",
      "Iteration 140: loss = 0.527369\n",
      "Iteration 141: loss = 0.526450\n",
      "Iteration 142: loss = 0.525541\n",
      "Iteration 143: loss = 0.524641\n",
      "Iteration 144: loss = 0.523750\n",
      "Iteration 145: loss = 0.522868\n",
      "Iteration 146: loss = 0.521996\n",
      "Iteration 147: loss = 0.521134\n",
      "Iteration 148: loss = 0.520289\n",
      "Iteration 149: loss = 0.519477\n",
      "Iteration 150: loss = 0.518785\n",
      "Iteration 151: loss = 0.518534\n",
      "Iteration 0: loss = 1.794775\n",
      "Iteration 1: loss = 1.745537\n",
      "Iteration 2: loss = 1.702910\n",
      "Iteration 3: loss = 1.663404\n",
      "Iteration 4: loss = 1.625464\n",
      "Iteration 5: loss = 1.588347\n",
      "Iteration 6: loss = 1.551650\n",
      "Iteration 7: loss = 1.515118\n",
      "Iteration 8: loss = 1.478579\n",
      "Iteration 9: loss = 1.441914\n",
      "Iteration 10: loss = 1.405062\n",
      "Iteration 11: loss = 1.368015\n",
      "Iteration 12: loss = 1.330823\n",
      "Iteration 13: loss = 1.293556\n",
      "Iteration 14: loss = 1.256322\n",
      "Iteration 15: loss = 1.219305\n",
      "Iteration 16: loss = 1.182667\n",
      "Iteration 17: loss = 1.146555\n",
      "Iteration 18: loss = 1.111096\n",
      "Iteration 19: loss = 1.076438\n",
      "Iteration 20: loss = 1.042742\n",
      "Iteration 21: loss = 1.010071\n",
      "Iteration 22: loss = 0.978444\n",
      "Iteration 23: loss = 0.947910\n",
      "Iteration 24: loss = 0.918515\n",
      "Iteration 25: loss = 0.890286\n",
      "Iteration 26: loss = 0.863223\n",
      "Iteration 27: loss = 0.837333\n",
      "Iteration 28: loss = 0.812646\n",
      "Iteration 29: loss = 0.789167\n",
      "Iteration 30: loss = 0.766851\n",
      "Iteration 31: loss = 0.745623\n",
      "Iteration 32: loss = 0.725427\n",
      "Iteration 33: loss = 0.706242\n",
      "Iteration 34: loss = 0.688056\n",
      "Iteration 35: loss = 0.670817\n",
      "Iteration 36: loss = 0.654454\n",
      "Iteration 37: loss = 0.638914\n",
      "Iteration 38: loss = 0.624157\n",
      "Iteration 39: loss = 0.610131\n",
      "Iteration 40: loss = 0.596787\n",
      "Iteration 41: loss = 0.584074\n",
      "Iteration 42: loss = 0.571949\n",
      "Iteration 43: loss = 0.560372\n",
      "Iteration 44: loss = 0.549314\n",
      "Iteration 45: loss = 0.538751\n",
      "Iteration 46: loss = 0.528663\n",
      "Iteration 47: loss = 0.519025\n",
      "Iteration 48: loss = 0.509818\n",
      "Iteration 49: loss = 0.501012\n",
      "Iteration 50: loss = 0.492572\n",
      "Iteration 51: loss = 0.484475\n",
      "Iteration 52: loss = 0.476708\n",
      "Iteration 53: loss = 0.469260\n",
      "Iteration 54: loss = 0.462116\n",
      "Iteration 55: loss = 0.455260\n",
      "Iteration 56: loss = 0.448676\n",
      "Iteration 57: loss = 0.442345\n",
      "Iteration 58: loss = 0.436252\n",
      "Iteration 59: loss = 0.430384\n",
      "Iteration 60: loss = 0.424731\n",
      "Iteration 61: loss = 0.419282\n",
      "Iteration 62: loss = 0.414028\n",
      "Iteration 63: loss = 0.408960\n",
      "Iteration 64: loss = 0.404071\n",
      "Iteration 65: loss = 0.399359\n",
      "Iteration 66: loss = 0.394817\n",
      "Iteration 67: loss = 0.390440\n",
      "Iteration 68: loss = 0.386219\n",
      "Iteration 69: loss = 0.382146\n",
      "Iteration 70: loss = 0.378211\n",
      "Iteration 71: loss = 0.374406\n",
      "Iteration 72: loss = 0.370720\n",
      "Iteration 73: loss = 0.367145\n",
      "Iteration 74: loss = 0.363670\n",
      "Iteration 75: loss = 0.360289\n",
      "Iteration 76: loss = 0.356998\n",
      "Iteration 77: loss = 0.353795\n",
      "Iteration 78: loss = 0.350682\n",
      "Iteration 79: loss = 0.347662\n",
      "Iteration 80: loss = 0.344735\n",
      "Iteration 81: loss = 0.341893\n",
      "Iteration 82: loss = 0.339133\n",
      "Iteration 83: loss = 0.336448\n",
      "Iteration 84: loss = 0.333838\n",
      "Iteration 85: loss = 0.331298\n",
      "Iteration 86: loss = 0.328826\n",
      "Iteration 87: loss = 0.326417\n",
      "Iteration 88: loss = 0.324069\n",
      "Iteration 89: loss = 0.321778\n",
      "Iteration 90: loss = 0.319541\n",
      "Iteration 91: loss = 0.317356\n",
      "Iteration 92: loss = 0.315221\n",
      "Iteration 93: loss = 0.313135\n",
      "Iteration 94: loss = 0.311099\n",
      "Iteration 95: loss = 0.309113\n",
      "Iteration 96: loss = 0.307175\n",
      "Iteration 97: loss = 0.305283\n",
      "Iteration 98: loss = 0.303436\n",
      "Iteration 99: loss = 0.301630\n",
      "Iteration 100: loss = 0.299859\n",
      "Iteration 101: loss = 0.298121\n",
      "Iteration 102: loss = 0.296410\n",
      "Iteration 103: loss = 0.294725\n",
      "Iteration 104: loss = 0.293064\n",
      "Iteration 105: loss = 0.291430\n",
      "Iteration 106: loss = 0.289822\n",
      "Iteration 107: loss = 0.288239\n",
      "Iteration 108: loss = 0.286680\n",
      "Iteration 109: loss = 0.285146\n",
      "Iteration 110: loss = 0.283638\n",
      "Iteration 111: loss = 0.282156\n",
      "Iteration 112: loss = 0.280701\n",
      "Iteration 113: loss = 0.279275\n",
      "Iteration 114: loss = 0.277879\n",
      "Iteration 115: loss = 0.276513\n",
      "Iteration 116: loss = 0.275174\n",
      "Iteration 117: loss = 0.273861\n",
      "Iteration 118: loss = 0.272572\n",
      "Iteration 119: loss = 0.271304\n",
      "Iteration 120: loss = 0.270056\n",
      "Iteration 121: loss = 0.268827\n",
      "Iteration 122: loss = 0.267614\n",
      "Iteration 123: loss = 0.266417\n",
      "Iteration 124: loss = 0.265236\n",
      "Iteration 125: loss = 0.264072\n",
      "Iteration 126: loss = 0.262927\n",
      "Iteration 127: loss = 0.261805\n",
      "Iteration 128: loss = 0.260705\n",
      "Iteration 129: loss = 0.259627\n",
      "Iteration 130: loss = 0.258570\n",
      "Iteration 131: loss = 0.257534\n",
      "Iteration 132: loss = 0.256521\n",
      "Iteration 133: loss = 0.255530\n",
      "Iteration 134: loss = 0.254558\n",
      "Iteration 135: loss = 0.253602\n",
      "Iteration 136: loss = 0.252663\n",
      "Iteration 137: loss = 0.251740\n",
      "Iteration 138: loss = 0.250832\n",
      "Iteration 139: loss = 0.249938\n",
      "Iteration 140: loss = 0.249057\n",
      "Iteration 141: loss = 0.248190\n",
      "Iteration 142: loss = 0.247335\n",
      "Iteration 143: loss = 0.246494\n",
      "Iteration 144: loss = 0.245665\n",
      "Iteration 145: loss = 0.244849\n",
      "Iteration 146: loss = 0.244044\n",
      "Iteration 147: loss = 0.243252\n",
      "Iteration 148: loss = 0.242470\n",
      "Iteration 149: loss = 0.241699\n",
      "Iteration 150: loss = 0.240938\n",
      "Iteration 151: loss = 0.240188\n",
      "Iteration 152: loss = 0.239448\n",
      "Iteration 153: loss = 0.238717\n",
      "Iteration 154: loss = 0.237998\n",
      "Iteration 155: loss = 0.237288\n",
      "Iteration 156: loss = 0.236588\n",
      "Iteration 157: loss = 0.235899\n",
      "Iteration 158: loss = 0.235219\n",
      "Iteration 159: loss = 0.234548\n",
      "Iteration 160: loss = 0.233886\n",
      "Iteration 161: loss = 0.233231\n",
      "Iteration 162: loss = 0.232584\n",
      "Iteration 163: loss = 0.231944\n",
      "Iteration 164: loss = 0.231312\n",
      "Iteration 165: loss = 0.230689\n",
      "Iteration 166: loss = 0.230075\n",
      "Iteration 167: loss = 0.229469\n",
      "Iteration 168: loss = 0.228872\n",
      "Iteration 169: loss = 0.228284\n",
      "Iteration 170: loss = 0.227703\n",
      "Iteration 171: loss = 0.227130\n",
      "Iteration 172: loss = 0.226563\n",
      "Iteration 173: loss = 0.226002\n",
      "Iteration 174: loss = 0.225446\n",
      "Iteration 175: loss = 0.224896\n",
      "Iteration 176: loss = 0.224351\n",
      "Iteration 177: loss = 0.223812\n",
      "Iteration 178: loss = 0.223278\n",
      "Iteration 179: loss = 0.222750\n",
      "Iteration 180: loss = 0.222229"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/ma00048/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 35: loss = 0.781762\n",
      "Iteration 36: loss = 0.774899\n",
      "Iteration 37: loss = 0.768332\n",
      "Iteration 38: loss = 0.762044\n",
      "Iteration 39: loss = 0.756018\n",
      "Iteration 40: loss = 0.750235\n",
      "Iteration 41: loss = 0.744678\n",
      "Iteration 42: loss = 0.739328\n",
      "Iteration 43: loss = 0.734171\n",
      "Iteration 44: loss = 0.729191\n",
      "Iteration 45: loss = 0.724375\n",
      "Iteration 46: loss = 0.719712\n",
      "Iteration 47: loss = 0.715193\n",
      "Iteration 48: loss = 0.710811\n",
      "Iteration 49: loss = 0.706557\n",
      "Iteration 50: loss = 0.702424\n",
      "Iteration 51: loss = 0.698406\n",
      "Iteration 52: loss = 0.694494\n",
      "Iteration 53: loss = 0.690681\n",
      "Iteration 54: loss = 0.686962\n",
      "Iteration 55: loss = 0.683333\n",
      "Iteration 56: loss = 0.679790\n",
      "Iteration 57: loss = 0.676331\n",
      "Iteration 58: loss = 0.672954\n",
      "Iteration 59: loss = 0.669654\n",
      "Iteration 60: loss = 0.666428\n",
      "Iteration 61: loss = 0.663274\n",
      "Iteration 62: loss = 0.660187\n",
      "Iteration 63: loss = 0.657168\n",
      "Iteration 64: loss = 0.654214\n",
      "Iteration 65: loss = 0.651324\n",
      "Iteration 66: loss = 0.648493\n",
      "Iteration 67: loss = 0.645718\n",
      "Iteration 68: loss = 0.642994\n",
      "Iteration 69: loss = 0.640318\n",
      "Iteration 70: loss = 0.637688\n",
      "Iteration 71: loss = 0.635101\n",
      "Iteration 72: loss = 0.632556\n",
      "Iteration 73: loss = 0.630052\n",
      "Iteration 74: loss = 0.627589\n",
      "Iteration 75: loss = 0.625165\n",
      "Iteration 76: loss = 0.622779\n",
      "Iteration 77: loss = 0.620431\n",
      "Iteration 78: loss = 0.618120\n",
      "Iteration 79: loss = 0.615844\n",
      "Iteration 80: loss = 0.613604\n",
      "Iteration 81: loss = 0.611398\n",
      "Iteration 82: loss = 0.609226\n",
      "Iteration 83: loss = 0.607086\n",
      "Iteration 84: loss = 0.604978\n",
      "Iteration 85: loss = 0.602899\n",
      "Iteration 86: loss = 0.600850\n",
      "Iteration 87: loss = 0.598827\n",
      "Iteration 88: loss = 0.596832\n",
      "Iteration 89: loss = 0.594861\n",
      "Iteration 90: loss = 0.592915\n",
      "Iteration 91: loss = 0.590991\n",
      "Iteration 92: loss = 0.589089\n",
      "Iteration 93: loss = 0.587207\n",
      "Iteration 94: loss = 0.585345\n",
      "Iteration 95: loss = 0.583503\n",
      "Iteration 96: loss = 0.581679\n",
      "Iteration 97: loss = 0.579874\n",
      "Iteration 98: loss = 0.578084\n",
      "Iteration 99: loss = 0.576310\n",
      "Iteration 100: loss = 0.574550\n",
      "Iteration 101: loss = 0.572803\n",
      "Iteration 102: loss = 0.571068\n",
      "Iteration 103: loss = 0.569346\n",
      "Iteration 104: loss = 0.567637\n",
      "Iteration 105: loss = 0.565942\n",
      "Iteration 106: loss = 0.564260\n",
      "Iteration 107: loss = 0.562592\n",
      "Iteration 108: loss = 0.560938\n",
      "Iteration 109: loss = 0.559301\n",
      "Iteration 110: loss = 0.557680\n",
      "Iteration 111: loss = 0.556094\n",
      "Iteration 112: loss = 0.554591\n",
      "Iteration 113: loss = 0.553497\n",
      "Iteration 114: loss = 0.552663\n",
      "Iteration 115: loss = 0.552294\n",
      "Iteration 0: loss = 1.794047\n",
      "Iteration 1: loss = 1.665175\n",
      "Iteration 2: loss = 1.572472\n",
      "Iteration 3: loss = 1.493641\n",
      "Iteration 4: loss = 1.425201\n",
      "Iteration 5: loss = 1.364842\n",
      "Iteration 6: loss = 1.310844\n",
      "Iteration 7: loss = 1.261912\n",
      "Iteration 8: loss = 1.217069\n",
      "Iteration 9: loss = 1.175568\n",
      "Iteration 10: loss = 1.136837\n",
      "Iteration 11: loss = 1.100434\n",
      "Iteration 12: loss = 1.066013\n",
      "Iteration 13: loss = 1.033296\n",
      "Iteration 14: loss = 1.002063\n",
      "Iteration 15: loss = 0.972143\n",
      "Iteration 16: loss = 0.943398\n",
      "Iteration 17: loss = 0.915723\n",
      "Iteration 18: loss = 0.889032\n",
      "Iteration 19: loss = 0.863260\n",
      "Iteration 20: loss = 0.838347\n",
      "Iteration 21: loss = 0.814238\n",
      "Iteration 22: loss = 0.790877\n",
      "Iteration 23: loss = 0.768232\n",
      "Iteration 24: loss = 0.746295\n",
      "Iteration 25: loss = 0.725060\n",
      "Iteration 26: loss = 0.704514\n",
      "Iteration 27: loss = 0.684648\n",
      "Iteration 28: loss = 0.665456\n",
      "Iteration 29: loss = 0.646937\n",
      "Iteration 30: loss = 0.629099\n",
      "Iteration 31: loss = 0.611935\n",
      "Iteration 32: loss = 0.595418\n",
      "Iteration 33: loss = 0.579530\n",
      "Iteration 34: loss = 0.564263\n",
      "Iteration 35: loss = 0.549606\n",
      "Iteration 36: loss = 0.535543\n",
      "Iteration 37: loss = 0.522060\n",
      "Iteration 38: loss = 0.509136\n",
      "Iteration 39: loss = 0.496754\n",
      "Iteration 40: loss = 0.484901\n",
      "Iteration 41: loss = 0.473563\n",
      "Iteration 42: loss = 0.462721\n",
      "Iteration 43: loss = 0.452352\n",
      "Iteration 44: loss = 0.442435\n",
      "Iteration 45: loss = 0.432946\n",
      "Iteration 46: loss = 0.423866\n",
      "Iteration 47: loss = 0.415178\n",
      "Iteration 48: loss = 0.406866\n",
      "Iteration 49: loss = 0.398916\n",
      "Iteration 50: loss = 0.391314\n",
      "Iteration 51: loss = 0.384046\n",
      "Iteration 52: loss = 0.377099\n",
      "Iteration 53: loss = 0.370460\n",
      "Iteration 54: loss = 0.364119\n",
      "Iteration 55: loss = 0.358064\n",
      "Iteration 56: loss = 0.352279\n",
      "Iteration 57: loss = 0.346744\n",
      "Iteration 58: loss = 0.341444\n",
      "Iteration 59: loss = 0.336363\n",
      "Iteration 60: loss = 0.331489\n",
      "Iteration 61: loss = 0.326811\n",
      "Iteration 62: loss = 0.322319\n",
      "Iteration 63: loss = 0.318006\n",
      "Iteration 64: loss = 0.313862\n",
      "Iteration 65: loss = 0.309881\n",
      "Iteration 66: loss = 0.306056\n",
      "Iteration 67: loss = 0.302378\n",
      "Iteration 68: loss = 0.298841\n",
      "Iteration 69: loss = 0.295438\n",
      "Iteration 70: loss = 0.292165\n",
      "Iteration 71: loss = 0.289014\n",
      "Iteration 72: loss = 0.285981\n",
      "Iteration 73: loss = 0.283058\n",
      "Iteration 74: loss = 0.280242\n",
      "Iteration 75: loss = 0.277525\n",
      "Iteration 76: loss = 0.274900\n",
      "Iteration 77: loss = 0.272360\n",
      "Iteration 78: loss = 0.269900\n",
      "Iteration 79: loss = 0.267517\n",
      "Iteration 80: loss = 0.265210\n",
      "Iteration 81: loss = 0.262980\n",
      "Iteration 82: loss = 0.260822\n",
      "Iteration 83: loss = 0.258735\n",
      "Iteration 84: loss = 0.256715\n",
      "Iteration 85: loss = 0.254758\n",
      "Iteration 86: loss = 0.252862\n",
      "Iteration 87: loss = 0.251024\n",
      "Iteration 88: loss = 0.249241\n",
      "Iteration 89: loss = 0.247510\n",
      "Iteration 90: loss = 0.245829\n",
      "Iteration 91: loss = 0.244196\n",
      "Iteration 92: loss = 0.242609\n",
      "Iteration 93: loss = 0.241065\n",
      "Iteration 94: loss = 0.239563\n",
      "Iteration 95: loss = 0.238102\n",
      "Iteration 96: loss = 0.236679\n",
      "Iteration 97: loss = 0.235293\n",
      "Iteration 98: loss = 0.233944\n",
      "Iteration 99: loss = 0.232629\n",
      "Iteration 100: loss = 0.231347\n",
      "Iteration 101: loss = 0.230098\n",
      "Iteration 102: loss = 0.228880\n",
      "Iteration 103: loss = 0.227692\n",
      "Iteration 104: loss = 0.226532\n",
      "Iteration 105: loss = 0.225401\n",
      "Iteration 106: loss = 0.224297\n",
      "Iteration 107: loss = 0.223218\n",
      "Iteration 108: loss = 0.222164\n",
      "Iteration 109: loss = 0.221133\n",
      "Iteration 110: loss = 0.220124\n",
      "Iteration 111: loss = 0.219135\n",
      "Iteration 112: loss = 0.218167\n",
      "Iteration 113: loss = 0.217218\n",
      "Iteration 114: loss = 0.216290\n",
      "Iteration 115: loss = 0.215382\n",
      "Iteration 116: loss = 0.214494\n",
      "Iteration 117: loss = 0.213626\n",
      "Iteration 118: loss = 0.212778\n",
      "Iteration 119: loss = 0.211949\n",
      "Iteration 120: loss = 0.211138\n",
      "Iteration 121: loss = 0.210345\n",
      "Iteration 122: loss = 0.209569\n",
      "Iteration 123: loss = 0.208810\n",
      "Iteration 124: loss = 0.208066\n",
      "Iteration 125: loss = 0.207338\n",
      "Iteration 126: loss = 0.206625\n",
      "Iteration 127: loss = 0.205927\n",
      "Iteration 128: loss = 0.205243\n",
      "Iteration 129: loss = 0.204572\n",
      "Iteration 130: loss = 0.203915\n",
      "Iteration 131: loss = 0.203270\n",
      "Iteration 132: loss = 0.202637\n",
      "Iteration 133: loss = 0.202016\n",
      "Iteration 134: loss = 0.201407\n",
      "Iteration 135: loss = 0.200809\n",
      "Iteration 136: loss = 0.200221\n",
      "Iteration 137: loss = 0.199643\n",
      "Iteration 138: loss = 0.199075\n",
      "Iteration 139: loss = 0.198517\n",
      "Iteration 140: loss = 0.197968\n",
      "Iteration 141: loss = 0.197428\n",
      "Iteration 142: loss = 0.196897\n",
      "Iteration 143: loss = 0.196374\n",
      "Iteration 144: loss = 0.195859\n",
      "Iteration 145: loss = 0.195353\n",
      "Iteration 146: loss = 0.194854\n",
      "Iteration 147: loss = 0.194364\n",
      "Iteration 148: loss = 0.193881\n",
      "Iteration 149: loss = 0.193405\n",
      "Iteration 150: loss = 0.192937\n",
      "Iteration 151: loss = 0.192477\n",
      "Iteration 152: loss = 0.192023\n",
      "Iteration 153: loss = 0.191576\n",
      "Iteration 154: loss = 0.191136\n",
      "Iteration 155: loss = 0.190702\n",
      "Iteration 156: loss = 0.190275\n",
      "Iteration 157: loss = 0.189855\n",
      "Iteration 158: loss = 0.189440\n",
      "Iteration 159: loss = 0.189031\n",
      "Iteration 160: loss = 0.188628\n",
      "Iteration 161: loss = 0.188231\n",
      "Iteration 162: loss = 0.187839\n",
      "Iteration 163: loss = 0.187453\n",
      "Iteration 164: loss = 0.187072\n",
      "Iteration 165: loss = 0.186695\n",
      "Iteration 166: loss = 0.186324\n",
      "Iteration 167: loss = 0.185958\n",
      "Iteration 168: loss = 0.185596\n",
      "Iteration 169: loss = 0.185239\n",
      "Iteration 170: loss = 0.184887\n",
      "Iteration 171: loss = 0.184539\n",
      "Iteration 172: loss = 0.184195\n",
      "Iteration 173: loss = 0.183855\n",
      "Iteration 174: loss = 0.183519\n",
      "Iteration 175: loss = 0.183188\n",
      "Iteration 176: loss = 0.182860\n",
      "Iteration 177: loss = 0.182537\n",
      "Iteration 178: loss = 0.182217\n",
      "Iteration 179: loss = 0.181900\n",
      "Iteration 180: loss = 0.181588\n",
      "Iteration 181: loss = 0.181279\n",
      "Iteration 182: loss = 0.180974\n",
      "Iteration 183: loss = 0.180672\n",
      "Iteration 184: loss = 0.180373\n",
      "Iteration 185: loss = 0.180078\n",
      "Iteration 186: loss = 0.179786\n",
      "Iteration 187: loss = 0.179498\n",
      "Iteration 107: loss = 1.241401\n",
      "Iteration 108: loss = 1.238489\n",
      "Iteration 109: loss = 1.235607\n",
      "Iteration 110: loss = 1.232754\n",
      "Iteration 111: loss = 1.229930\n",
      "Iteration 112: loss = 1.227134\n",
      "Iteration 113: loss = 1.224367\n",
      "Iteration 114: loss = 1.221627\n",
      "Iteration 115: loss = 1.218914\n",
      "Iteration 116: loss = 1.216227\n",
      "Iteration 117: loss = 1.213567\n",
      "Iteration 118: loss = 1.210932\n",
      "Iteration 119: loss = 1.208323\n",
      "Iteration 120: loss = 1.205740\n",
      "Iteration 121: loss = 1.203182\n",
      "Iteration 122: loss = 1.200649\n",
      "Iteration 123: loss = 1.198141\n",
      "Iteration 124: loss = 1.195657\n",
      "Iteration 125: loss = 1.193199\n",
      "Iteration 126: loss = 1.190771\n",
      "Iteration 127: loss = 1.188391\n",
      "Iteration 128: loss = 1.186129\n",
      "Iteration 129: loss = 1.184321\n",
      "Iteration 130: loss = 1.183249\n",
      "Iteration 0: loss = 1.789537\n",
      "Iteration 1: loss = 1.680471\n",
      "Iteration 2: loss = 1.593914\n",
      "Iteration 3: loss = 1.518590\n",
      "Iteration 4: loss = 1.452029\n",
      "Iteration 5: loss = 1.392864\n",
      "Iteration 6: loss = 1.340015\n",
      "Iteration 7: loss = 1.292584\n",
      "Iteration 8: loss = 1.249819\n",
      "Iteration 9: loss = 1.211092\n",
      "Iteration 10: loss = 1.175879\n",
      "Iteration 11: loss = 1.143740\n",
      "Iteration 12: loss = 1.114287\n",
      "Iteration 13: loss = 1.087190\n",
      "Iteration 14: loss = 1.062157\n",
      "Iteration 15: loss = 1.038947\n",
      "Iteration 16: loss = 1.017362\n",
      "Iteration 17: loss = 0.997241\n",
      "Iteration 18: loss = 0.978460\n",
      "Iteration 19: loss = 0.960899\n",
      "Iteration 20: loss = 0.944439\n",
      "Iteration 21: loss = 0.928983\n",
      "Iteration 22: loss = 0.914453\n",
      "Iteration 23: loss = 0.900777\n",
      "Iteration 24: loss = 0.887889\n",
      "Iteration 25: loss = 0.875721\n",
      "Iteration 26: loss = 0.864203\n",
      "Iteration 27: loss = 0.853265\n",
      "Iteration 28: loss = 0.842844\n",
      "Iteration 29: loss = 0.832913\n",
      "Iteration 30: loss = 0.823456\n",
      "Iteration 31: loss = 0.814446\n",
      "Iteration 32: loss = 0.805854\n",
      "Iteration 33: loss = 0.797654\n",
      "Iteration 34: loss = 0.789825\n",
      "Iteration 35: loss = 0.782342\n",
      "Iteration 36: loss = 0.775183\n",
      "Iteration 37: loss = 0.768323\n",
      "Iteration 38: loss = 0.761739\n",
      "Iteration 39: loss = 0.755411\n",
      "Iteration 40: loss = 0.749322\n",
      "Iteration 41: loss = 0.743460\n",
      "Iteration 42: loss = 0.737816\n",
      "Iteration 43: loss = 0.732378\n",
      "Iteration 44: loss = 0.727138\n",
      "Iteration 45: loss = 0.722081\n",
      "Iteration 46: loss = 0.717197\n",
      "Iteration 47: loss = 0.712474\n",
      "Iteration 48: loss = 0.707902\n",
      "Iteration 49: loss = 0.703471\n",
      "Iteration 50: loss = 0.699172\n",
      "Iteration 51: loss = 0.694999\n",
      "Iteration 52: loss = 0.690944\n",
      "Iteration 53: loss = 0.687000\n",
      "Iteration 54: loss = 0.683163\n",
      "Iteration 55: loss = 0.679429\n",
      "Iteration 56: loss = 0.675794\n",
      "Iteration 57: loss = 0.672251\n",
      "Iteration 58: loss = 0.668795\n",
      "Iteration 59: loss = 0.665420\n",
      "Iteration 60: loss = 0.662120\n",
      "Iteration 61: loss = 0.658893\n",
      "Iteration 62: loss = 0.655734\n",
      "Iteration 63: loss = 0.652640\n",
      "Iteration 64: loss = 0.649611\n",
      "Iteration 65: loss = 0.646643\n",
      "Iteration 66: loss = 0.643735\n",
      "Iteration 67: loss = 0.640883\n",
      "Iteration 68: loss = 0.638085\n",
      "Iteration 69: loss = 0.635336\n",
      "Iteration 70: loss = 0.632634\n",
      "Iteration 71: loss = 0.629978\n",
      "Iteration 72: loss = 0.627368\n",
      "Iteration 73: loss = 0.624803\n",
      "Iteration 74: loss = 0.622282\n",
      "Iteration 75: loss = 0.619802\n",
      "Iteration 76: loss = 0.617363\n",
      "Iteration 77: loss = 0.614965\n",
      "Iteration 78: loss = 0.612605\n",
      "Iteration 79: loss = 0.610281\n",
      "Iteration 80: loss = 0.607989\n",
      "Iteration 81: loss = 0.605727\n",
      "Iteration 82: loss = 0.603494\n",
      "Iteration 83: loss = 0.601289\n",
      "Iteration 84: loss = 0.599108\n",
      "Iteration 85: loss = 0.596951\n",
      "Iteration 86: loss = 0.594814\n",
      "Iteration 87: loss = 0.592697\n",
      "Iteration 88: loss = 0.590597\n",
      "Iteration 89: loss = 0.588515\n",
      "Iteration 90: loss = 0.586448\n",
      "Iteration 91: loss = 0.584398\n",
      "Iteration 92: loss = 0.582369\n",
      "Iteration 93: loss = 0.580368\n",
      "Iteration 94: loss = 0.578395\n",
      "Iteration 95: loss = 0.576445\n",
      "Iteration 96: loss = 0.574509\n",
      "Iteration 97: loss = 0.572584\n",
      "Iteration 98: loss = 0.570671\n",
      "Iteration 99: loss = 0.568776\n",
      "Iteration 100: loss = 0.566902\n",
      "Iteration 101: loss = 0.565054\n",
      "Iteration 102: loss = 0.563237\n",
      "Iteration 103: loss = 0.561512\n",
      "Iteration 104: loss = 0.560032\n",
      "Iteration 105: loss = 0.559588\n",
      "Iteration 0: loss = 1.788174\n",
      "Iteration 1: loss = 1.681805\n",
      "Iteration 2: loss = 1.593062\n",
      "Iteration 3: loss = 1.512969\n",
      "Iteration 4: loss = 1.439658\n",
      "Iteration 5: loss = 1.372227\n",
      "Iteration 6: loss = 1.309997\n",
      "Iteration 7: loss = 1.252402\n",
      "Iteration 8: loss = 1.198958\n",
      "Iteration 9: loss = 1.149261\n",
      "Iteration 10: loss = 1.102953\n",
      "Iteration 11: loss = 1.059706\n",
      "Iteration 12: loss = 1.019241\n",
      "Iteration 13: loss = 0.981340\n",
      "Iteration 14: loss = 0.945824\n",
      "Iteration 15: loss = 0.912513\n",
      "Iteration 16: loss = 0.881218\n",
      "Iteration 17: loss = 0.851759\n",
      "Iteration 18: loss = 0.823976\n",
      "Iteration 19: loss = 0.797734\n",
      "Iteration 20: loss = 0.772921\n",
      "Iteration 21: loss = 0.749464\n",
      "Iteration 22: loss = 0.727307\n",
      "Iteration 23: loss = 0.706381\n",
      "Iteration 24: loss = 0.686597\n",
      "Iteration 25: loss = 0.667867\n",
      "Iteration 26: loss = 0.650112\n",
      "Iteration 27: loss = 0.633269\n",
      "Iteration 28: loss = 0.617281\n",
      "Iteration 29: loss = 0.602093\n",
      "Iteration 30: loss = 0.587657\n",
      "Iteration 31: loss = 0.573931\n",
      "Iteration 32: loss = 0.560877\n",
      "Iteration 33: loss = 0.548458\n",
      "Iteration 34: loss = 0.536636\n",
      "Iteration 35: loss = 0.525375\n",
      "Iteration 36: loss = 0.514642\n",
      "Iteration 37: loss = 0.504402\n",
      "Iteration 38: loss = 0.494625\n",
      "Iteration 39: loss = 0.485283\n",
      "Iteration 40: loss = 0.476350\n",
      "Iteration 41: loss = 0.467800\n",
      "Iteration 42: loss = 0.459606\n",
      "Iteration 43: loss = 0.451740\n",
      "Iteration 44: loss = 0.444180\n",
      "Iteration 45: loss = 0.436911\n",
      "Iteration 46: loss = 0.429926\n",
      "Iteration 47: loss = 0.423215\n",
      "Iteration 48: loss = 0.416770\n",
      "Iteration 49: loss = 0.410577\n",
      "Iteration 50: loss = 0.404620\n",
      "Iteration 51: loss = 0.398884\n",
      "Iteration 52: loss = 0.393356\n",
      "Iteration 53: loss = 0.388021\n",
      "Iteration 54: loss = 0.382867\n",
      "Iteration 55: loss = 0.377887\n",
      "Iteration 56: loss = 0.373075\n",
      "Iteration 57: loss = 0.368425\n",
      "Iteration 58: loss = 0.363931\n",
      "Iteration 59: loss = 0.359588\n",
      "Iteration 60: loss = 0.355390\n",
      "Iteration 61: loss = 0.351329\n",
      "Iteration 62: loss = 0.347397\n",
      "Iteration 63: loss = 0.343588\n",
      "Iteration 64: loss = 0.339897\n",
      "Iteration 65: loss = 0.336318\n",
      "Iteration 66: loss = 0.332848\n",
      "Iteration 67: loss = 0.329483\n",
      "Iteration 68: loss = 0.326218\n",
      "Iteration 69: loss = 0.323051\n",
      "Iteration 70: loss = 0.319977\n",
      "Iteration 71: loss = 0.316993\n",
      "Iteration 72: loss = 0.314096\n",
      "Iteration 73: loss = 0.311282\n",
      "Iteration 74: loss = 0.308548\n",
      "Iteration 75: loss = 0.305890\n",
      "Iteration 76: loss = 0.303304\n",
      "Iteration 77: loss = 0.300788\n",
      "Iteration 78: loss = 0.298338\n",
      "Iteration 79: loss = 0.295952\n",
      "Iteration 80: loss = 0.293628\n",
      "Iteration 81: loss = 0.291362\n",
      "Iteration 82: loss = 0.289153\n",
      "Iteration 83: loss = 0.287000\n",
      "Iteration 84: loss = 0.284901\n",
      "Iteration 85: loss = 0.282855\n",
      "Iteration 86: loss = 0.280860\n",
      "Iteration 87: loss = 0.278915\n",
      "Iteration 88: loss = 0.277019\n",
      "Iteration 89: loss = 0.275168\n",
      "Iteration 90: loss = 0.273363\n",
      "Iteration 91: loss = 0.271600\n",
      "Iteration 92: loss = 0.269879\n",
      "Iteration 93: loss = 0.268199\n",
      "Iteration 94: loss = 0.266556\n",
      "Iteration 95: loss = 0.264951\n",
      "Iteration 96: loss = 0.263382\n",
      "Iteration 97: loss = 0.261846\n",
      "Iteration 98: loss = 0.260343\n",
      "Iteration 99: loss = 0.258873\n",
      "Iteration 100: loss = 0.257434\n",
      "Iteration 101: loss = 0.256027\n",
      "Iteration 102: loss = 0.254650\n",
      "Iteration 103: loss = 0.253301\n",
      "Iteration 104: loss = 0.251978\n",
      "Iteration 105: loss = 0.250680\n",
      "Iteration 106: loss = 0.249407\n",
      "Iteration 107: loss = 0.248158\n",
      "Iteration 108: loss = 0.246934\n",
      "Iteration 109: loss = 0.245734\n",
      "Iteration 110: loss = 0.244559\n",
      "Iteration 111: loss = 0.243408\n",
      "Iteration 112: loss = 0.242279\n",
      "Iteration 113: loss = 0.241172\n",
      "Iteration 114: loss = 0.240086\n",
      "Iteration 115: loss = 0.239021\n",
      "Iteration 116: loss = 0.237976\n",
      "Iteration 117: loss = 0.236950\n",
      "Iteration 118: loss = 0.235945\n",
      "Iteration 119: loss = 0.234958\n",
      "Iteration 120: loss = 0.233990\n",
      "Iteration 121: loss = 0.233039\n",
      "Iteration 122: loss = 0.232106\n",
      "Iteration 123: loss = 0.231189\n",
      "Iteration 124: loss = 0.230289\n",
      "Iteration 125: loss = 0.229405\n",
      "Iteration 126: loss = 0.228536\n",
      "Iteration 127: loss = 0.227682\n",
      "Iteration 128: loss = 0.226842\n",
      "Iteration 129: loss = 0.226017\n",
      "Iteration 130: loss = 0.225205\n",
      "Iteration 131: loss = 0.224407\n",
      "Iteration 132: loss = 0.223621\n",
      "Iteration 133: loss = 0.222848\n",
      "Iteration 134: loss = 0.222087\n",
      "Iteration 135: loss = 0.221338\n",
      "Iteration 136: loss = 0.220600\n",
      "Iteration 137: loss = 0.219873\n",
      "Iteration 138: loss = 0.219157\n",
      "Iteration 139: loss = 0.218450\n",
      "Iteration 140: loss = 0.217754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25: loss = 0.853779\n",
      "Iteration 26: loss = 0.842146\n",
      "Iteration 27: loss = 0.831150\n",
      "Iteration 28: loss = 0.820738\n",
      "Iteration 29: loss = 0.810886\n",
      "Iteration 30: loss = 0.801566\n",
      "Iteration 31: loss = 0.792736\n",
      "Iteration 32: loss = 0.784355\n",
      "Iteration 33: loss = 0.776388\n",
      "Iteration 34: loss = 0.768809\n",
      "Iteration 35: loss = 0.761594\n",
      "Iteration 36: loss = 0.754715\n",
      "Iteration 37: loss = 0.748148\n",
      "Iteration 38: loss = 0.741868\n",
      "Iteration 39: loss = 0.735855\n",
      "Iteration 40: loss = 0.730087\n",
      "Iteration 41: loss = 0.724546\n",
      "Iteration 42: loss = 0.719218\n",
      "Iteration 43: loss = 0.714093\n",
      "Iteration 44: loss = 0.709160\n",
      "Iteration 45: loss = 0.704411\n",
      "Iteration 46: loss = 0.699834\n",
      "Iteration 47: loss = 0.695412\n",
      "Iteration 48: loss = 0.691125\n",
      "Iteration 49: loss = 0.686966\n",
      "Iteration 50: loss = 0.682936\n",
      "Iteration 51: loss = 0.679031\n",
      "Iteration 52: loss = 0.675244\n",
      "Iteration 53: loss = 0.671567\n",
      "Iteration 54: loss = 0.667994\n",
      "Iteration 55: loss = 0.664520\n",
      "Iteration 56: loss = 0.661139\n",
      "Iteration 57: loss = 0.657845\n",
      "Iteration 58: loss = 0.654632\n",
      "Iteration 59: loss = 0.651496\n",
      "Iteration 60: loss = 0.648431\n",
      "Iteration 61: loss = 0.645433\n",
      "Iteration 62: loss = 0.642498\n",
      "Iteration 63: loss = 0.639625\n",
      "Iteration 64: loss = 0.636810\n",
      "Iteration 65: loss = 0.634050\n",
      "Iteration 66: loss = 0.631344\n",
      "Iteration 67: loss = 0.628691\n",
      "Iteration 68: loss = 0.626089\n",
      "Iteration 69: loss = 0.623538\n",
      "Iteration 70: loss = 0.621036\n",
      "Iteration 71: loss = 0.618582\n",
      "Iteration 72: loss = 0.616173\n",
      "Iteration 73: loss = 0.613806\n",
      "Iteration 74: loss = 0.611480\n",
      "Iteration 75: loss = 0.609190\n",
      "Iteration 76: loss = 0.606934\n",
      "Iteration 77: loss = 0.604714\n",
      "Iteration 78: loss = 0.602526\n",
      "Iteration 79: loss = 0.600372\n",
      "Iteration 80: loss = 0.598250\n",
      "Iteration 81: loss = 0.596159\n",
      "Iteration 82: loss = 0.594095\n",
      "Iteration 83: loss = 0.592059\n",
      "Iteration 84: loss = 0.590049\n",
      "Iteration 85: loss = 0.588065\n",
      "Iteration 86: loss = 0.586108\n",
      "Iteration 87: loss = 0.584176\n",
      "Iteration 88: loss = 0.582269\n",
      "Iteration 89: loss = 0.580386\n",
      "Iteration 90: loss = 0.578526\n",
      "Iteration 91: loss = 0.576689\n",
      "Iteration 92: loss = 0.574872\n",
      "Iteration 93: loss = 0.573075\n",
      "Iteration 94: loss = 0.571297\n",
      "Iteration 95: loss = 0.569539\n",
      "Iteration 96: loss = 0.567799\n",
      "Iteration 97: loss = 0.566078\n",
      "Iteration 98: loss = 0.564375\n",
      "Iteration 99: loss = 0.562690\n",
      "Iteration 100: loss = 0.561021\n",
      "Iteration 101: loss = 0.559369\n",
      "Iteration 102: loss = 0.557733\n",
      "Iteration 103: loss = 0.556112\n",
      "Iteration 104: loss = 0.554507\n",
      "Iteration 105: loss = 0.552917\n",
      "Iteration 106: loss = 0.551344\n",
      "Iteration 107: loss = 0.549786\n",
      "Iteration 108: loss = 0.548244\n",
      "Iteration 109: loss = 0.546717\n",
      "Iteration 110: loss = 0.545204\n",
      "Iteration 111: loss = 0.543704\n",
      "Iteration 112: loss = 0.542217\n",
      "Iteration 113: loss = 0.540742\n",
      "Iteration 114: loss = 0.539281\n",
      "Iteration 115: loss = 0.537835\n",
      "Iteration 116: loss = 0.536418\n",
      "Iteration 117: loss = 0.535087\n",
      "Iteration 118: loss = 0.534118\n",
      "Iteration 119: loss = 0.533345\n",
      "Iteration 120: loss = 0.533006\n",
      "Iteration 0: loss = 1.789984\n",
      "Iteration 1: loss = 1.670512\n",
      "Iteration 2: loss = 1.573429\n",
      "Iteration 3: loss = 1.486390\n",
      "Iteration 4: loss = 1.407240\n",
      "Iteration 5: loss = 1.334958\n",
      "Iteration 6: loss = 1.268755\n",
      "Iteration 7: loss = 1.207967\n",
      "Iteration 8: loss = 1.152023\n",
      "Iteration 9: loss = 1.100425\n",
      "Iteration 10: loss = 1.052729\n",
      "Iteration 11: loss = 1.008530\n",
      "Iteration 12: loss = 0.967489\n",
      "Iteration 13: loss = 0.929319\n",
      "Iteration 14: loss = 0.893763\n",
      "Iteration 15: loss = 0.860547\n",
      "Iteration 16: loss = 0.829455\n",
      "Iteration 17: loss = 0.800341\n",
      "Iteration 18: loss = 0.773041\n",
      "Iteration 19: loss = 0.747364\n",
      "Iteration 20: loss = 0.723201\n",
      "Iteration 21: loss = 0.700522\n",
      "Iteration 22: loss = 0.679173\n",
      "Iteration 23: loss = 0.659006\n",
      "Iteration 24: loss = 0.639950\n",
      "Iteration 25: loss = 0.621944\n",
      "Iteration 26: loss = 0.604957\n",
      "Iteration 27: loss = 0.588931\n",
      "Iteration 28: loss = 0.573787\n",
      "Iteration 29: loss = 0.559462\n",
      "Iteration 30: loss = 0.545878\n",
      "Iteration 31: loss = 0.532970\n",
      "Iteration 32: loss = 0.520710\n",
      "Iteration 33: loss = 0.509071\n",
      "Iteration 34: loss = 0.498024\n",
      "Iteration 35: loss = 0.487540\n",
      "Iteration 36: loss = 0.477558\n",
      "Iteration 37: loss = 0.468035\n",
      "Iteration 38: loss = 0.458956\n",
      "Iteration 39: loss = 0.450298\n",
      "Iteration 40: loss = 0.442037\n",
      "Iteration 41: loss = 0.434155\n",
      "Iteration 42: loss = 0.426629\n",
      "Iteration 43: loss = 0.419439\n",
      "Iteration 44: loss = 0.412560\n",
      "Iteration 45: loss = 0.405973\n",
      "Iteration 46: loss = 0.399658\n",
      "Iteration 47: loss = 0.393601\n",
      "Iteration 48: loss = 0.387793\n",
      "Iteration 49: loss = 0.382222\n",
      "Iteration 50: loss = 0.376875\n",
      "Iteration 51: loss = 0.371738\n",
      "Iteration 52: loss = 0.366798\n",
      "Iteration 53: loss = 0.362044\n",
      "Iteration 54: loss = 0.357467\n",
      "Iteration 55: loss = 0.353055\n",
      "Iteration 56: loss = 0.348799\n",
      "Iteration 57: loss = 0.344690\n",
      "Iteration 58: loss = 0.340719\n",
      "Iteration 59: loss = 0.336877\n",
      "Iteration 60: loss = 0.333159\n",
      "Iteration 61: loss = 0.329559\n",
      "Iteration 62: loss = 0.326073\n",
      "Iteration 63: loss = 0.322697\n",
      "Iteration 64: loss = 0.319426\n",
      "Iteration 65: loss = 0.316254\n",
      "Iteration 66: loss = 0.313178\n",
      "Iteration 67: loss = 0.310193\n",
      "Iteration 68: loss = 0.307295\n",
      "Iteration 69: loss = 0.304481\n",
      "Iteration 70: loss = 0.301747\n",
      "Iteration 71: loss = 0.299091\n",
      "Iteration 72: loss = 0.296511\n",
      "Iteration 73: loss = 0.294005\n",
      "Iteration 74: loss = 0.291569\n",
      "Iteration 75: loss = 0.289201\n",
      "Iteration 76: loss = 0.286899\n",
      "Iteration 77: loss = 0.284659\n",
      "Iteration 78: loss = 0.282478\n",
      "Iteration 79: loss = 0.280355\n",
      "Iteration 80: loss = 0.278286\n",
      "Iteration 81: loss = 0.276269\n",
      "Iteration 82: loss = 0.274303\n",
      "Iteration 83: loss = 0.272385\n",
      "Iteration 84: loss = 0.270514\n",
      "Iteration 85: loss = 0.268689\n",
      "Iteration 86: loss = 0.266908\n",
      "Iteration 87: loss = 0.265169\n",
      "Iteration 88: loss = 0.263473\n",
      "Iteration 89: loss = 0.261817\n",
      "Iteration 90: loss = 0.260200\n",
      "Iteration 91: loss = 0.258621\n",
      "Iteration 92: loss = 0.257079\n",
      "Iteration 93: loss = 0.255572\n",
      "Iteration 94: loss = 0.254100\n",
      "Iteration 95: loss = 0.252662\n",
      "Iteration 96: loss = 0.251257\n",
      "Iteration 97: loss = 0.249886\n",
      "Iteration 98: loss = 0.248547\n",
      "Iteration 99: loss = 0.247239\n",
      "Iteration 100: loss = 0.245962\n",
      "Iteration 101: loss = 0.244713\n",
      "Iteration 102: loss = 0.243492\n",
      "Iteration 103: loss = 0.242297\n",
      "Iteration 104: loss = 0.241128\n",
      "Iteration 105: loss = 0.239983\n",
      "Iteration 106: loss = 0.238861\n",
      "Iteration 107: loss = 0.237763\n",
      "Iteration 108: loss = 0.236688\n",
      "Iteration 109: loss = 0.235634\n",
      "Iteration 110: loss = 0.234602\n",
      "Iteration 111: loss = 0.233590\n",
      "Iteration 112: loss = 0.232598\n",
      "Iteration 113: loss = 0.231625\n",
      "Iteration 114: loss = 0.230670\n",
      "Iteration 115: loss = 0.229733\n",
      "Iteration 116: loss = 0.228813\n",
      "Iteration 117: loss = 0.227909\n",
      "Iteration 118: loss = 0.227020\n",
      "Iteration 119: loss = 0.226145\n",
      "Iteration 120: loss = 0.225285\n",
      "Iteration 121: loss = 0.224438\n",
      "Iteration 122: loss = 0.223604\n",
      "Iteration 123: loss = 0.222782\n",
      "Iteration 124: loss = 0.221974\n",
      "Iteration 125: loss = 0.221178\n",
      "Iteration 126: loss = 0.220394\n",
      "Iteration 127: loss = 0.219622\n",
      "Iteration 128: loss = 0.218863\n",
      "Iteration 129: loss = 0.218115\n",
      "Iteration 130: loss = 0.217379\n",
      "Iteration 131: loss = 0.216653\n",
      "Iteration 132: loss = 0.215939\n",
      "Iteration 133: loss = 0.215234\n",
      "Iteration 134: loss = 0.214539\n",
      "Iteration 135: loss = 0.213853\n",
      "Iteration 136: loss = 0.213177\n",
      "Iteration 137: loss = 0.212508\n",
      "Iteration 138: loss = 0.211848\n",
      "Iteration 139: loss = 0.211195\n",
      "Iteration 140: loss = 0.210550\n",
      "Iteration 141: loss = 0.209912\n",
      "Iteration 142: loss = 0.209281\n",
      "Iteration 143: loss = 0.208657\n",
      "Iteration 144: loss = 0.208040\n",
      "Iteration 145: loss = 0.207430\n",
      "Iteration 146: loss = 0.206828\n",
      "Iteration 147: loss = 0.206233\n",
      "Iteration 148: loss = 0.205646\n",
      "Iteration 149: loss = 0.205066\n",
      "Iteration 150: loss = 0.204495\n",
      "Iteration 151: loss = 0.203931\n",
      "Iteration 152: loss = 0.203376\n",
      "Iteration 153: loss = 0.202828\n",
      "Iteration 154: loss = 0.202288\n",
      "Iteration 155: loss = 0.201755\n",
      "Iteration 156: loss = 0.201229\n",
      "Iteration 157: loss = 0.200710\n",
      "Iteration 158: loss = 0.200197\n",
      "Iteration 159: loss = 0.199691\n",
      "Iteration 160: loss = 0.199192\n",
      "Iteration 161: loss = 0.198698\n",
      "Iteration 162: loss = 0.198210\n",
      "Iteration 163: loss = 0.197728\n",
      "Iteration 164: loss = 0.197251\n",
      "Iteration 165: loss = 0.196780\n",
      "Iteration 166: loss = 0.196314\n",
      "Iteration 167: loss = 0.195853\n",
      "Iteration 168: loss = 0.195397\n",
      "Iteration 169: loss = 0.194946\n",
      "Iteration 170: loss = 0.194500\n",
      "Iteration 171: loss = 0.194059\n",
      "Iteration 172: loss = 0.193623\n",
      "Iteration 173: loss = 0.193192\n",
      "Iteration 40: loss = 0.878103\n",
      "Iteration 41: loss = 0.868739\n",
      "Iteration 42: loss = 0.859821\n",
      "Iteration 43: loss = 0.851302\n",
      "Iteration 44: loss = 0.843151\n",
      "Iteration 45: loss = 0.835342\n",
      "Iteration 46: loss = 0.827839\n",
      "Iteration 47: loss = 0.820599\n",
      "Iteration 48: loss = 0.813597\n",
      "Iteration 49: loss = 0.806824\n",
      "Iteration 50: loss = 0.800270\n",
      "Iteration 51: loss = 0.793918\n",
      "Iteration 52: loss = 0.787759\n",
      "Iteration 53: loss = 0.781787\n",
      "Iteration 54: loss = 0.775995\n",
      "Iteration 55: loss = 0.770366\n",
      "Iteration 56: loss = 0.764866\n",
      "Iteration 57: loss = 0.759468\n",
      "Iteration 58: loss = 0.754187\n",
      "Iteration 59: loss = 0.749045\n",
      "Iteration 60: loss = 0.744035\n",
      "Iteration 61: loss = 0.739123\n",
      "Iteration 62: loss = 0.734294\n",
      "Iteration 63: loss = 0.729542\n",
      "Iteration 64: loss = 0.724844\n",
      "Iteration 65: loss = 0.720134\n",
      "Iteration 66: loss = 0.715466\n",
      "Iteration 67: loss = 0.710870\n",
      "Iteration 68: loss = 0.706353\n",
      "Iteration 69: loss = 0.701916\n",
      "Iteration 70: loss = 0.697542\n",
      "Iteration 71: loss = 0.693237\n",
      "Iteration 72: loss = 0.689002\n",
      "Iteration 73: loss = 0.684806\n",
      "Iteration 74: loss = 0.680646\n",
      "Iteration 75: loss = 0.676552\n",
      "Iteration 76: loss = 0.672525\n",
      "Iteration 77: loss = 0.668554\n",
      "Iteration 78: loss = 0.664605\n",
      "Iteration 79: loss = 0.660608\n",
      "Iteration 80: loss = 0.656663\n",
      "Iteration 81: loss = 0.652785\n",
      "Iteration 82: loss = 0.648943\n",
      "Iteration 83: loss = 0.645151\n",
      "Iteration 84: loss = 0.641441\n",
      "Iteration 85: loss = 0.637818\n",
      "Iteration 86: loss = 0.634257\n",
      "Iteration 87: loss = 0.630752\n",
      "Iteration 88: loss = 0.627317\n",
      "Iteration 89: loss = 0.623945\n",
      "Iteration 90: loss = 0.620621\n",
      "Iteration 91: loss = 0.617366\n",
      "Iteration 92: loss = 0.614194\n",
      "Iteration 93: loss = 0.611131\n",
      "Iteration 94: loss = 0.608183\n",
      "Iteration 95: loss = 0.605461\n",
      "Iteration 96: loss = 0.603294\n",
      "Iteration 97: loss = 0.603157\n",
      "Iteration 0: loss = 1.796931\n",
      "Iteration 1: loss = 1.538787\n",
      "Iteration 2: loss = 1.391187\n",
      "Iteration 3: loss = 1.287273\n",
      "Iteration 4: loss = 1.219404\n",
      "Iteration 5: loss = 1.155770\n",
      "Iteration 6: loss = 1.112759\n",
      "Iteration 7: loss = 1.064020\n",
      "Iteration 8: loss = 1.030060\n",
      "Iteration 9: loss = 0.992064\n",
      "Iteration 10: loss = 0.963324\n",
      "Iteration 11: loss = 0.934638\n",
      "Iteration 12: loss = 0.911077\n",
      "Iteration 13: loss = 0.889364\n",
      "Iteration 14: loss = 0.870465\n",
      "Iteration 15: loss = 0.853297\n",
      "Iteration 16: loss = 0.837766\n",
      "Iteration 17: loss = 0.823436\n",
      "Iteration 18: loss = 0.810160\n",
      "Iteration 19: loss = 0.797737\n",
      "Iteration 20: loss = 0.786083\n",
      "Iteration 21: loss = 0.775086\n",
      "Iteration 22: loss = 0.764680\n",
      "Iteration 23: loss = 0.754791\n",
      "Iteration 24: loss = 0.745381\n",
      "Iteration 25: loss = 0.736412\n",
      "Iteration 26: loss = 0.727849\n",
      "Iteration 27: loss = 0.719659\n",
      "Iteration 28: loss = 0.711824\n",
      "Iteration 29: loss = 0.704321\n",
      "Iteration 30: loss = 0.697128\n",
      "Iteration 31: loss = 0.690217\n",
      "Iteration 32: loss = 0.683566\n",
      "Iteration 33: loss = 0.677158\n",
      "Iteration 34: loss = 0.670978\n",
      "Iteration 35: loss = 0.665009\n",
      "Iteration 36: loss = 0.659239\n",
      "Iteration 37: loss = 0.653654\n",
      "Iteration 38: loss = 0.648242\n",
      "Iteration 39: loss = 0.642995\n",
      "Iteration 40: loss = 0.637909\n",
      "Iteration 41: loss = 0.632977\n",
      "Iteration 42: loss = 0.628194\n",
      "Iteration 43: loss = 0.623551\n",
      "Iteration 44: loss = 0.619041\n",
      "Iteration 45: loss = 0.614658\n",
      "Iteration 46: loss = 0.610397\n",
      "Iteration 47: loss = 0.606251\n",
      "Iteration 48: loss = 0.602215\n",
      "Iteration 49: loss = 0.598285\n",
      "Iteration 50: loss = 0.594457\n",
      "Iteration 51: loss = 0.590727\n",
      "Iteration 52: loss = 0.587092\n",
      "Iteration 53: loss = 0.583549\n",
      "Iteration 54: loss = 0.580095\n",
      "Iteration 55: loss = 0.576727\n",
      "Iteration 56: loss = 0.573442\n",
      "Iteration 57: loss = 0.570236\n",
      "Iteration 58: loss = 0.567107\n",
      "Iteration 59: loss = 0.564050\n",
      "Iteration 60: loss = 0.561065\n",
      "Iteration 61: loss = 0.558148\n",
      "Iteration 62: loss = 0.555297\n",
      "Iteration 63: loss = 0.552509\n",
      "Iteration 64: loss = 0.549784\n",
      "Iteration 65: loss = 0.547118\n",
      "Iteration 66: loss = 0.544511\n",
      "Iteration 67: loss = 0.541959\n",
      "Iteration 68: loss = 0.539461\n",
      "Iteration 69: loss = 0.537016\n",
      "Iteration 70: loss = 0.534620\n",
      "Iteration 71: loss = 0.532271\n",
      "Iteration 72: loss = 0.529966\n",
      "Iteration 73: loss = 0.527704\n",
      "Iteration 74: loss = 0.525482\n",
      "Iteration 75: loss = 0.523301\n",
      "Iteration 76: loss = 0.521160\n",
      "Iteration 77: loss = 0.519058\n",
      "Iteration 78: loss = 0.516995\n",
      "Iteration 79: loss = 0.514969\n",
      "Iteration 80: loss = 0.512978\n",
      "Iteration 81: loss = 0.511023\n",
      "Iteration 82: loss = 0.509102\n",
      "Iteration 83: loss = 0.507215\n",
      "Iteration 84: loss = 0.505360\n",
      "Iteration 85: loss = 0.503536\n",
      "Iteration 86: loss = 0.501742\n",
      "Iteration 87: loss = 0.499978\n",
      "Iteration 88: loss = 0.498242\n",
      "Iteration 89: loss = 0.496535\n",
      "Iteration 90: loss = 0.494855\n",
      "Iteration 91: loss = 0.493201\n",
      "Iteration 92: loss = 0.491574\n",
      "Iteration 93: loss = 0.489973\n",
      "Iteration 94: loss = 0.488396\n",
      "Iteration 95: loss = 0.486845\n",
      "Iteration 96: loss = 0.485317\n",
      "Iteration 97: loss = 0.483813\n",
      "Iteration 98: loss = 0.482331\n",
      "Iteration 99: loss = 0.480872\n",
      "Iteration 100: loss = 0.479435\n",
      "Iteration 101: loss = 0.478020\n",
      "Iteration 102: loss = 0.476625\n",
      "Iteration 103: loss = 0.475251\n",
      "Iteration 104: loss = 0.473897\n",
      "Iteration 105: loss = 0.472562\n",
      "Iteration 106: loss = 0.471245\n",
      "Iteration 107: loss = 0.469947\n",
      "Iteration 108: loss = 0.468667\n",
      "Iteration 109: loss = 0.467404\n",
      "Iteration 110: loss = 0.466158\n",
      "Iteration 111: loss = 0.464930\n",
      "Iteration 112: loss = 0.463718\n",
      "Iteration 113: loss = 0.462522\n",
      "Iteration 114: loss = 0.461342\n",
      "Iteration 115: loss = 0.460178\n",
      "Iteration 116: loss = 0.459028\n",
      "Iteration 117: loss = 0.457893\n",
      "Iteration 118: loss = 0.456773\n",
      "Iteration 119: loss = 0.455666\n",
      "Iteration 120: loss = 0.454573\n",
      "Iteration 121: loss = 0.453493\n",
      "Iteration 122: loss = 0.452426\n",
      "Iteration 123: loss = 0.451371\n",
      "Iteration 124: loss = 0.450328\n",
      "Iteration 125: loss = 0.449296\n",
      "Iteration 126: loss = 0.448277\n",
      "Iteration 127: loss = 0.447269\n",
      "Iteration 128: loss = 0.446271\n",
      "Iteration 129: loss = 0.445285\n",
      "Iteration 130: loss = 0.444310\n",
      "Iteration 131: loss = 0.443346\n",
      "Iteration 132: loss = 0.442392\n",
      "Iteration 133: loss = 0.441448\n",
      "Iteration 134: loss = 0.440515\n",
      "Iteration 135: loss = 0.439592\n",
      "Iteration 136: loss = 0.438679\n",
      "Iteration 137: loss = 0.437775\n",
      "Iteration 138: loss = 0.436881\n",
      "Iteration 139: loss = 0.435996\n",
      "Iteration 140: loss = 0.435120\n",
      "Iteration 141: loss = 0.434253\n",
      "Iteration 142: loss = 0.433394\n",
      "Iteration 143: loss = 0.432544\n",
      "Iteration 144: loss = 0.431703\n",
      "Iteration 145: loss = 0.430869\n",
      "Iteration 146: loss = 0.430044\n",
      "Iteration 147: loss = 0.429226\n",
      "Iteration 148: loss = 0.428416\n",
      "Iteration 149: loss = 0.427614\n",
      "Iteration 150: loss = 0.426820\n",
      "Iteration 151: loss = 0.426032\n",
      "Iteration 152: loss = 0.425252\n",
      "Iteration 153: loss = 0.424480\n",
      "Iteration 154: loss = 0.423714\n",
      "Iteration 155: loss = 0.422956\n",
      "Iteration 156: loss = 0.422204\n",
      "Iteration 157: loss = 0.421461\n",
      "Iteration 158: loss = 0.420730\n",
      "Iteration 159: loss = 0.420026\n",
      "Iteration 160: loss = 0.419387\n",
      "Iteration 161: loss = 0.418973\n",
      "Iteration 0: loss = 1.793871\n",
      "Iteration 1: loss = 1.532553\n",
      "Iteration 2: loss = 1.372628\n",
      "Iteration 3: loss = 1.253239\n",
      "Iteration 4: loss = 1.172603\n",
      "Iteration 5: loss = 1.093244\n",
      "Iteration 6: loss = 1.039297\n",
      "Iteration 7: loss = 0.972516\n",
      "Iteration 8: loss = 0.926478\n",
      "Iteration 9: loss = 0.871645\n",
      "Iteration 10: loss = 0.830068\n",
      "Iteration 11: loss = 0.787989\n",
      "Iteration 12: loss = 0.752979\n",
      "Iteration 13: loss = 0.720777\n",
      "Iteration 14: loss = 0.692323\n",
      "Iteration 15: loss = 0.666271\n",
      "Iteration 16: loss = 0.642345\n",
      "Iteration 17: loss = 0.620043\n",
      "Iteration 18: loss = 0.599184\n",
      "Iteration 19: loss = 0.579567\n",
      "Iteration 20: loss = 0.561088\n",
      "Iteration 21: loss = 0.543640\n",
      "Iteration 22: loss = 0.527143\n",
      "Iteration 23: loss = 0.511521\n",
      "Iteration 24: loss = 0.496715\n",
      "Iteration 25: loss = 0.482667\n",
      "Iteration 26: loss = 0.469328\n",
      "Iteration 27: loss = 0.456653\n",
      "Iteration 28: loss = 0.444599\n",
      "Iteration 29: loss = 0.433129\n",
      "Iteration 30: loss = 0.422204\n",
      "Iteration 31: loss = 0.411793\n",
      "Iteration 32: loss = 0.401867\n",
      "Iteration 33: loss = 0.392402\n",
      "Iteration 34: loss = 0.383371\n",
      "Iteration 35: loss = 0.374753\n",
      "Iteration 36: loss = 0.366525\n",
      "Iteration 37: loss = 0.358666\n",
      "Iteration 38: loss = 0.351157\n",
      "Iteration 39: loss = 0.343979\n",
      "Iteration 40: loss = 0.337116\n",
      "Iteration 41: loss = 0.330549\n",
      "Iteration 42: loss = 0.324265\n",
      "Iteration 43: loss = 0.318247\n",
      "Iteration 44: loss = 0.312484\n",
      "Iteration 45: loss = 0.306962\n",
      "Iteration 46: loss = 0.301668\n",
      "Iteration 47: loss = 0.296593\n",
      "Iteration 48: loss = 0.291723\n",
      "Iteration 49: loss = 0.287050\n",
      "Iteration 50: loss = 0.282562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 92: loss = 0.599363\n",
      "Iteration 93: loss = 0.597528\n",
      "Iteration 94: loss = 0.595729\n",
      "Iteration 95: loss = 0.593964\n",
      "Iteration 96: loss = 0.592232\n",
      "Iteration 97: loss = 0.590533\n",
      "Iteration 98: loss = 0.588866\n",
      "Iteration 99: loss = 0.587229\n",
      "Iteration 100: loss = 0.585621\n",
      "Iteration 101: loss = 0.584042\n",
      "Iteration 102: loss = 0.582490\n",
      "Iteration 103: loss = 0.580964\n",
      "Iteration 104: loss = 0.579462\n",
      "Iteration 105: loss = 0.577985\n",
      "Iteration 106: loss = 0.576531\n",
      "Iteration 107: loss = 0.575098\n",
      "Iteration 108: loss = 0.573687\n",
      "Iteration 109: loss = 0.572297\n",
      "Iteration 110: loss = 0.570927\n",
      "Iteration 111: loss = 0.569576\n",
      "Iteration 112: loss = 0.568244\n",
      "Iteration 113: loss = 0.566932\n",
      "Iteration 114: loss = 0.565637\n",
      "Iteration 115: loss = 0.564360\n",
      "Iteration 116: loss = 0.563101\n",
      "Iteration 117: loss = 0.561859\n",
      "Iteration 118: loss = 0.560634\n",
      "Iteration 119: loss = 0.559426\n",
      "Iteration 120: loss = 0.558233\n",
      "Iteration 121: loss = 0.557056\n",
      "Iteration 122: loss = 0.555895\n",
      "Iteration 123: loss = 0.554749\n",
      "Iteration 124: loss = 0.553617\n",
      "Iteration 125: loss = 0.552500\n",
      "Iteration 126: loss = 0.551396\n",
      "Iteration 127: loss = 0.550307\n",
      "Iteration 128: loss = 0.549232\n",
      "Iteration 129: loss = 0.548170\n",
      "Iteration 130: loss = 0.547122\n",
      "Iteration 131: loss = 0.546086\n",
      "Iteration 132: loss = 0.545063\n",
      "Iteration 133: loss = 0.544052\n",
      "Iteration 134: loss = 0.543053\n",
      "Iteration 135: loss = 0.542065\n",
      "Iteration 136: loss = 0.541088\n",
      "Iteration 137: loss = 0.540121\n",
      "Iteration 138: loss = 0.539164\n",
      "Iteration 139: loss = 0.538217\n",
      "Iteration 140: loss = 0.537280\n",
      "Iteration 141: loss = 0.536351\n",
      "Iteration 142: loss = 0.535432\n",
      "Iteration 143: loss = 0.534521\n",
      "Iteration 144: loss = 0.533619\n",
      "Iteration 145: loss = 0.532725\n",
      "Iteration 146: loss = 0.531841\n",
      "Iteration 147: loss = 0.530964\n",
      "Iteration 148: loss = 0.530097\n",
      "Iteration 149: loss = 0.529239\n",
      "Iteration 150: loss = 0.528389\n",
      "Iteration 151: loss = 0.527547\n",
      "Iteration 152: loss = 0.526714\n",
      "Iteration 153: loss = 0.525888\n",
      "Iteration 154: loss = 0.525071\n",
      "Iteration 155: loss = 0.524261\n",
      "Iteration 156: loss = 0.523458\n",
      "Iteration 157: loss = 0.522662\n",
      "Iteration 158: loss = 0.521874\n",
      "Iteration 159: loss = 0.521092\n",
      "Iteration 160: loss = 0.520317\n",
      "Iteration 161: loss = 0.519553\n",
      "Iteration 162: loss = 0.518801\n",
      "Iteration 163: loss = 0.518126\n",
      "Iteration 164: loss = 0.517726\n",
      "Iteration 0: loss = 1.795447\n",
      "Iteration 1: loss = 1.674582\n",
      "Iteration 2: loss = 1.576136\n",
      "Iteration 3: loss = 1.487841\n",
      "Iteration 4: loss = 1.407477\n",
      "Iteration 5: loss = 1.333989\n",
      "Iteration 6: loss = 1.266561\n",
      "Iteration 7: loss = 1.204509\n",
      "Iteration 8: loss = 1.147259\n",
      "Iteration 9: loss = 1.094326\n",
      "Iteration 10: loss = 1.045305\n",
      "Iteration 11: loss = 0.999839\n",
      "Iteration 12: loss = 0.957605\n",
      "Iteration 13: loss = 0.918303\n",
      "Iteration 14: loss = 0.881662\n",
      "Iteration 15: loss = 0.847438\n",
      "Iteration 16: loss = 0.815413\n",
      "Iteration 17: loss = 0.785414\n",
      "Iteration 18: loss = 0.757283\n",
      "Iteration 19: loss = 0.730875\n",
      "Iteration 20: loss = 0.706085\n",
      "Iteration 21: loss = 0.682814\n",
      "Iteration 22: loss = 0.660964\n",
      "Iteration 23: loss = 0.640441\n",
      "Iteration 24: loss = 0.621154\n",
      "Iteration 25: loss = 0.603020\n",
      "Iteration 26: loss = 0.585956\n",
      "Iteration 27: loss = 0.569893\n",
      "Iteration 28: loss = 0.554780\n",
      "Iteration 29: loss = 0.540551\n",
      "Iteration 30: loss = 0.527129\n",
      "Iteration 31: loss = 0.514441\n",
      "Iteration 32: loss = 0.502428\n",
      "Iteration 33: loss = 0.491046\n",
      "Iteration 34: loss = 0.480256\n",
      "Iteration 35: loss = 0.470023\n",
      "Iteration 36: loss = 0.460313\n",
      "Iteration 37: loss = 0.451093\n",
      "Iteration 38: loss = 0.442330\n",
      "Iteration 39: loss = 0.433997\n",
      "Iteration 40: loss = 0.426066\n",
      "Iteration 41: loss = 0.418510\n",
      "Iteration 42: loss = 0.411302\n",
      "Iteration 43: loss = 0.404420\n",
      "Iteration 44: loss = 0.397846\n",
      "Iteration 45: loss = 0.391562\n",
      "Iteration 46: loss = 0.385552\n",
      "Iteration 47: loss = 0.379798\n",
      "Iteration 48: loss = 0.374285\n",
      "Iteration 49: loss = 0.368998\n",
      "Iteration 50: loss = 0.363925\n",
      "Iteration 51: loss = 0.359052\n",
      "Iteration 52: loss = 0.354372\n",
      "Iteration 53: loss = 0.349873\n",
      "Iteration 54: loss = 0.345547\n",
      "Iteration 55: loss = 0.341385\n",
      "Iteration 56: loss = 0.337378\n",
      "Iteration 57: loss = 0.333517\n",
      "Iteration 58: loss = 0.329796\n",
      "Iteration 59: loss = 0.326209\n",
      "Iteration 60: loss = 0.322748\n",
      "Iteration 61: loss = 0.319409\n",
      "Iteration 62: loss = 0.316185\n",
      "Iteration 63: loss = 0.313071\n",
      "Iteration 64: loss = 0.310061\n",
      "Iteration 65: loss = 0.307151\n",
      "Iteration 66: loss = 0.304335\n",
      "Iteration 67: loss = 0.301608\n",
      "Iteration 68: loss = 0.298967\n",
      "Iteration 69: loss = 0.296408\n",
      "Iteration 70: loss = 0.293925\n",
      "Iteration 71: loss = 0.291517\n",
      "Iteration 72: loss = 0.289180\n",
      "Iteration 73: loss = 0.286911\n",
      "Iteration 74: loss = 0.284708\n",
      "Iteration 75: loss = 0.282569\n",
      "Iteration 76: loss = 0.280490\n",
      "Iteration 77: loss = 0.278470\n",
      "Iteration 78: loss = 0.276507\n",
      "Iteration 79: loss = 0.274598\n",
      "Iteration 80: loss = 0.272742\n",
      "Iteration 81: loss = 0.270935\n",
      "Iteration 82: loss = 0.269177\n",
      "Iteration 83: loss = 0.267465\n",
      "Iteration 84: loss = 0.265798\n",
      "Iteration 85: loss = 0.264173\n",
      "Iteration 86: loss = 0.262590\n",
      "Iteration 87: loss = 0.261046\n",
      "Iteration 88: loss = 0.259540\n",
      "Iteration 89: loss = 0.258071\n",
      "Iteration 90: loss = 0.256637\n",
      "Iteration 91: loss = 0.255238\n",
      "Iteration 92: loss = 0.253872\n",
      "Iteration 93: loss = 0.252538\n",
      "Iteration 94: loss = 0.251234\n",
      "Iteration 95: loss = 0.249960\n",
      "Iteration 96: loss = 0.248715\n",
      "Iteration 97: loss = 0.247497\n",
      "Iteration 98: loss = 0.246305\n",
      "Iteration 99: loss = 0.245139\n",
      "Iteration 100: loss = 0.243997\n",
      "Iteration 101: loss = 0.242878\n",
      "Iteration 102: loss = 0.241782\n",
      "Iteration 103: loss = 0.240707\n",
      "Iteration 104: loss = 0.239653\n",
      "Iteration 105: loss = 0.238618\n",
      "Iteration 106: loss = 0.237603\n",
      "Iteration 107: loss = 0.236606\n",
      "Iteration 108: loss = 0.235628\n",
      "Iteration 109: loss = 0.234667\n",
      "Iteration 110: loss = 0.233723\n",
      "Iteration 111: loss = 0.232796\n",
      "Iteration 112: loss = 0.231886\n",
      "Iteration 113: loss = 0.230991\n",
      "Iteration 114: loss = 0.230112\n",
      "Iteration 115: loss = 0.229249\n",
      "Iteration 116: loss = 0.228401\n",
      "Iteration 117: loss = 0.227567\n",
      "Iteration 118: loss = 0.226748\n",
      "Iteration 119: loss = 0.225942\n",
      "Iteration 120: loss = 0.225150\n",
      "Iteration 121: loss = 0.224371\n",
      "Iteration 122: loss = 0.223603\n",
      "Iteration 123: loss = 0.222848\n",
      "Iteration 124: loss = 0.222103\n",
      "Iteration 125: loss = 0.221370\n",
      "Iteration 126: loss = 0.220647\n",
      "Iteration 127: loss = 0.219935\n",
      "Iteration 128: loss = 0.219233\n",
      "Iteration 129: loss = 0.218540\n",
      "Iteration 130: loss = 0.217858\n",
      "Iteration 131: loss = 0.217185\n",
      "Iteration 132: loss = 0.216522\n",
      "Iteration 133: loss = 0.215869\n",
      "Iteration 134: loss = 0.215225\n",
      "Iteration 135: loss = 0.214591\n",
      "Iteration 136: loss = 0.213966\n",
      "Iteration 137: loss = 0.213350\n",
      "Iteration 138: loss = 0.212743\n",
      "Iteration 139: loss = 0.212144\n",
      "Iteration 140: loss = 0.211554\n",
      "Iteration 141: loss = 0.210972\n",
      "Iteration 142: loss = 0.210398\n",
      "Iteration 143: loss = 0.209833\n",
      "Iteration 144: loss = 0.209274\n",
      "Iteration 145: loss = 0.208724\n",
      "Iteration 146: loss = 0.208180\n",
      "Iteration 147: loss = 0.207644\n",
      "Iteration 148: loss = 0.207115\n",
      "Iteration 149: loss = 0.206593\n",
      "Iteration 150: loss = 0.206077\n",
      "Iteration 151: loss = 0.205568\n",
      "Iteration 152: loss = 0.205066\n",
      "Iteration 153: loss = 0.204569\n",
      "Iteration 154: loss = 0.204078\n",
      "Iteration 155: loss = 0.203592\n",
      "Iteration 156: loss = 0.203112\n",
      "Iteration 157: loss = 0.202637\n",
      "Iteration 158: loss = 0.202167\n",
      "Iteration 159: loss = 0.201702\n",
      "Iteration 160: loss = 0.201240\n",
      "Iteration 161: loss = 0.200783\n",
      "Iteration 162: loss = 0.200330\n",
      "Iteration 163: loss = 0.199880\n",
      "Iteration 164: loss = 0.199435\n",
      "Iteration 165: loss = 0.198994\n",
      "Iteration 166: loss = 0.198557\n",
      "Iteration 167: loss = 0.198126\n",
      "Iteration 168: loss = 0.197699\n",
      "Iteration 169: loss = 0.197278\n",
      "Iteration 170: loss = 0.196861\n",
      "Iteration 171: loss = 0.196451\n",
      "Iteration 172: loss = 0.196045\n",
      "Iteration 173: loss = 0.195644\n",
      "Iteration 174: loss = 0.195249\n",
      "Iteration 175: loss = 0.194858\n",
      "Iteration 176: loss = 0.194473\n",
      "Iteration 177: loss = 0.194091\n",
      "Iteration 178: loss = 0.193715\n",
      "Iteration 179: loss = 0.193343\n",
      "Iteration 180: loss = 0.192975\n",
      "Iteration 181: loss = 0.192611\n",
      "Iteration 182: loss = 0.192252\n",
      "Iteration 183: loss = 0.191896\n",
      "Iteration 184: loss = 0.191544\n",
      "Iteration 185: loss = 0.191195\n",
      "Iteration 186: loss = 0.190851\n",
      "Iteration 187: loss = 0.190509\n",
      "Iteration 188: loss = 0.190171\n",
      "Iteration 189: loss = 0.189836\n",
      "Iteration 190: loss = 0.189505\n",
      "Iteration 191: loss = 0.189176\n",
      "Iteration 192: loss = 0.188850\n",
      "Iteration 193: loss = 0.188528\n",
      "Iteration 194: loss = 0.188208\n",
      "\n",
      "Iteration 70: loss = 0.528944\n",
      "Iteration 71: loss = 0.526534\n",
      "Iteration 72: loss = 0.524166\n",
      "Iteration 73: loss = 0.521840\n",
      "Iteration 74: loss = 0.519554\n",
      "Iteration 75: loss = 0.517307\n",
      "Iteration 76: loss = 0.515099\n",
      "Iteration 77: loss = 0.512929\n",
      "Iteration 78: loss = 0.510796\n",
      "Iteration 79: loss = 0.508701\n",
      "Iteration 80: loss = 0.506642\n",
      "Iteration 81: loss = 0.504618\n",
      "Iteration 82: loss = 0.502629\n",
      "Iteration 83: loss = 0.500674\n",
      "Iteration 84: loss = 0.498753\n",
      "Iteration 85: loss = 0.496863\n",
      "Iteration 86: loss = 0.495006\n",
      "Iteration 87: loss = 0.493180\n",
      "Iteration 88: loss = 0.491384\n",
      "Iteration 89: loss = 0.489617\n",
      "Iteration 90: loss = 0.487879\n",
      "Iteration 91: loss = 0.486169\n",
      "Iteration 92: loss = 0.484485\n",
      "Iteration 93: loss = 0.482826\n",
      "Iteration 94: loss = 0.481192\n",
      "Iteration 95: loss = 0.479581\n",
      "Iteration 96: loss = 0.477992\n",
      "Iteration 97: loss = 0.476425\n",
      "Iteration 98: loss = 0.474879\n",
      "Iteration 99: loss = 0.473353\n",
      "Iteration 100: loss = 0.471847\n",
      "Iteration 101: loss = 0.470361\n",
      "Iteration 102: loss = 0.468893\n",
      "Iteration 103: loss = 0.467445\n",
      "Iteration 104: loss = 0.466015\n",
      "Iteration 105: loss = 0.464604\n",
      "Iteration 106: loss = 0.463211\n",
      "Iteration 107: loss = 0.461837\n",
      "Iteration 108: loss = 0.460480\n",
      "Iteration 109: loss = 0.459142\n",
      "Iteration 110: loss = 0.457820\n",
      "Iteration 111: loss = 0.456515\n",
      "Iteration 112: loss = 0.455226\n",
      "Iteration 113: loss = 0.453952\n",
      "Iteration 114: loss = 0.452694\n",
      "Iteration 115: loss = 0.451450\n",
      "Iteration 116: loss = 0.450221\n",
      "Iteration 117: loss = 0.449007\n",
      "Iteration 118: loss = 0.447808\n",
      "Iteration 119: loss = 0.446624\n",
      "Iteration 120: loss = 0.445453\n",
      "Iteration 121: loss = 0.444298\n",
      "Iteration 122: loss = 0.443156\n",
      "Iteration 123: loss = 0.442027\n",
      "Iteration 124: loss = 0.440912\n",
      "Iteration 125: loss = 0.439811\n",
      "Iteration 126: loss = 0.438722\n",
      "Iteration 127: loss = 0.437645\n",
      "Iteration 128: loss = 0.436581\n",
      "Iteration 129: loss = 0.435529\n",
      "Iteration 130: loss = 0.434488\n",
      "Iteration 131: loss = 0.433459\n",
      "Iteration 132: loss = 0.432441\n",
      "Iteration 133: loss = 0.431434\n",
      "Iteration 134: loss = 0.430437\n",
      "Iteration 135: loss = 0.429450\n",
      "Iteration 136: loss = 0.428474\n",
      "Iteration 137: loss = 0.427507\n",
      "Iteration 138: loss = 0.426551\n",
      "Iteration 139: loss = 0.425604\n",
      "Iteration 140: loss = 0.424666\n",
      "Iteration 141: loss = 0.423737\n",
      "Iteration 142: loss = 0.422817\n",
      "Iteration 143: loss = 0.421907\n",
      "Iteration 144: loss = 0.421005\n",
      "Iteration 145: loss = 0.420112\n",
      "Iteration 146: loss = 0.419228\n",
      "Iteration 147: loss = 0.418352\n",
      "Iteration 148: loss = 0.417484\n",
      "Iteration 149: loss = 0.416625\n",
      "Iteration 150: loss = 0.415774\n",
      "Iteration 151: loss = 0.414930\n",
      "Iteration 152: loss = 0.414095\n",
      "Iteration 153: loss = 0.413266\n",
      "Iteration 154: loss = 0.412446\n",
      "Iteration 155: loss = 0.411633\n",
      "Iteration 156: loss = 0.410828\n",
      "Iteration 157: loss = 0.410031\n",
      "Iteration 158: loss = 0.409249\n",
      "Iteration 159: loss = 0.408483\n",
      "Iteration 160: loss = 0.407773\n",
      "Iteration 161: loss = 0.407174\n",
      "Iteration 162: loss = 0.406934\n",
      "Iteration 0: loss = 1.797168\n",
      "Iteration 1: loss = 1.666697\n",
      "Iteration 2: loss = 1.574586\n",
      "Iteration 3: loss = 1.495878\n",
      "Iteration 4: loss = 1.427304\n",
      "Iteration 5: loss = 1.366692\n",
      "Iteration 6: loss = 1.312400\n",
      "Iteration 7: loss = 1.263181\n",
      "Iteration 8: loss = 1.218086\n",
      "Iteration 9: loss = 1.176390\n",
      "Iteration 10: loss = 1.137531\n",
      "Iteration 11: loss = 1.101063\n",
      "Iteration 12: loss = 1.066631\n",
      "Iteration 13: loss = 1.033957\n",
      "Iteration 14: loss = 1.002818\n",
      "Iteration 15: loss = 0.973037\n",
      "Iteration 16: loss = 0.944472\n",
      "Iteration 17: loss = 0.917009\n",
      "Iteration 18: loss = 0.890557\n",
      "Iteration 19: loss = 0.865049\n",
      "Iteration 20: loss = 0.840426\n",
      "Iteration 21: loss = 0.816647\n",
      "Iteration 22: loss = 0.793685\n",
      "Iteration 23: loss = 0.771518\n",
      "Iteration 24: loss = 0.750119\n",
      "Iteration 25: loss = 0.729458\n",
      "Iteration 26: loss = 0.709504\n",
      "Iteration 27: loss = 0.690240\n",
      "Iteration 28: loss = 0.671654\n",
      "Iteration 29: loss = 0.653734\n",
      "Iteration 30: loss = 0.636461\n",
      "Iteration 31: loss = 0.619822\n",
      "Iteration 32: loss = 0.603805\n",
      "Iteration 33: loss = 0.588398\n",
      "Iteration 34: loss = 0.573579\n",
      "Iteration 35: loss = 0.559326\n",
      "Iteration 36: loss = 0.545622\n",
      "Iteration 37: loss = 0.532452\n",
      "Iteration 38: loss = 0.519804\n",
      "Iteration 39: loss = 0.507663\n",
      "Iteration 40: loss = 0.496011\n",
      "Iteration 41: loss = 0.484830\n",
      "Iteration 42: loss = 0.474102\n",
      "Iteration 43: loss = 0.463811\n",
      "Iteration 44: loss = 0.453943\n",
      "Iteration 45: loss = 0.444483\n",
      "Iteration 46: loss = 0.435418\n",
      "Iteration 47: loss = 0.426732\n",
      "Iteration 48: loss = 0.418413\n",
      "Iteration 49: loss = 0.410445\n",
      "Iteration 50: loss = 0.402815\n",
      "Iteration 51: loss = 0.395507\n",
      "Iteration 52: loss = 0.388507\n",
      "Iteration 53: loss = 0.381802\n",
      "Iteration 54: loss = 0.375378\n",
      "Iteration 55: loss = 0.369220\n",
      "Iteration 56: loss = 0.363316\n",
      "Iteration 57: loss = 0.357656\n",
      "Iteration 58: loss = 0.352232\n",
      "Iteration 59: loss = 0.347035\n",
      "Iteration 60: loss = 0.342052\n",
      "Iteration 61: loss = 0.337271\n",
      "Iteration 62: loss = 0.332679\n",
      "Iteration 63: loss = 0.328267\n",
      "Iteration 64: loss = 0.324026\n",
      "Iteration 65: loss = 0.319947\n",
      "Iteration 66: loss = 0.316022\n",
      "Iteration 67: loss = 0.312244\n",
      "Iteration 68: loss = 0.308606\n",
      "Iteration 69: loss = 0.305101\n",
      "Iteration 70: loss = 0.301722\n",
      "Iteration 71: loss = 0.298463\n",
      "Iteration 72: loss = 0.295318\n",
      "Iteration 73: loss = 0.292283\n",
      "Iteration 74: loss = 0.289353\n",
      "Iteration 75: loss = 0.286523\n",
      "Iteration 76: loss = 0.283790\n",
      "Iteration 77: loss = 0.281150\n",
      "Iteration 78: loss = 0.278600\n",
      "Iteration 79: loss = 0.276135\n",
      "Iteration 80: loss = 0.273753\n",
      "Iteration 81: loss = 0.271449\n",
      "Iteration 82: loss = 0.269222\n",
      "Iteration 83: loss = 0.267067\n",
      "Iteration 84: loss = 0.264981\n",
      "Iteration 85: loss = 0.262962\n",
      "Iteration 86: loss = 0.261007\n",
      "Iteration 87: loss = 0.259114\n",
      "Iteration 88: loss = 0.257280\n",
      "Iteration 89: loss = 0.255503\n",
      "Iteration 90: loss = 0.253781\n",
      "Iteration 91: loss = 0.252111\n",
      "Iteration 92: loss = 0.250492\n",
      "Iteration 93: loss = 0.248921\n",
      "Iteration 94: loss = 0.247397\n",
      "Iteration 95: loss = 0.245917\n",
      "Iteration 96: loss = 0.244479\n",
      "Iteration 97: loss = 0.243082\n",
      "Iteration 98: loss = 0.241724\n",
      "Iteration 99: loss = 0.240404\n",
      "Iteration 100: loss = 0.239120\n",
      "Iteration 101: loss = 0.237871\n",
      "Iteration 102: loss = 0.236656\n",
      "Iteration 103: loss = 0.235473\n",
      "Iteration 104: loss = 0.234322\n",
      "Iteration 105: loss = 0.233200\n",
      "Iteration 106: loss = 0.232107\n",
      "Iteration 107: loss = 0.231041\n",
      "Iteration 108: loss = 0.230002\n",
      "Iteration 109: loss = 0.228989\n",
      "Iteration 110: loss = 0.228000\n",
      "Iteration 111: loss = 0.227036\n",
      "Iteration 112: loss = 0.226094\n",
      "Iteration 113: loss = 0.225174\n",
      "Iteration 114: loss = 0.224275\n",
      "Iteration 115: loss = 0.223397\n",
      "Iteration 116: loss = 0.222539\n",
      "Iteration 117: loss = 0.221700\n",
      "Iteration 118: loss = 0.220880\n",
      "Iteration 119: loss = 0.220078\n",
      "Iteration 120: loss = 0.219293\n",
      "Iteration 121: loss = 0.218524\n",
      "Iteration 122: loss = 0.217772\n",
      "Iteration 123: loss = 0.217036\n",
      "Iteration 124: loss = 0.216315\n",
      "Iteration 125: loss = 0.215609\n",
      "Iteration 126: loss = 0.214917\n",
      "Iteration 127: loss = 0.214239\n",
      "Iteration 128: loss = 0.213575\n",
      "Iteration 129: loss = 0.212923\n",
      "Iteration 130: loss = 0.212284\n",
      "Iteration 131: loss = 0.211658\n",
      "Iteration 132: loss = 0.211043\n",
      "Iteration 133: loss = 0.210440\n",
      "Iteration 134: loss = 0.209848\n",
      "Iteration 135: loss = 0.209266\n",
      "Iteration 136: loss = 0.208696\n",
      "Iteration 137: loss = 0.208135\n",
      "Iteration 138: loss = 0.207585\n",
      "Iteration 139: loss = 0.207044\n",
      "Iteration 140: loss = 0.206513\n",
      "Iteration 141: loss = 0.205990\n",
      "Iteration 142: loss = 0.205477\n",
      "Iteration 143: loss = 0.204972\n",
      "Iteration 144: loss = 0.204476\n",
      "Iteration 145: loss = 0.203988\n",
      "Iteration 146: loss = 0.203508\n",
      "Iteration 147: loss = 0.203036\n",
      "Iteration 148: loss = 0.202572\n",
      "Iteration 149: loss = 0.202115\n",
      "Iteration 150: loss = 0.201665\n",
      "Iteration 151: loss = 0.201222\n",
      "Iteration 152: loss = 0.200786\n",
      "Iteration 153: loss = 0.200357\n",
      "Iteration 154: loss = 0.199935\n",
      "Iteration 155: loss = 0.199519\n",
      "Iteration 156: loss = 0.199109\n",
      "Iteration 157: loss = 0.198706\n",
      "Iteration 158: loss = 0.198308\n",
      "Iteration 159: loss = 0.197916\n",
      "Iteration 160: loss = 0.197530\n",
      "Iteration 161: loss = 0.197149\n",
      "Iteration 162: loss = 0.196774\n",
      "Iteration 163: loss = 0.196403\n",
      "Iteration 164: loss = 0.196038\n",
      "Iteration 165: loss = 0.195678\n",
      "Iteration 166: loss = 0.195323\n",
      "Iteration 167: loss = 0.194972\n",
      "Iteration 168: loss = 0.194626\n",
      "Iteration 169: loss = 0.194285\n",
      "Iteration 170: loss = 0.193948\n",
      "Iteration 171: loss = 0.193615\n",
      "Iteration 172: loss = 0.193286\n",
      "Iteration 173: loss = 0.192961\n",
      "Iteration 174: loss = 0.192640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 119: loss = 0.255130\n",
      "Iteration 120: loss = 0.253989\n",
      "Iteration 121: loss = 0.252869\n",
      "Iteration 122: loss = 0.251772\n",
      "Iteration 123: loss = 0.250695\n",
      "Iteration 124: loss = 0.249639\n",
      "Iteration 125: loss = 0.248602\n",
      "Iteration 126: loss = 0.247584\n",
      "Iteration 127: loss = 0.246585\n",
      "Iteration 128: loss = 0.245605\n",
      "Iteration 129: loss = 0.244642\n",
      "Iteration 130: loss = 0.243697\n",
      "Iteration 131: loss = 0.242769\n",
      "Iteration 132: loss = 0.241857\n",
      "Iteration 133: loss = 0.240962\n",
      "Iteration 134: loss = 0.240085\n",
      "Iteration 135: loss = 0.239224\n",
      "Iteration 136: loss = 0.238380\n",
      "Iteration 137: loss = 0.237552\n",
      "Iteration 138: loss = 0.236739\n",
      "Iteration 139: loss = 0.235940\n",
      "Iteration 140: loss = 0.235154\n",
      "Iteration 141: loss = 0.234380\n",
      "Iteration 142: loss = 0.233618\n",
      "Iteration 143: loss = 0.232868\n",
      "Iteration 144: loss = 0.232128\n",
      "Iteration 145: loss = 0.231398\n",
      "Iteration 146: loss = 0.230679\n",
      "Iteration 147: loss = 0.229969\n",
      "Iteration 148: loss = 0.229269\n",
      "Iteration 149: loss = 0.228578\n",
      "Iteration 150: loss = 0.227896\n",
      "Iteration 151: loss = 0.227223\n",
      "Iteration 152: loss = 0.226558\n",
      "Iteration 153: loss = 0.225903\n",
      "Iteration 154: loss = 0.225256\n",
      "Iteration 155: loss = 0.224617\n",
      "Iteration 156: loss = 0.223988\n",
      "Iteration 157: loss = 0.223367\n",
      "Iteration 158: loss = 0.222755\n",
      "Iteration 159: loss = 0.222152\n",
      "Iteration 160: loss = 0.221558\n",
      "Iteration 161: loss = 0.220974\n",
      "Iteration 162: loss = 0.220398\n",
      "Iteration 163: loss = 0.219831\n",
      "Iteration 164: loss = 0.219272\n",
      "Iteration 165: loss = 0.218721\n",
      "Iteration 166: loss = 0.218178\n",
      "Iteration 167: loss = 0.217641\n",
      "Iteration 168: loss = 0.217111\n",
      "Iteration 169: loss = 0.216588\n",
      "Iteration 170: loss = 0.216072\n",
      "Iteration 171: loss = 0.215562\n",
      "Iteration 172: loss = 0.215058\n",
      "Iteration 173: loss = 0.214560\n",
      "Iteration 174: loss = 0.214067\n",
      "Iteration 175: loss = 0.213580\n",
      "Iteration 176: loss = 0.213099\n",
      "Iteration 177: loss = 0.212623\n",
      "Iteration 178: loss = 0.212152\n",
      "Iteration 179: loss = 0.211686\n",
      "Iteration 180: loss = 0.211225\n",
      "Iteration 181: loss = 0.210769\n",
      "Iteration 182: loss = 0.210318\n",
      "Iteration 183: loss = 0.209873\n",
      "Iteration 184: loss = 0.209433\n",
      "Iteration 185: loss = 0.208999\n",
      "Iteration 186: loss = 0.208571\n",
      "Iteration 187: loss = 0.208148\n",
      "Iteration 188: loss = 0.207730\n",
      "Iteration 189: loss = 0.207318\n",
      "Iteration 190: loss = 0.206911\n",
      "Iteration 191: loss = 0.206509\n",
      "Iteration 192: loss = 0.206112\n",
      "Iteration 193: loss = 0.205720\n",
      "Iteration 194: loss = 0.205332\n",
      "Iteration 195: loss = 0.204949\n",
      "Iteration 196: loss = 0.204570\n",
      "Iteration 197: loss = 0.204195\n",
      "Iteration 198: loss = 0.203824\n",
      "Iteration 199: loss = 0.203457\n",
      "Iteration 200: loss = 0.203093\n",
      "Iteration 201: loss = 0.202732\n",
      "Iteration 202: loss = 0.202375\n",
      "Iteration 203: loss = 0.202021\n",
      "Iteration 204: loss = 0.201670\n",
      "Iteration 205: loss = 0.201323\n",
      "Iteration 206: loss = 0.200979\n",
      "Iteration 207: loss = 0.200639\n",
      "Iteration 208: loss = 0.200301\n",
      "Iteration 209: loss = 0.199965\n",
      "Iteration 210: loss = 0.199632\n",
      "Iteration 211: loss = 0.199301\n",
      "Iteration 212: loss = 0.198971\n",
      "Iteration 213: loss = 0.198644\n",
      "Iteration 214: loss = 0.198317\n",
      "Iteration 215: loss = 0.197992\n",
      "Iteration 216: loss = 0.197669\n",
      "Iteration 217: loss = 0.197347\n",
      "Iteration 218: loss = 0.197027\n",
      "Iteration 219: loss = 0.196710\n",
      "Iteration 220: loss = 0.196396\n",
      "Iteration 221: loss = 0.196088\n",
      "Iteration 222: loss = 0.195786\n",
      "Iteration 223: loss = 0.195499\n",
      "Iteration 224: loss = 0.195230\n",
      "Iteration 225: loss = 0.195005\n",
      "Iteration 226: loss = 0.194847\n",
      "Iteration 0: loss = 1.790208\n",
      "Iteration 1: loss = 1.526570\n",
      "Iteration 2: loss = 1.366371\n",
      "Iteration 3: loss = 1.249963\n",
      "Iteration 4: loss = 1.167458\n",
      "Iteration 5: loss = 1.091588\n",
      "Iteration 6: loss = 1.033777\n",
      "Iteration 7: loss = 0.973896\n",
      "Iteration 8: loss = 0.926048\n",
      "Iteration 9: loss = 0.878029\n",
      "Iteration 10: loss = 0.837522\n",
      "Iteration 11: loss = 0.799722\n",
      "Iteration 12: loss = 0.766383\n",
      "Iteration 13: loss = 0.735912\n",
      "Iteration 14: loss = 0.708102\n",
      "Iteration 15: loss = 0.682329\n",
      "Iteration 16: loss = 0.658327\n",
      "Iteration 17: loss = 0.635829\n",
      "Iteration 18: loss = 0.614687\n",
      "Iteration 19: loss = 0.594756\n",
      "Iteration 20: loss = 0.575934\n",
      "Iteration 21: loss = 0.558125\n",
      "Iteration 22: loss = 0.541247\n",
      "Iteration 23: loss = 0.525232\n",
      "Iteration 24: loss = 0.510022\n",
      "Iteration 25: loss = 0.495562\n",
      "Iteration 26: loss = 0.481804\n",
      "Iteration 27: loss = 0.468703\n",
      "Iteration 28: loss = 0.456220\n",
      "Iteration 29: loss = 0.444317\n",
      "Iteration 30: loss = 0.432965\n",
      "Iteration 31: loss = 0.422135\n",
      "Iteration 32: loss = 0.411802\n",
      "Iteration 33: loss = 0.401941\n",
      "Iteration 34: loss = 0.392524\n",
      "Iteration 35: loss = 0.383525\n",
      "Iteration 36: loss = 0.374920\n",
      "Iteration 37: loss = 0.366686\n",
      "Iteration 38: loss = 0.358803\n",
      "Iteration 39: loss = 0.351253\n",
      "Iteration 40: loss = 0.344016\n",
      "Iteration 41: loss = 0.337075\n",
      "Iteration 42: loss = 0.330414\n",
      "Iteration 43: loss = 0.324021\n",
      "Iteration 44: loss = 0.317885\n",
      "Iteration 45: loss = 0.311999\n",
      "Iteration 46: loss = 0.306351\n",
      "Iteration 47: loss = 0.300932\n",
      "Iteration 48: loss = 0.295730\n",
      "Iteration 49: loss = 0.290735\n",
      "Iteration 50: loss = 0.285938\n",
      "Iteration 51: loss = 0.281329\n",
      "Iteration 52: loss = 0.276898\n",
      "Iteration 53: loss = 0.272639\n",
      "Iteration 54: loss = 0.268542\n",
      "Iteration 55: loss = 0.264601\n",
      "Iteration 56: loss = 0.260809\n",
      "Iteration 57: loss = 0.257160\n",
      "Iteration 58: loss = 0.253648\n",
      "Iteration 59: loss = 0.250266\n",
      "Iteration 60: loss = 0.247011\n",
      "Iteration 61: loss = 0.243875\n",
      "Iteration 62: loss = 0.240854\n",
      "Iteration 63: loss = 0.237943\n",
      "Iteration 64: loss = 0.235136\n",
      "Iteration 65: loss = 0.232429\n",
      "Iteration 66: loss = 0.229818\n",
      "Iteration 67: loss = 0.227298\n",
      "Iteration 68: loss = 0.224864\n",
      "Iteration 69: loss = 0.222514\n",
      "Iteration 70: loss = 0.220242\n",
      "Iteration 71: loss = 0.218046\n",
      "Iteration 72: loss = 0.215923\n",
      "Iteration 73: loss = 0.213869\n",
      "Iteration 74: loss = 0.211881\n",
      "Iteration 75: loss = 0.209957\n",
      "Iteration 76: loss = 0.208093\n",
      "Iteration 77: loss = 0.206288\n",
      "Iteration 78: loss = 0.204538\n",
      "Iteration 79: loss = 0.202843\n",
      "Iteration 80: loss = 0.201199\n",
      "Iteration 81: loss = 0.199604\n",
      "Iteration 82: loss = 0.198057\n",
      "Iteration 83: loss = 0.196556\n",
      "Iteration 84: loss = 0.195098\n",
      "Iteration 85: loss = 0.193682\n",
      "Iteration 86: loss = 0.192305\n",
      "Iteration 87: loss = 0.190967\n",
      "Iteration 88: loss = 0.189668\n",
      "Iteration 89: loss = 0.188404\n",
      "Iteration 90: loss = 0.187176\n",
      "Iteration 91: loss = 0.185982\n",
      "Iteration 92: loss = 0.184820\n",
      "Iteration 93: loss = 0.183690\n",
      "Iteration 94: loss = 0.182589\n",
      "Iteration 95: loss = 0.181518\n",
      "Iteration 96: loss = 0.180474\n",
      "Iteration 97: loss = 0.179457\n",
      "Iteration 98: loss = 0.178466\n",
      "Iteration 99: loss = 0.177499\n",
      "Iteration 100: loss = 0.176557\n",
      "Iteration 101: loss = 0.175637\n",
      "Iteration 102: loss = 0.174740\n",
      "Iteration 103: loss = 0.173865\n",
      "Iteration 104: loss = 0.173012\n",
      "Iteration 105: loss = 0.172179\n",
      "Iteration 106: loss = 0.171367\n",
      "Iteration 107: loss = 0.170575\n",
      "Iteration 108: loss = 0.169801\n",
      "Iteration 109: loss = 0.169046\n",
      "Iteration 110: loss = 0.168309\n",
      "Iteration 111: loss = 0.167588\n",
      "Iteration 112: loss = 0.166884\n",
      "Iteration 113: loss = 0.166195\n",
      "Iteration 114: loss = 0.165521\n",
      "Iteration 115: loss = 0.164862\n",
      "Iteration 116: loss = 0.164216\n",
      "Iteration 117: loss = 0.163584\n",
      "Iteration 118: loss = 0.162965\n",
      "Iteration 119: loss = 0.162358\n",
      "Iteration 120: loss = 0.161763\n",
      "Iteration 121: loss = 0.161180\n",
      "Iteration 122: loss = 0.160608\n",
      "Iteration 123: loss = 0.160047\n",
      "Iteration 124: loss = 0.159497\n",
      "Iteration 125: loss = 0.158957\n",
      "Iteration 126: loss = 0.158427\n",
      "Iteration 127: loss = 0.157907\n",
      "Iteration 128: loss = 0.157396\n",
      "Iteration 129: loss = 0.156895\n",
      "Iteration 130: loss = 0.156402\n",
      "Iteration 131: loss = 0.155918\n",
      "Iteration 132: loss = 0.155442\n",
      "Iteration 133: loss = 0.154974\n",
      "Iteration 134: loss = 0.154515\n",
      "Iteration 135: loss = 0.154063\n",
      "Iteration 136: loss = 0.153618\n",
      "Iteration 137: loss = 0.153180\n",
      "Iteration 138: loss = 0.152750\n",
      "Iteration 139: loss = 0.152326\n",
      "Iteration 140: loss = 0.151909\n",
      "Iteration 141: loss = 0.151498\n",
      "Iteration 142: loss = 0.151093\n",
      "Iteration 143: loss = 0.150694\n",
      "Iteration 144: loss = 0.150301\n",
      "Iteration 145: loss = 0.149914\n",
      "Iteration 146: loss = 0.149532\n",
      "Iteration 147: loss = 0.149156\n",
      "Iteration 148: loss = 0.148785\n",
      "Iteration 149: loss = 0.148419\n",
      "Iteration 150: loss = 0.148058\n",
      "Iteration 151: loss = 0.147703\n",
      "Iteration 152: loss = 0.147352\n",
      "Iteration 153: loss = 0.147007\n",
      "Iteration 154: loss = 0.146666\n",
      "Iteration 155: loss = 0.146330\n",
      "Iteration 156: loss = 0.145998\n",
      "Iteration 157: loss = 0.145671\n",
      "Iteration 158: loss = 0.145348\n",
      "\n",
      "Iteration 181: loss = 0.221713\n",
      "Iteration 182: loss = 0.221204\n",
      "Iteration 183: loss = 0.220700\n",
      "Iteration 184: loss = 0.220202\n",
      "Iteration 185: loss = 0.219710\n",
      "Iteration 186: loss = 0.219222\n",
      "Iteration 187: loss = 0.218739\n",
      "Iteration 188: loss = 0.218261\n",
      "Iteration 189: loss = 0.217788\n",
      "Iteration 190: loss = 0.217318\n",
      "Iteration 191: loss = 0.216852\n",
      "Iteration 192: loss = 0.216390\n",
      "Iteration 193: loss = 0.215931\n",
      "Iteration 194: loss = 0.215475\n",
      "Iteration 195: loss = 0.215022\n",
      "Iteration 196: loss = 0.214572\n",
      "Iteration 197: loss = 0.214125\n",
      "Iteration 198: loss = 0.213681\n",
      "Iteration 199: loss = 0.213241\n",
      "Iteration 200: loss = 0.212803\n",
      "Iteration 201: loss = 0.212369\n",
      "Iteration 202: loss = 0.211937\n",
      "Iteration 203: loss = 0.211508\n",
      "Iteration 204: loss = 0.211081\n",
      "Iteration 205: loss = 0.210657\n",
      "Iteration 206: loss = 0.210236\n",
      "Iteration 207: loss = 0.209817\n",
      "Iteration 208: loss = 0.209400\n",
      "Iteration 209: loss = 0.208987\n",
      "Iteration 210: loss = 0.208576\n",
      "Iteration 211: loss = 0.208167\n",
      "Iteration 212: loss = 0.207761\n",
      "Iteration 213: loss = 0.207357\n",
      "Iteration 214: loss = 0.206956\n",
      "Iteration 215: loss = 0.206557\n",
      "Iteration 216: loss = 0.206161\n",
      "Iteration 217: loss = 0.205767\n",
      "Iteration 218: loss = 0.205376\n",
      "Iteration 219: loss = 0.204986\n",
      "Iteration 220: loss = 0.204600\n",
      "Iteration 221: loss = 0.204216\n",
      "Iteration 222: loss = 0.203838\n",
      "Iteration 223: loss = 0.203459\n",
      "Iteration 224: loss = 0.203092\n",
      "Iteration 225: loss = 0.202725\n",
      "Iteration 226: loss = 0.202376\n",
      "Iteration 227: loss = 0.202030\n",
      "Iteration 228: loss = 0.201720\n",
      "Iteration 229: loss = 0.201426\n",
      "Iteration 230: loss = 0.201212\n",
      "Iteration 231: loss = 0.201064\n",
      "Iteration 0: loss = 1.781423\n",
      "Iteration 1: loss = 1.521787\n",
      "Iteration 2: loss = 1.360896\n",
      "Iteration 3: loss = 1.246293\n",
      "Iteration 4: loss = 1.162599\n",
      "Iteration 5: loss = 1.089182\n",
      "Iteration 6: loss = 1.030966\n",
      "Iteration 7: loss = 0.973626\n",
      "Iteration 8: loss = 0.926101\n",
      "Iteration 9: loss = 0.879612\n",
      "Iteration 10: loss = 0.839555\n",
      "Iteration 11: loss = 0.802375\n",
      "Iteration 12: loss = 0.769274\n",
      "Iteration 13: loss = 0.738966\n",
      "Iteration 14: loss = 0.711199\n",
      "Iteration 15: loss = 0.685412\n",
      "Iteration 16: loss = 0.661346\n",
      "Iteration 17: loss = 0.638754\n",
      "Iteration 18: loss = 0.617494\n",
      "Iteration 19: loss = 0.597434\n",
      "Iteration 20: loss = 0.578477\n",
      "Iteration 21: loss = 0.560530\n",
      "Iteration 22: loss = 0.543518\n",
      "Iteration 23: loss = 0.527373\n",
      "Iteration 24: loss = 0.512035\n",
      "Iteration 25: loss = 0.497451\n",
      "Iteration 26: loss = 0.483573\n",
      "Iteration 27: loss = 0.470358\n",
      "Iteration 28: loss = 0.457763\n",
      "Iteration 29: loss = 0.445752\n",
      "Iteration 30: loss = 0.434294\n",
      "Iteration 31: loss = 0.423358\n",
      "Iteration 32: loss = 0.412918\n",
      "Iteration 33: loss = 0.402949\n",
      "Iteration 34: loss = 0.393429\n",
      "Iteration 35: loss = 0.384338\n",
      "Iteration 36: loss = 0.375655\n",
      "Iteration 37: loss = 0.367361\n",
      "Iteration 38: loss = 0.359436\n",
      "Iteration 39: loss = 0.351858\n",
      "Iteration 40: loss = 0.344611\n",
      "Iteration 41: loss = 0.337677\n",
      "Iteration 42: loss = 0.331042\n",
      "Iteration 43: loss = 0.324692\n",
      "Iteration 44: loss = 0.318612\n",
      "Iteration 45: loss = 0.312788\n",
      "Iteration 46: loss = 0.307208\n",
      "Iteration 47: loss = 0.301859\n",
      "Iteration 48: loss = 0.296728\n",
      "Iteration 49: loss = 0.291806\n",
      "Iteration 50: loss = 0.287082\n",
      "Iteration 51: loss = 0.282547\n",
      "Iteration 52: loss = 0.278192\n",
      "Iteration 53: loss = 0.274009\n",
      "Iteration 54: loss = 0.269990\n",
      "Iteration 55: loss = 0.266127\n",
      "Iteration 56: loss = 0.262414\n",
      "Iteration 57: loss = 0.258843\n",
      "Iteration 58: loss = 0.255408\n",
      "Iteration 59: loss = 0.252103\n",
      "Iteration 60: loss = 0.248921\n",
      "Iteration 61: loss = 0.245858\n",
      "Iteration 62: loss = 0.242906\n",
      "Iteration 63: loss = 0.240062\n",
      "Iteration 64: loss = 0.237321\n",
      "Iteration 65: loss = 0.234677\n",
      "Iteration 66: loss = 0.232127\n",
      "Iteration 67: loss = 0.229665\n",
      "Iteration 68: loss = 0.227289\n",
      "Iteration 69: loss = 0.224993\n",
      "Iteration 70: loss = 0.222775\n",
      "Iteration 71: loss = 0.220632\n",
      "Iteration 72: loss = 0.218559\n",
      "Iteration 73: loss = 0.216555\n",
      "Iteration 74: loss = 0.214615\n",
      "Iteration 75: loss = 0.212738\n",
      "Iteration 76: loss = 0.210920\n",
      "Iteration 77: loss = 0.209160\n",
      "Iteration 78: loss = 0.207455\n",
      "Iteration 79: loss = 0.205803\n",
      "Iteration 80: loss = 0.204201\n",
      "Iteration 81: loss = 0.202649\n",
      "Iteration 82: loss = 0.201143\n",
      "Iteration 83: loss = 0.199682\n",
      "Iteration 84: loss = 0.198265\n",
      "Iteration 85: loss = 0.196889\n",
      "Iteration 86: loss = 0.195554\n",
      "Iteration 87: loss = 0.194258\n",
      "Iteration 88: loss = 0.193000\n",
      "Iteration 89: loss = 0.191777\n",
      "Iteration 90: loss = 0.190590\n",
      "Iteration 91: loss = 0.189435\n",
      "Iteration 92: loss = 0.188313\n",
      "Iteration 93: loss = 0.187221\n",
      "Iteration 94: loss = 0.186158\n",
      "Iteration 95: loss = 0.185124\n",
      "Iteration 96: loss = 0.184117\n",
      "Iteration 97: loss = 0.183136\n",
      "Iteration 98: loss = 0.182180\n",
      "Iteration 99: loss = 0.181248\n",
      "Iteration 100: loss = 0.180338\n",
      "Iteration 101: loss = 0.179452\n",
      "Iteration 102: loss = 0.178586\n",
      "Iteration 103: loss = 0.177741\n",
      "Iteration 104: loss = 0.176916\n",
      "Iteration 105: loss = 0.176110\n",
      "Iteration 106: loss = 0.175322\n",
      "Iteration 107: loss = 0.174552\n",
      "Iteration 108: loss = 0.173799\n",
      "Iteration 109: loss = 0.173062\n",
      "Iteration 110: loss = 0.172341\n",
      "Iteration 111: loss = 0.171635\n",
      "Iteration 112: loss = 0.170943\n",
      "Iteration 113: loss = 0.170266\n",
      "Iteration 114: loss = 0.169602\n",
      "Iteration 115: loss = 0.168952\n",
      "Iteration 116: loss = 0.168315\n",
      "Iteration 117: loss = 0.167690\n",
      "Iteration 118: loss = 0.167077\n",
      "Iteration 119: loss = 0.166476\n",
      "Iteration 120: loss = 0.165887\n",
      "Iteration 121: loss = 0.165310\n",
      "Iteration 122: loss = 0.164743\n",
      "Iteration 123: loss = 0.164187\n",
      "Iteration 124: loss = 0.163641\n",
      "Iteration 125: loss = 0.163106\n",
      "Iteration 126: loss = 0.162580\n",
      "Iteration 127: loss = 0.162064\n",
      "Iteration 128: loss = 0.161557\n",
      "Iteration 129: loss = 0.161059\n",
      "Iteration 130: loss = 0.160570\n",
      "Iteration 131: loss = 0.160089\n",
      "Iteration 132: loss = 0.159615\n",
      "Iteration 133: loss = 0.159150\n",
      "Iteration 134: loss = 0.158691\n",
      "Iteration 135: loss = 0.158240\n",
      "Iteration 136: loss = 0.157796\n",
      "Iteration 137: loss = 0.157359\n",
      "Iteration 138: loss = 0.156929\n",
      "Iteration 139: loss = 0.156505\n",
      "Iteration 140: loss = 0.156089\n",
      "Iteration 141: loss = 0.155678\n",
      "Iteration 142: loss = 0.155275\n",
      "Iteration 143: loss = 0.154877\n",
      "Iteration 144: loss = 0.154486\n",
      "Iteration 145: loss = 0.154101\n",
      "Iteration 146: loss = 0.153721\n",
      "Iteration 147: loss = 0.153347\n",
      "Iteration 148: loss = 0.152978\n",
      "Iteration 149: loss = 0.152615\n",
      "Iteration 150: loss = 0.152257\n",
      "Iteration 151: loss = 0.151903\n",
      "Iteration 152: loss = 0.151555\n",
      "Iteration 153: loss = 0.151212\n",
      "Iteration 154: loss = 0.150873\n",
      "Iteration 155: loss = 0.150540\n",
      "Iteration 156: loss = 0.150211\n",
      "Iteration 157: loss = 0.149886\n",
      "Iteration 158: loss = 0.149566\n",
      "Iteration 159: loss = 0.149250\n",
      "Iteration 160: loss = 0.148939\n",
      "Iteration 161: loss = 0.148632\n",
      "Iteration 162: loss = 0.148328\n",
      "Iteration 163: loss = 0.148029\n",
      "Iteration 164: loss = 0.147734\n",
      "Iteration 165: loss = 0.147443\n",
      "Iteration 166: loss = 0.147155\n",
      "Iteration 167: loss = 0.146871\n",
      "Iteration 168: loss = 0.146591\n",
      "Iteration 169: loss = 0.146313\n",
      "Iteration 170: loss = 0.146040\n",
      "Iteration 171: loss = 0.145769\n",
      "Iteration 172: loss = 0.145502\n",
      "Iteration 173: loss = 0.145238\n",
      "Iteration 174: loss = 0.144977\n",
      "Iteration 175: loss = 0.144719\n",
      "Iteration 176: loss = 0.144464\n",
      "Iteration 177: loss = 0.144211\n",
      "Iteration 178: loss = 0.143962\n",
      "Iteration 179: loss = 0.143715\n",
      "Iteration 180: loss = 0.143470\n",
      "Iteration 181: loss = 0.143229\n",
      "Iteration 182: loss = 0.142989\n",
      "Iteration 183: loss = 0.142752\n",
      "Iteration 184: loss = 0.142518\n",
      "Iteration 185: loss = 0.142285\n",
      "Iteration 186: loss = 0.142055\n",
      "Iteration 187: loss = 0.141827\n",
      "Iteration 188: loss = 0.141602\n",
      "Iteration 189: loss = 0.141378\n",
      "Iteration 190: loss = 0.141157\n",
      "Iteration 191: loss = 0.140937\n",
      "Iteration 192: loss = 0.140720\n",
      "Iteration 193: loss = 0.140505\n",
      "Iteration 194: loss = 0.140291\n",
      "Iteration 195: loss = 0.140080\n",
      "Iteration 196: loss = 0.139870\n",
      "Iteration 197: loss = 0.139663\n",
      "Iteration 198: loss = 0.139457\n",
      "Iteration 199: loss = 0.139253\n",
      "Iteration 200: loss = 0.139051\n",
      "Iteration 201: loss = 0.138851\n",
      "Iteration 202: loss = 0.138652\n",
      "Iteration 203: loss = 0.138456\n",
      "Iteration 204: loss = 0.138260\n",
      "Iteration 205: loss = 0.138067\n",
      "Iteration 206: loss = 0.137875\n",
      "Iteration 207: loss = 0.137684\n",
      "Iteration 208: loss = 0.137496\n",
      "Iteration 209: loss = 0.137308\n",
      "Iteration 210: loss = 0.137122\n",
      "Iteration 211: loss = 0.136938\n",
      "Iteration 212: loss = 0.136755\n",
      "Iteration 213: loss = 0.136573\n",
      "Iteration 214: loss = 0.136393\n",
      "Iteration 215: loss = 0.136214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss = 1.794673\n",
      "Iteration 1: loss = 1.664751\n",
      "Iteration 2: loss = 1.576527\n",
      "Iteration 3: loss = 1.504182\n",
      "Iteration 4: loss = 1.443560\n",
      "Iteration 5: loss = 1.391926\n",
      "Iteration 6: loss = 1.347284\n",
      "Iteration 7: loss = 1.308156\n",
      "Iteration 8: loss = 1.273432\n",
      "Iteration 9: loss = 1.242267\n",
      "Iteration 10: loss = 1.214013\n",
      "Iteration 11: loss = 1.188163\n",
      "Iteration 12: loss = 1.164315\n",
      "Iteration 13: loss = 1.142152\n",
      "Iteration 14: loss = 1.121419\n",
      "Iteration 15: loss = 1.101912\n",
      "Iteration 16: loss = 1.083462\n",
      "Iteration 17: loss = 1.065927\n",
      "Iteration 18: loss = 1.049192\n",
      "Iteration 19: loss = 1.033160\n",
      "Iteration 20: loss = 1.017751\n",
      "Iteration 21: loss = 1.002901\n",
      "Iteration 22: loss = 0.988565\n",
      "Iteration 23: loss = 0.974707\n",
      "Iteration 24: loss = 0.961289\n",
      "Iteration 25: loss = 0.948275\n",
      "Iteration 26: loss = 0.935636\n",
      "Iteration 27: loss = 0.923360\n",
      "Iteration 28: loss = 0.911442\n",
      "Iteration 29: loss = 0.899878\n",
      "Iteration 30: loss = 0.888668\n",
      "Iteration 31: loss = 0.877804\n",
      "Iteration 32: loss = 0.867275\n",
      "Iteration 33: loss = 0.857068\n",
      "Iteration 34: loss = 0.847173\n",
      "Iteration 35: loss = 0.837588\n",
      "Iteration 36: loss = 0.828308\n",
      "Iteration 37: loss = 0.819327\n",
      "Iteration 38: loss = 0.810641\n",
      "Iteration 39: loss = 0.802245\n",
      "Iteration 40: loss = 0.794136\n",
      "Iteration 41: loss = 0.786303\n",
      "Iteration 42: loss = 0.778736\n",
      "Iteration 43: loss = 0.771427\n",
      "Iteration 44: loss = 0.764369\n",
      "Iteration 45: loss = 0.757553\n",
      "Iteration 46: loss = 0.750973\n",
      "Iteration 47: loss = 0.744619\n",
      "Iteration 48: loss = 0.738483\n",
      "Iteration 49: loss = 0.732558\n",
      "Iteration 50: loss = 0.726842\n",
      "Iteration 51: loss = 0.721333\n",
      "Iteration 52: loss = 0.716027\n",
      "Iteration 53: loss = 0.710912\n",
      "Iteration 54: loss = 0.705974\n",
      "Iteration 55: loss = 0.701202\n",
      "Iteration 56: loss = 0.696587\n",
      "Iteration 57: loss = 0.692122\n",
      "Iteration 58: loss = 0.687801\n",
      "Iteration 59: loss = 0.683616\n",
      "Iteration 60: loss = 0.679563\n",
      "Iteration 61: loss = 0.675637\n",
      "Iteration 62: loss = 0.671832\n",
      "Iteration 63: loss = 0.668144\n",
      "Iteration 64: loss = 0.664566\n",
      "Iteration 65: loss = 0.661096\n",
      "Iteration 66: loss = 0.657728\n",
      "Iteration 67: loss = 0.654458\n",
      "Iteration 68: loss = 0.651282\n",
      "Iteration 69: loss = 0.648195\n",
      "Iteration 70: loss = 0.645194\n",
      "Iteration 71: loss = 0.642273\n",
      "Iteration 72: loss = 0.639430\n",
      "Iteration 73: loss = 0.636662\n",
      "Iteration 74: loss = 0.633964\n",
      "Iteration 75: loss = 0.631333\n",
      "Iteration 76: loss = 0.628766\n",
      "Iteration 77: loss = 0.626257\n",
      "Iteration 78: loss = 0.623802\n",
      "Iteration 79: loss = 0.621400\n",
      "Iteration 80: loss = 0.619051\n",
      "Iteration 81: loss = 0.616755\n",
      "Iteration 82: loss = 0.614511\n",
      "Iteration 83: loss = 0.612316\n",
      "Iteration 84: loss = 0.610169\n",
      "Iteration 85: loss = 0.608068\n",
      "Iteration 86: loss = 0.606010\n",
      "Iteration 87: loss = 0.603994\n",
      "Iteration 88: loss = 0.602019\n",
      "Iteration 89: loss = 0.600083\n",
      "Iteration 90: loss = 0.598185\n",
      "Iteration 91: loss = 0.596322\n",
      "Iteration 92: loss = 0.594493\n",
      "Iteration 93: loss = 0.592697\n",
      "Iteration 94: loss = 0.590933\n",
      "Iteration 95: loss = 0.589198\n",
      "Iteration 96: loss = 0.587491\n",
      "Iteration 97: loss = 0.585811\n",
      "Iteration 98: loss = 0.584157\n",
      "Iteration 99: loss = 0.582529\n",
      "Iteration 100: loss = 0.580926\n",
      "Iteration 101: loss = 0.579349\n",
      "Iteration 102: loss = 0.577797\n",
      "Iteration 103: loss = 0.576270\n",
      "Iteration 104: loss = 0.574767\n",
      "Iteration 105: loss = 0.573287\n",
      "Iteration 106: loss = 0.571831\n",
      "Iteration 107: loss = 0.570396\n",
      "Iteration 108: loss = 0.568983\n",
      "Iteration 109: loss = 0.567591\n",
      "Iteration 110: loss = 0.566218\n",
      "Iteration 111: loss = 0.564863\n",
      "Iteration 112: loss = 0.563527\n",
      "Iteration 113: loss = 0.562209\n",
      "Iteration 114: loss = 0.560908\n",
      "Iteration 115: loss = 0.559625\n",
      "Iteration 116: loss = 0.558359\n",
      "Iteration 117: loss = 0.557109\n",
      "Iteration 118: loss = 0.555877\n",
      "Iteration 119: loss = 0.554663\n",
      "Iteration 120: loss = 0.553465\n",
      "Iteration 121: loss = 0.552284\n",
      "Iteration 122: loss = 0.551119\n",
      "Iteration 123: loss = 0.549971\n",
      "Iteration 124: loss = 0.548838\n",
      "Iteration 125: loss = 0.547721\n",
      "Iteration 126: loss = 0.546619\n",
      "Iteration 127: loss = 0.545531\n",
      "Iteration 128: loss = 0.544457\n",
      "Iteration 129: loss = 0.543396\n",
      "Iteration 130: loss = 0.542348\n",
      "Iteration 131: loss = 0.541313\n",
      "Iteration 132: loss = 0.540290\n",
      "Iteration 133: loss = 0.539280\n",
      "Iteration 134: loss = 0.538281\n",
      "Iteration 135: loss = 0.537294\n",
      "Iteration 136: loss = 0.536318\n",
      "Iteration 137: loss = 0.535353\n",
      "Iteration 138: loss = 0.534399\n",
      "Iteration 139: loss = 0.533456\n",
      "Iteration 140: loss = 0.532522\n",
      "Iteration 141: loss = 0.531599\n",
      "Iteration 142: loss = 0.530686\n",
      "Iteration 143: loss = 0.529782\n",
      "Iteration 144: loss = 0.528887\n",
      "Iteration 145: loss = 0.528002\n",
      "Iteration 146: loss = 0.527125\n",
      "Iteration 147: loss = 0.526257\n",
      "Iteration 148: loss = 0.525398\n",
      "Iteration 149: loss = 0.524554\n",
      "Iteration 150: loss = 0.523736\n",
      "Iteration 151: loss = 0.523029\n",
      "Iteration 152: loss = 0.522707\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3, estimator=LearningShapelets(verbose=1), n_jobs=10,\n",
       "             param_grid={&#x27;C&#x27;: [10, 100, 1000],\n",
       "                         &#x27;min_shapelet_length&#x27;: [0.05, 0.1],\n",
       "                         &#x27;n_shapelets_per_size&#x27;: [0.1, 0.2]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3, estimator=LearningShapelets(verbose=1), n_jobs=10,\n",
       "             param_grid={&#x27;C&#x27;: [10, 100, 1000],\n",
       "                         &#x27;min_shapelet_length&#x27;: [0.05, 0.1],\n",
       "                         &#x27;n_shapelets_per_size&#x27;: [0.1, 0.2]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LearningShapelets</label><div class=\"sk-toggleable__content\"><pre>LearningShapelets(verbose=1)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LearningShapelets</label><div class=\"sk-toggleable__content\"><pre>LearningShapelets(verbose=1)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3, estimator=LearningShapelets(verbose=1), n_jobs=10,\n",
       "             param_grid={'C': [10, 100, 1000],\n",
       "                         'min_shapelet_length': [0.05, 0.1],\n",
       "                         'n_shapelets_per_size': [0.1, 0.2]})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Fit the data\n",
    "grid_search.fit(X_train, y_train)  # X_train and y_train are your training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2047d7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 100, 'min_shapelet_length': 0.1, 'n_shapelets_per_size': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Access the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cecb220e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9833333333333333\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Evaluate the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "accuracy = best_model.score(X_test, y_test)  # X_test and y_test are your validation/test data\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c63a1c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LearningShapelets(n_shapelets_per_size=0.1, min_shapelet_length=0.05, C=1000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0325e17b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss = 1.794477\n",
      "Iteration 1: loss = 1.746170\n",
      "Iteration 2: loss = 1.704576\n",
      "Iteration 3: loss = 1.666062\n",
      "Iteration 4: loss = 1.629012\n",
      "Iteration 5: loss = 1.592659\n",
      "Iteration 6: loss = 1.556587\n",
      "Iteration 7: loss = 1.520538\n",
      "Iteration 8: loss = 1.484337\n",
      "Iteration 9: loss = 1.447871\n",
      "Iteration 10: loss = 1.411073\n",
      "Iteration 11: loss = 1.373937\n",
      "Iteration 12: loss = 1.336526\n",
      "Iteration 13: loss = 1.298943\n",
      "Iteration 14: loss = 1.261289\n",
      "Iteration 15: loss = 1.223709\n",
      "Iteration 16: loss = 1.186350\n",
      "Iteration 17: loss = 1.149377\n",
      "Iteration 18: loss = 1.112974\n",
      "Iteration 19: loss = 1.077293\n",
      "Iteration 20: loss = 1.042453\n",
      "Iteration 21: loss = 1.008601\n",
      "Iteration 22: loss = 0.975838\n",
      "Iteration 23: loss = 0.944203\n",
      "Iteration 24: loss = 0.913761\n",
      "Iteration 25: loss = 0.884549\n",
      "Iteration 26: loss = 0.856630\n",
      "Iteration 27: loss = 0.830067\n",
      "Iteration 28: loss = 0.804853\n",
      "Iteration 29: loss = 0.780918\n",
      "Iteration 30: loss = 0.758177\n",
      "Iteration 31: loss = 0.736581\n",
      "Iteration 32: loss = 0.716103\n",
      "Iteration 33: loss = 0.696707\n",
      "Iteration 34: loss = 0.678342\n",
      "Iteration 35: loss = 0.660935\n",
      "Iteration 36: loss = 0.644406\n",
      "Iteration 37: loss = 0.628702\n",
      "Iteration 38: loss = 0.613799\n",
      "Iteration 39: loss = 0.599677\n",
      "Iteration 40: loss = 0.586303\n",
      "Iteration 41: loss = 0.573628\n",
      "Iteration 42: loss = 0.561601\n",
      "Iteration 43: loss = 0.550171\n",
      "Iteration 44: loss = 0.539294\n",
      "Iteration 45: loss = 0.528933\n",
      "Iteration 46: loss = 0.519055\n",
      "Iteration 47: loss = 0.509633\n",
      "Iteration 48: loss = 0.500643\n",
      "Iteration 49: loss = 0.492062\n",
      "Iteration 50: loss = 0.483865\n",
      "Iteration 51: loss = 0.476029\n",
      "Iteration 52: loss = 0.468529\n",
      "Iteration 53: loss = 0.461342\n",
      "Iteration 54: loss = 0.454441\n",
      "Iteration 55: loss = 0.447805\n",
      "Iteration 56: loss = 0.441418\n",
      "Iteration 57: loss = 0.435272\n",
      "Iteration 58: loss = 0.429356\n",
      "Iteration 59: loss = 0.423656\n",
      "Iteration 60: loss = 0.418154\n",
      "Iteration 61: loss = 0.412841\n",
      "Iteration 62: loss = 0.407710\n",
      "Iteration 63: loss = 0.402753\n",
      "Iteration 64: loss = 0.397959\n",
      "Iteration 65: loss = 0.393328\n",
      "Iteration 66: loss = 0.388856\n",
      "Iteration 67: loss = 0.384540\n",
      "Iteration 68: loss = 0.380376\n",
      "Iteration 69: loss = 0.376359\n",
      "Iteration 70: loss = 0.372484\n",
      "Iteration 71: loss = 0.368744\n",
      "Iteration 72: loss = 0.365128\n",
      "Iteration 73: loss = 0.361627\n",
      "Iteration 74: loss = 0.358234\n",
      "Iteration 75: loss = 0.354942\n",
      "Iteration 76: loss = 0.351741\n",
      "Iteration 77: loss = 0.348625\n",
      "Iteration 78: loss = 0.345591\n",
      "Iteration 79: loss = 0.342635\n",
      "Iteration 80: loss = 0.339756\n",
      "Iteration 81: loss = 0.336953\n",
      "Iteration 82: loss = 0.334222\n",
      "Iteration 83: loss = 0.331560\n",
      "Iteration 84: loss = 0.328967\n",
      "Iteration 85: loss = 0.326441\n",
      "Iteration 86: loss = 0.323980\n",
      "Iteration 87: loss = 0.321582\n",
      "Iteration 88: loss = 0.319244\n",
      "Iteration 89: loss = 0.316963\n",
      "Iteration 90: loss = 0.314738\n",
      "Iteration 91: loss = 0.312568\n",
      "Iteration 92: loss = 0.310450\n",
      "Iteration 93: loss = 0.308383\n",
      "Iteration 94: loss = 0.306367\n",
      "Iteration 95: loss = 0.304404\n",
      "Iteration 96: loss = 0.302491\n",
      "Iteration 97: loss = 0.300625\n",
      "Iteration 98: loss = 0.298805\n",
      "Iteration 99: loss = 0.297031\n",
      "Iteration 100: loss = 0.295306\n",
      "Iteration 101: loss = 0.293626\n",
      "Iteration 102: loss = 0.291981\n",
      "Iteration 103: loss = 0.290373\n",
      "Iteration 104: loss = 0.288802\n",
      "Iteration 105: loss = 0.287268\n",
      "Iteration 106: loss = 0.285768\n",
      "Iteration 107: loss = 0.284302\n",
      "Iteration 108: loss = 0.282866\n",
      "Iteration 109: loss = 0.281460\n",
      "Iteration 110: loss = 0.280086\n",
      "Iteration 111: loss = 0.278742\n",
      "Iteration 112: loss = 0.277429\n",
      "Iteration 113: loss = 0.276148\n",
      "Iteration 114: loss = 0.274898\n",
      "Iteration 115: loss = 0.273678\n",
      "Iteration 116: loss = 0.272485\n",
      "Iteration 117: loss = 0.271317\n",
      "Iteration 118: loss = 0.270172\n",
      "Iteration 119: loss = 0.269048\n",
      "Iteration 120: loss = 0.267945\n",
      "Iteration 121: loss = 0.266861\n",
      "Iteration 122: loss = 0.265794\n",
      "Iteration 123: loss = 0.264743\n",
      "Iteration 124: loss = 0.263709\n",
      "Iteration 125: loss = 0.262693\n",
      "Iteration 126: loss = 0.261694\n",
      "Iteration 127: loss = 0.260713\n",
      "Iteration 128: loss = 0.259750\n",
      "Iteration 129: loss = 0.258804\n",
      "Iteration 130: loss = 0.257874\n",
      "Iteration 131: loss = 0.256961\n",
      "Iteration 132: loss = 0.256062\n",
      "Iteration 133: loss = 0.255178\n",
      "Iteration 134: loss = 0.254307\n",
      "Iteration 135: loss = 0.253450\n",
      "Iteration 136: loss = 0.252605\n",
      "Iteration 137: loss = 0.251772\n",
      "Iteration 138: loss = 0.250953\n",
      "Iteration 139: loss = 0.250148\n",
      "Iteration 140: loss = 0.249357\n",
      "Iteration 141: loss = 0.248579\n",
      "Iteration 142: loss = 0.247815\n",
      "Iteration 143: loss = 0.247063\n",
      "Iteration 144: loss = 0.246322\n",
      "Iteration 145: loss = 0.245593\n",
      "Iteration 146: loss = 0.244874\n",
      "Iteration 147: loss = 0.244166\n",
      "Iteration 148: loss = 0.243467\n",
      "Iteration 149: loss = 0.242779\n",
      "Iteration 150: loss = 0.242099\n",
      "Iteration 151: loss = 0.241430\n",
      "Iteration 152: loss = 0.240769\n",
      "Iteration 153: loss = 0.240118\n",
      "Iteration 154: loss = 0.239476\n",
      "Iteration 155: loss = 0.238842\n",
      "Iteration 156: loss = 0.238218\n",
      "Iteration 157: loss = 0.237601\n",
      "Iteration 158: loss = 0.236994\n",
      "Iteration 159: loss = 0.236395\n",
      "Iteration 160: loss = 0.235803\n",
      "Iteration 161: loss = 0.235220\n",
      "Iteration 162: loss = 0.234644\n",
      "Iteration 163: loss = 0.234075\n",
      "Iteration 164: loss = 0.233513\n",
      "Iteration 165: loss = 0.232957\n",
      "Iteration 166: loss = 0.232408\n",
      "Iteration 167: loss = 0.231863\n",
      "Iteration 168: loss = 0.231324\n",
      "Iteration 169: loss = 0.230789\n",
      "Iteration 170: loss = 0.230259\n",
      "Iteration 171: loss = 0.229733\n",
      "Iteration 172: loss = 0.229210\n",
      "Iteration 173: loss = 0.228691\n",
      "Iteration 174: loss = 0.228176\n",
      "Iteration 175: loss = 0.227665\n",
      "Iteration 176: loss = 0.227158\n",
      "Iteration 177: loss = 0.226657\n",
      "Iteration 178: loss = 0.226160\n",
      "Iteration 179: loss = 0.225669\n",
      "Iteration 180: loss = 0.225184\n",
      "Iteration 181: loss = 0.224703\n",
      "Iteration 182: loss = 0.224229\n",
      "Iteration 183: loss = 0.223759\n",
      "Iteration 184: loss = 0.223295\n",
      "Iteration 185: loss = 0.222836\n",
      "Iteration 186: loss = 0.222383\n",
      "Iteration 187: loss = 0.221934\n",
      "Iteration 188: loss = 0.221491\n",
      "Iteration 189: loss = 0.221053\n",
      "Iteration 190: loss = 0.220619\n",
      "Iteration 191: loss = 0.220191\n",
      "Iteration 192: loss = 0.219767\n",
      "Iteration 193: loss = 0.219348\n",
      "Iteration 194: loss = 0.218934\n",
      "Iteration 195: loss = 0.218524\n",
      "Iteration 196: loss = 0.218118\n",
      "Iteration 197: loss = 0.217716\n",
      "Iteration 198: loss = 0.217317\n",
      "Iteration 199: loss = 0.216922\n",
      "Iteration 200: loss = 0.216531\n",
      "Iteration 201: loss = 0.216142\n",
      "Iteration 202: loss = 0.215757\n",
      "Iteration 203: loss = 0.215375\n",
      "Iteration 204: loss = 0.214997\n",
      "Iteration 205: loss = 0.214621\n",
      "Iteration 206: loss = 0.214249\n",
      "Iteration 207: loss = 0.213879\n",
      "Iteration 208: loss = 0.213513\n",
      "Iteration 209: loss = 0.213149\n",
      "Iteration 210: loss = 0.212791\n",
      "Iteration 211: loss = 0.212435\n",
      "Iteration 212: loss = 0.212089\n",
      "Iteration 213: loss = 0.211748\n",
      "Iteration 214: loss = 0.211432\n",
      "Iteration 215: loss = 0.211138\n",
      "Iteration 216: loss = 0.210919\n",
      "Iteration 217: loss = 0.210802\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LearningShapelets(min_shapelet_length=0.05, n_shapelets_per_size=0.1, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LearningShapelets</label><div class=\"sk-toggleable__content\"><pre>LearningShapelets(min_shapelet_length=0.05, n_shapelets_per_size=0.1, verbose=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LearningShapelets(min_shapelet_length=0.05, n_shapelets_per_size=0.1, verbose=1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ae9f5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eea15b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9916666666666667\n",
      "Iteration 82: loss = 0.670097\n",
      "Iteration 83: loss = 0.667235\n",
      "Iteration 84: loss = 0.664413\n",
      "Iteration 85: loss = 0.661633\n",
      "Iteration 86: loss = 0.658891\n",
      "Iteration 87: loss = 0.656182\n",
      "Iteration 88: loss = 0.653502\n",
      "Iteration 89: loss = 0.650850\n",
      "Iteration 90: loss = 0.648219\n",
      "Iteration 91: loss = 0.645584\n",
      "Iteration 92: loss = 0.642951\n",
      "Iteration 93: loss = 0.640326\n",
      "Iteration 94: loss = 0.637722\n",
      "Iteration 95: loss = 0.635148\n",
      "Iteration 96: loss = 0.632600\n",
      "Iteration 97: loss = 0.630046\n",
      "Iteration 98: loss = 0.627481\n",
      "Iteration 99: loss = 0.624938\n",
      "Iteration 100: loss = 0.622441\n",
      "Iteration 101: loss = 0.619971\n",
      "Iteration 102: loss = 0.617647\n",
      "Iteration 103: loss = 0.615789\n",
      "Iteration 104: loss = 0.614518\n",
      "Iteration 105: loss = 0.613902\n",
      "Iteration 106: loss = 0.613382\n",
      "Iteration 0: loss = 1.790266\n",
      "Iteration 1: loss = 1.530333\n",
      "Iteration 2: loss = 1.378892\n",
      "Iteration 3: loss = 1.272936\n",
      "Iteration 4: loss = 1.206893\n",
      "Iteration 5: loss = 1.141512\n",
      "Iteration 6: loss = 1.103761\n",
      "Iteration 7: loss = 1.050442\n",
      "Iteration 8: loss = 1.020986\n",
      "Iteration 9: loss = 0.977697\n",
      "Iteration 10: loss = 0.950893\n",
      "Iteration 11: loss = 0.918387\n",
      "Iteration 12: loss = 0.894843\n",
      "Iteration 13: loss = 0.871175\n",
      "Iteration 14: loss = 0.851976\n",
      "Iteration 15: loss = 0.834366\n",
      "Iteration 16: loss = 0.819080\n",
      "Iteration 17: loss = 0.805203\n",
      "Iteration 18: loss = 0.792651\n",
      "Iteration 19: loss = 0.781055\n",
      "Iteration 20: loss = 0.770316\n",
      "Iteration 21: loss = 0.760252\n",
      "Iteration 22: loss = 0.750811\n",
      "Iteration 23: loss = 0.741887\n",
      "Iteration 24: loss = 0.733446\n",
      "Iteration 25: loss = 0.725419\n",
      "Iteration 26: loss = 0.717780\n",
      "Iteration 27: loss = 0.710485\n",
      "Iteration 28: loss = 0.703510\n",
      "Iteration 29: loss = 0.696824\n",
      "Iteration 30: loss = 0.690406\n",
      "Iteration 31: loss = 0.684231\n",
      "Iteration 32: loss = 0.678282\n",
      "Iteration 33: loss = 0.672541\n",
      "Iteration 34: loss = 0.666998\n",
      "Iteration 35: loss = 0.661640\n",
      "Iteration 36: loss = 0.656453\n",
      "Iteration 37: loss = 0.651425\n",
      "Iteration 38: loss = 0.646545\n",
      "Iteration 39: loss = 0.641808\n",
      "Iteration 40: loss = 0.637207\n",
      "Iteration 41: loss = 0.632734\n",
      "Iteration 42: loss = 0.628385\n",
      "Iteration 43: loss = 0.624152\n",
      "Iteration 44: loss = 0.620031\n",
      "Iteration 45: loss = 0.616015\n",
      "Iteration 46: loss = 0.612100\n",
      "Iteration 47: loss = 0.608280\n",
      "Iteration 48: loss = 0.604551\n",
      "Iteration 49: loss = 0.600910\n",
      "Iteration 50: loss = 0.597350\n",
      "Iteration 51: loss = 0.593870\n",
      "Iteration 52: loss = 0.590465\n",
      "Iteration 53: loss = 0.587133\n",
      "Iteration 54: loss = 0.583872\n",
      "Iteration 55: loss = 0.580679\n",
      "Iteration 56: loss = 0.577554\n",
      "Iteration 57: loss = 0.574494\n",
      "Iteration 58: loss = 0.571498\n",
      "Iteration 59: loss = 0.568563\n",
      "Iteration 60: loss = 0.565688\n",
      "Iteration 61: loss = 0.562871\n",
      "Iteration 62: loss = 0.560108\n",
      "Iteration 63: loss = 0.557397\n",
      "Iteration 64: loss = 0.554737\n",
      "Iteration 65: loss = 0.552127\n",
      "Iteration 66: loss = 0.549565\n",
      "Iteration 67: loss = 0.547049\n",
      "Iteration 68: loss = 0.544579\n",
      "Iteration 69: loss = 0.542151\n",
      "Iteration 70: loss = 0.539766\n",
      "Iteration 71: loss = 0.537421\n",
      "Iteration 72: loss = 0.535115\n",
      "Iteration 73: loss = 0.532849\n",
      "Iteration 74: loss = 0.530623\n",
      "Iteration 75: loss = 0.528437\n",
      "Iteration 76: loss = 0.526291\n",
      "Iteration 77: loss = 0.524181\n",
      "Iteration 78: loss = 0.522106\n",
      "Iteration 79: loss = 0.520062\n",
      "Iteration 80: loss = 0.518049\n",
      "Iteration 81: loss = 0.516067\n",
      "Iteration 82: loss = 0.514115\n",
      "Iteration 83: loss = 0.512192\n",
      "Iteration 84: loss = 0.510298\n",
      "Iteration 85: loss = 0.508432\n",
      "Iteration 86: loss = 0.506594\n",
      "Iteration 87: loss = 0.504784\n",
      "Iteration 88: loss = 0.502999\n",
      "Iteration 89: loss = 0.501241\n",
      "Iteration 90: loss = 0.499508\n",
      "Iteration 91: loss = 0.497801\n",
      "Iteration 92: loss = 0.496118\n",
      "Iteration 93: loss = 0.494459\n",
      "Iteration 94: loss = 0.492824\n",
      "Iteration 95: loss = 0.491212\n",
      "Iteration 96: loss = 0.489622\n",
      "Iteration 97: loss = 0.488055\n",
      "Iteration 98: loss = 0.486509\n",
      "Iteration 99: loss = 0.484984\n",
      "Iteration 100: loss = 0.483480\n",
      "Iteration 101: loss = 0.481995\n",
      "Iteration 102: loss = 0.480529\n",
      "Iteration 103: loss = 0.479082\n",
      "Iteration 104: loss = 0.477653\n",
      "Iteration 105: loss = 0.476241\n",
      "Iteration 106: loss = 0.474847\n",
      "Iteration 107: loss = 0.473469\n",
      "Iteration 108: loss = 0.472108\n",
      "Iteration 109: loss = 0.470762\n",
      "Iteration 110: loss = 0.469433\n",
      "Iteration 111: loss = 0.468119\n",
      "Iteration 112: loss = 0.466822\n",
      "Iteration 113: loss = 0.465540\n",
      "Iteration 114: loss = 0.464274\n",
      "Iteration 115: loss = 0.463023\n",
      "Iteration 116: loss = 0.461786\n",
      "Iteration 117: loss = 0.460564\n",
      "Iteration 118: loss = 0.459355\n",
      "Iteration 119: loss = 0.458159\n",
      "Iteration 120: loss = 0.456975\n",
      "Iteration 121: loss = 0.455803\n",
      "Iteration 122: loss = 0.454643\n",
      "Iteration 123: loss = 0.453495\n",
      "Iteration 124: loss = 0.452359\n",
      "Iteration 125: loss = 0.451235\n",
      "Iteration 126: loss = 0.450123\n",
      "Iteration 127: loss = 0.449023\n",
      "Iteration 128: loss = 0.447934\n",
      "Iteration 129: loss = 0.446858\n",
      "Iteration 130: loss = 0.445793\n",
      "Iteration 131: loss = 0.444738\n",
      "Iteration 132: loss = 0.443695\n",
      "Iteration 133: loss = 0.442663\n",
      "Iteration 134: loss = 0.441641\n",
      "Iteration 135: loss = 0.440629\n",
      "Iteration 136: loss = 0.439627\n",
      "Iteration 137: loss = 0.438635\n",
      "Iteration 138: loss = 0.437652\n",
      "Iteration 139: loss = 0.436679\n",
      "Iteration 140: loss = 0.435716\n",
      "Iteration 141: loss = 0.434762\n",
      "Iteration 142: loss = 0.433817\n",
      "Iteration 143: loss = 0.432881\n",
      "Iteration 144: loss = 0.431955\n",
      "Iteration 145: loss = 0.431037\n",
      "Iteration 146: loss = 0.430127\n",
      "Iteration 147: loss = 0.429226\n",
      "Iteration 148: loss = 0.428332\n",
      "Iteration 149: loss = 0.427446\n",
      "Iteration 150: loss = 0.426568\n",
      "Iteration 151: loss = 0.425697\n",
      "Iteration 152: loss = 0.424833\n",
      "Iteration 153: loss = 0.423976\n",
      "Iteration 154: loss = 0.423127\n",
      "Iteration 155: loss = 0.422283\n",
      "Iteration 156: loss = 0.421448\n",
      "Iteration 157: loss = 0.420617\n",
      "Iteration 158: loss = 0.419800\n",
      "Iteration 159: loss = 0.418983\n",
      "Iteration 160: loss = 0.418195\n",
      "Iteration 161: loss = 0.417413\n",
      "Iteration 162: loss = 0.416731\n",
      "Iteration 163: loss = 0.416168\n",
      "Iteration 164: loss = 0.415695\n",
      "Iteration 165: loss = 0.415476\n",
      "Iteration 188: loss = 0.179212\n",
      "Iteration 189: loss = 0.178930\n",
      "Iteration 190: loss = 0.178650\n",
      "Iteration 191: loss = 0.178374\n",
      "Iteration 192: loss = 0.178100\n",
      "Iteration 193: loss = 0.177830\n",
      "Iteration 194: loss = 0.177562\n",
      "Iteration 195: loss = 0.177297\n",
      "Iteration 196: loss = 0.177034\n",
      "Iteration 197: loss = 0.176775\n",
      "Iteration 198: loss = 0.176518\n",
      "Iteration 199: loss = 0.176263\n",
      "Iteration 200: loss = 0.176011\n",
      "Iteration 201: loss = 0.175761\n",
      "Iteration 202: loss = 0.175514\n",
      "Iteration 203: loss = 0.175270\n",
      "Iteration 204: loss = 0.175027\n",
      "Iteration 205: loss = 0.174788\n",
      "Iteration 206: loss = 0.174550\n",
      "Iteration 207: loss = 0.174315\n",
      "Iteration 208: loss = 0.174081\n",
      "Iteration 209: loss = 0.173851\n",
      "Iteration 210: loss = 0.173622\n",
      "Iteration 211: loss = 0.173395\n",
      "Iteration 212: loss = 0.173171\n",
      "Iteration 213: loss = 0.172948\n",
      "Iteration 214: loss = 0.172728\n",
      "Iteration 215: loss = 0.172509\n",
      "Iteration 216: loss = 0.172293\n",
      "Iteration 217: loss = 0.172078\n",
      "Iteration 218: loss = 0.171866\n",
      "Iteration 219: loss = 0.171655\n",
      "Iteration 220: loss = 0.171446\n",
      "Iteration 221: loss = 0.171239\n",
      "Iteration 222: loss = 0.171034\n",
      "Iteration 223: loss = 0.170830\n",
      "Iteration 224: loss = 0.170628\n",
      "Iteration 225: loss = 0.170428\n",
      "Iteration 226: loss = 0.170229\n",
      "Iteration 227: loss = 0.170033\n",
      "Iteration 228: loss = 0.169838\n",
      "Iteration 229: loss = 0.169644\n",
      "Iteration 230: loss = 0.169452\n",
      "Iteration 231: loss = 0.169262\n",
      "Iteration 232: loss = 0.169073\n",
      "Iteration 233: loss = 0.168886\n",
      "Iteration 234: loss = 0.168701\n",
      "Iteration 235: loss = 0.168517\n",
      "Iteration 236: loss = 0.168334\n",
      "Iteration 237: loss = 0.168153\n",
      "Iteration 238: loss = 0.167974\n",
      "Iteration 239: loss = 0.167796\n",
      "Iteration 240: loss = 0.167619\n",
      "Iteration 241: loss = 0.167444\n",
      "Iteration 242: loss = 0.167270\n",
      "Iteration 243: loss = 0.167098\n",
      "Iteration 244: loss = 0.166927\n",
      "Iteration 245: loss = 0.166758\n",
      "Iteration 246: loss = 0.166590\n",
      "Iteration 247: loss = 0.166423\n",
      "Iteration 248: loss = 0.166258\n",
      "Iteration 246: loss = 0.191971\n",
      "Iteration 247: loss = 0.191698\n",
      "Iteration 248: loss = 0.191429\n",
      "Iteration 249: loss = 0.191160\n",
      "Iteration 250: loss = 0.190902\n",
      "Iteration 251: loss = 0.190644\n",
      "Iteration 252: loss = 0.190413\n",
      "Iteration 253: loss = 0.190199\n",
      "Iteration 254: loss = 0.190078\n",
      "Iteration 0: loss = 1.792044\n",
      "Iteration 1: loss = 1.662092\n",
      "Iteration 2: loss = 1.567118\n",
      "Iteration 3: loss = 1.486517\n",
      "Iteration 4: loss = 1.416727\n",
      "Iteration 5: loss = 1.355336\n",
      "Iteration 6: loss = 1.300548\n",
      "Iteration 7: loss = 1.251016\n",
      "Iteration 8: loss = 1.205722\n",
      "Iteration 9: loss = 1.163894\n",
      "Iteration 10: loss = 1.124938\n",
      "Iteration 11: loss = 1.088392\n",
      "Iteration 12: loss = 1.053900\n",
      "Iteration 13: loss = 1.021182\n",
      "Iteration 14: loss = 0.990011\n",
      "Iteration 15: loss = 0.960204\n",
      "Iteration 16: loss = 0.931612\n",
      "Iteration 17: loss = 0.904114\n",
      "Iteration 18: loss = 0.877620\n",
      "Iteration 19: loss = 0.852056\n",
      "Iteration 20: loss = 0.827364\n",
      "Iteration 21: loss = 0.803502\n",
      "Iteration 22: loss = 0.780434\n",
      "Iteration 23: loss = 0.758130\n",
      "Iteration 24: loss = 0.736578\n",
      "Iteration 25: loss = 0.715774\n",
      "Iteration 26: loss = 0.695698\n",
      "Iteration 27: loss = 0.676327\n",
      "Iteration 28: loss = 0.657650\n",
      "Iteration 29: loss = 0.639666\n",
      "Iteration 30: loss = 0.622366\n",
      "Iteration 31: loss = 0.605726\n",
      "Iteration 32: loss = 0.589721\n",
      "Iteration 33: loss = 0.574337\n",
      "Iteration 34: loss = 0.559556\n",
      "Iteration 35: loss = 0.545366\n",
      "Iteration 36: loss = 0.531752\n",
      "Iteration 37: loss = 0.518703\n",
      "Iteration 38: loss = 0.506202\n",
      "Iteration 39: loss = 0.494233\n",
      "Iteration 40: loss = 0.482776\n",
      "Iteration 41: loss = 0.471812\n",
      "Iteration 42: loss = 0.461321\n",
      "Iteration 43: loss = 0.451284\n",
      "Iteration 44: loss = 0.441681\n",
      "Iteration 45: loss = 0.432493\n",
      "Iteration 46: loss = 0.423703\n",
      "Iteration 47: loss = 0.415291\n",
      "Iteration 48: loss = 0.407244\n",
      "Iteration 49: loss = 0.399544\n",
      "Iteration 50: loss = 0.392178\n",
      "Iteration 51: loss = 0.385131\n",
      "Iteration 52: loss = 0.378387\n",
      "Iteration 53: loss = 0.371932\n",
      "Iteration 54: loss = 0.365751\n",
      "Iteration 55: loss = 0.359831\n",
      "Iteration 56: loss = 0.354158\n",
      "Iteration 57: loss = 0.348721\n",
      "Iteration 58: loss = 0.343506\n",
      "Iteration 59: loss = 0.338504\n",
      "Iteration 60: loss = 0.333703\n",
      "Iteration 61: loss = 0.329095\n",
      "Iteration 62: loss = 0.324670\n",
      "Iteration 63: loss = 0.320420\n",
      "Iteration 64: loss = 0.316336\n",
      "Iteration 65: loss = 0.312411\n",
      "Iteration 66: loss = 0.308637\n",
      "Iteration 67: loss = 0.305007\n",
      "Iteration 68: loss = 0.301515\n",
      "Iteration 69: loss = 0.298152\n",
      "Iteration 70: loss = 0.294913\n",
      "Iteration 71: loss = 0.291790\n",
      "Iteration 72: loss = 0.288777\n",
      "Iteration 73: loss = 0.285870\n",
      "Iteration 74: loss = 0.283064\n",
      "Iteration 75: loss = 0.280353\n",
      "Iteration 76: loss = 0.277735\n",
      "Iteration 77: loss = 0.275206\n",
      "Iteration 78: loss = 0.272761\n",
      "Iteration 79: loss = 0.270397\n",
      "Iteration 80: loss = 0.268111\n",
      "Iteration 81: loss = 0.265899\n",
      "Iteration 82: loss = 0.263758\n",
      "Iteration 83: loss = 0.261686\n",
      "Iteration 84: loss = 0.259678\n",
      "Iteration 85: loss = 0.257733\n",
      "Iteration 86: loss = 0.255848\n",
      "Iteration 87: loss = 0.254021\n",
      "Iteration 88: loss = 0.252249\n",
      "Iteration 89: loss = 0.250530\n",
      "Iteration 90: loss = 0.248862\n",
      "Iteration 91: loss = 0.247242\n",
      "Iteration 92: loss = 0.245670\n",
      "Iteration 93: loss = 0.244142\n",
      "Iteration 94: loss = 0.242658\n",
      "Iteration 95: loss = 0.241215\n",
      "Iteration 96: loss = 0.239812\n",
      "Iteration 97: loss = 0.238448\n",
      "Iteration 98: loss = 0.237120\n",
      "Iteration 99: loss = 0.235828\n",
      "Iteration 100: loss = 0.234570\n",
      "Iteration 101: loss = 0.233344\n",
      "Iteration 102: loss = 0.232150\n",
      "Iteration 103: loss = 0.230986\n",
      "Iteration 104: loss = 0.229851\n",
      "Iteration 105: loss = 0.228743\n",
      "Iteration 106: loss = 0.227662\n",
      "Iteration 107: loss = 0.226603\n",
      "Iteration 108: loss = 0.225567\n",
      "Iteration 109: loss = 0.224550\n",
      "Iteration 110: loss = 0.223551\n",
      "Iteration 111: loss = 0.222572\n",
      "Iteration 112: loss = 0.221613\n",
      "Iteration 113: loss = 0.220675\n",
      "Iteration 114: loss = 0.219758\n",
      "Iteration 115: loss = 0.218863\n",
      "Iteration 116: loss = 0.217988\n",
      "Iteration 117: loss = 0.217133\n",
      "Iteration 118: loss = 0.216298\n",
      "Iteration 119: loss = 0.215481\n",
      "Iteration 120: loss = 0.214681\n",
      "Iteration 121: loss = 0.213899\n",
      "Iteration 122: loss = 0.213134\n",
      "Iteration 123: loss = 0.212385\n",
      "Iteration 124: loss = 0.211651\n",
      "Iteration 125: loss = 0.210933\n",
      "Iteration 126: loss = 0.210230\n",
      "Iteration 127: loss = 0.209541\n",
      "Iteration 128: loss = 0.208865\n",
      "Iteration 129: loss = 0.208203\n",
      "Iteration 130: loss = 0.207554\n",
      "Iteration 131: loss = 0.206917\n",
      "Iteration 132: loss = 0.206293\n",
      "Iteration 133: loss = 0.205681\n",
      "Iteration 134: loss = 0.205079\n",
      "Iteration 135: loss = 0.204489\n",
      "Iteration 136: loss = 0.203910\n",
      "Iteration 137: loss = 0.203342\n",
      "Iteration 138: loss = 0.202783\n",
      "Iteration 139: loss = 0.202235\n",
      "Iteration 140: loss = 0.201696\n",
      "Iteration 141: loss = 0.201167\n",
      "Iteration 142: loss = 0.200647\n",
      "Iteration 143: loss = 0.200136\n",
      "Iteration 144: loss = 0.199634\n",
      "Iteration 145: loss = 0.199141\n",
      "Iteration 146: loss = 0.198656\n",
      "Iteration 147: loss = 0.198179\n",
      "Iteration 148: loss = 0.197711\n",
      "Iteration 149: loss = 0.197250\n",
      "Iteration 150: loss = 0.196796\n",
      "Iteration 151: loss = 0.196351\n",
      "Iteration 152: loss = 0.195912\n",
      "Iteration 153: loss = 0.195480\n",
      "Iteration 154: loss = 0.195056\n",
      "Iteration 155: loss = 0.194638\n",
      "Iteration 156: loss = 0.194227\n",
      "Iteration 157: loss = 0.193822\n",
      "Iteration 158: loss = 0.193424\n",
      "Iteration 159: loss = 0.193031\n",
      "Iteration 160: loss = 0.192645\n",
      "Iteration 161: loss = 0.192264\n",
      "Iteration 162: loss = 0.191890\n",
      "Iteration 163: loss = 0.191521\n",
      "Iteration 164: loss = 0.191157\n",
      "Iteration 165: loss = 0.190798\n",
      "Iteration 166: loss = 0.190445\n",
      "Iteration 167: loss = 0.190097\n",
      "Iteration 168: loss = 0.189754\n",
      "Iteration 169: loss = 0.189415\n",
      "Iteration 170: loss = 0.189082\n",
      "Iteration 171: loss = 0.188753\n",
      "Iteration 172: loss = 0.188428\n",
      "Iteration 173: loss = 0.188108\n",
      "Iteration 174: loss = 0.187792\n",
      "Iteration 175: loss = 0.187480\n",
      "Iteration 176: loss = 0.187172\n",
      "Iteration 177: loss = 0.186869\n",
      "Iteration 178: loss = 0.186569\n",
      "Iteration 179: loss = 0.186273\n",
      "Iteration 180: loss = 0.185981\n",
      "Iteration 181: loss = 0.185692\n",
      "Iteration 182: loss = 0.185407\n",
      "Iteration 183: loss = 0.185126\n",
      "Iteration 184: loss = 0.184848\n",
      "Iteration 185: loss = 0.184573\n",
      "Iteration 186: loss = 0.184301\n",
      "Iteration 187: loss = 0.184033\n",
      "Iteration 188: loss = 0.183768\n",
      "Iteration 189: loss = 0.183505\n",
      "Iteration 190: loss = 0.183246\n",
      "Iteration 191: loss = 0.182990\n",
      "Iteration 192: loss = 0.182736\n",
      "Iteration 193: loss = 0.182485\n",
      "Iteration 194: loss = 0.182237\n",
      "Iteration 195: loss = 0.181992\n",
      "Iteration 196: loss = 0.181749\n",
      "Iteration 197: loss = 0.181509\n",
      "Iteration 198: loss = 0.181271\n",
      "Iteration 199: loss = 0.181036\n",
      "Iteration 200: loss = 0.180803\n",
      "Iteration 201: loss = 0.180573\n",
      "Iteration 202: loss = 0.180344\n",
      "Iteration 203: loss = 0.180118\n",
      "Iteration 204: loss = 0.179895\n",
      "Iteration 205: loss = 0.179673\n",
      "Iteration 206: loss = 0.179453\n",
      "Iteration 207: loss = 0.179236\n",
      "Iteration 208: loss = 0.179020\n",
      "Iteration 209: loss = 0.178807\n",
      "Iteration 210: loss = 0.178595\n",
      "Iteration 211: loss = 0.178385\n",
      "Iteration 212: loss = 0.178177\n",
      "Iteration 213: loss = 0.177971\n",
      "Iteration 214: loss = 0.177766\n",
      "Iteration 215: loss = 0.177564\n",
      "Iteration 216: loss = 0.177363\n",
      "Iteration 217: loss = 0.177163\n",
      "Iteration 218: loss = 0.176965\n",
      "Iteration 219: loss = 0.176769\n",
      "Iteration 220: loss = 0.176574\n",
      "Iteration 221: loss = 0.176381\n",
      "Iteration 222: loss = 0.176189\n",
      "Iteration 223: loss = 0.175999\n",
      "Iteration 224: loss = 0.175810\n",
      "Iteration 225: loss = 0.175623\n",
      "Iteration 226: loss = 0.175437\n",
      "Iteration 227: loss = 0.175252\n",
      "Iteration 228: loss = 0.175068\n",
      "Iteration 229: loss = 0.174886\n",
      "Iteration 230: loss = 0.174705\n",
      "Iteration 231: loss = 0.174525\n",
      "Iteration 232: loss = 0.174347\n",
      "Iteration 233: loss = 0.174170\n",
      "Iteration 234: loss = 0.173994\n",
      "Iteration 235: loss = 0.173820\n",
      "Iteration 236: loss = 0.173646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 175: loss = 0.192323\n",
      "Iteration 176: loss = 0.192010\n",
      "Iteration 177: loss = 0.191701\n",
      "Iteration 178: loss = 0.191395\n",
      "Iteration 179: loss = 0.191092\n",
      "Iteration 180: loss = 0.190794\n",
      "Iteration 181: loss = 0.190498\n",
      "Iteration 182: loss = 0.190206\n",
      "Iteration 183: loss = 0.189917\n",
      "Iteration 184: loss = 0.189631\n",
      "Iteration 185: loss = 0.189348\n",
      "Iteration 186: loss = 0.189068\n",
      "Iteration 187: loss = 0.188792\n",
      "Iteration 188: loss = 0.188518\n",
      "Iteration 189: loss = 0.188247\n",
      "Iteration 190: loss = 0.187978\n",
      "Iteration 191: loss = 0.187713\n",
      "Iteration 192: loss = 0.187450\n",
      "Iteration 193: loss = 0.187190\n",
      "Iteration 194: loss = 0.186932\n",
      "Iteration 195: loss = 0.186678\n",
      "Iteration 196: loss = 0.186425\n",
      "Iteration 197: loss = 0.186175\n",
      "Iteration 198: loss = 0.185928\n",
      "Iteration 199: loss = 0.185683\n",
      "Iteration 200: loss = 0.185441\n",
      "Iteration 201: loss = 0.185201\n",
      "Iteration 202: loss = 0.184963\n",
      "Iteration 203: loss = 0.184727\n",
      "Iteration 204: loss = 0.184494\n",
      "Iteration 205: loss = 0.184263\n",
      "Iteration 206: loss = 0.184035\n",
      "Iteration 207: loss = 0.183808\n",
      "Iteration 208: loss = 0.183584\n",
      "Iteration 209: loss = 0.183362\n",
      "Iteration 210: loss = 0.183142\n",
      "Iteration 211: loss = 0.182924\n",
      "Iteration 212: loss = 0.182708\n",
      "Iteration 213: loss = 0.182494\n",
      "Iteration 214: loss = 0.182281\n",
      "Iteration 215: loss = 0.182071\n",
      "Iteration 216: loss = 0.181863\n",
      "Iteration 217: loss = 0.181656\n",
      "Iteration 218: loss = 0.181451\n",
      "Iteration 219: loss = 0.181248\n",
      "Iteration 220: loss = 0.181047\n",
      "Iteration 221: loss = 0.180847\n",
      "Iteration 222: loss = 0.180649\n",
      "Iteration 223: loss = 0.180453\n",
      "Iteration 224: loss = 0.180258\n",
      "Iteration 225: loss = 0.180065\n",
      "Iteration 226: loss = 0.179873\n",
      "Iteration 227: loss = 0.179683\n",
      "Iteration 228: loss = 0.179494\n",
      "Iteration 229: loss = 0.179306\n",
      "Iteration 230: loss = 0.179120\n",
      "Iteration 231: loss = 0.178935\n",
      "Iteration 232: loss = 0.178751\n",
      "Iteration 233: loss = 0.178569\n",
      "Iteration 234: loss = 0.178388\n",
      "Iteration 235: loss = 0.178208\n",
      "Iteration 236: loss = 0.178029\n",
      "Iteration 237: loss = 0.177852\n",
      "Iteration 195: loss = 0.187891\n",
      "Iteration 196: loss = 0.187577\n",
      "Iteration 197: loss = 0.187266\n",
      "Iteration 198: loss = 0.186957\n",
      "Iteration 199: loss = 0.186650\n",
      "Iteration 200: loss = 0.186346\n",
      "Iteration 201: loss = 0.186045\n",
      "Iteration 202: loss = 0.185745\n",
      "Iteration 203: loss = 0.185448\n",
      "Iteration 204: loss = 0.185153\n",
      "Iteration 205: loss = 0.184860\n",
      "Iteration 206: loss = 0.184569\n",
      "Iteration 207: loss = 0.184280\n",
      "Iteration 208: loss = 0.183994\n",
      "Iteration 209: loss = 0.183709\n",
      "Iteration 210: loss = 0.183427\n",
      "Iteration 211: loss = 0.183147\n",
      "Iteration 212: loss = 0.182869\n",
      "Iteration 213: loss = 0.182593\n",
      "Iteration 214: loss = 0.182320\n",
      "Iteration 215: loss = 0.182048\n",
      "Iteration 216: loss = 0.181779\n",
      "Iteration 217: loss = 0.181512\n",
      "Iteration 218: loss = 0.181246\n",
      "Iteration 219: loss = 0.180983\n",
      "Iteration 220: loss = 0.180722\n",
      "Iteration 221: loss = 0.180462\n",
      "Iteration 222: loss = 0.180204\n",
      "Iteration 223: loss = 0.179949\n",
      "Iteration 224: loss = 0.179695\n",
      "Iteration 225: loss = 0.179442\n",
      "Iteration 226: loss = 0.179192\n",
      "Iteration 227: loss = 0.178943\n",
      "Iteration 228: loss = 0.178696\n",
      "Iteration 229: loss = 0.178450\n",
      "Iteration 230: loss = 0.178206\n",
      "Iteration 231: loss = 0.177963\n",
      "Iteration 232: loss = 0.177722\n",
      "Iteration 233: loss = 0.177482\n",
      "Iteration 234: loss = 0.177244\n",
      "Iteration 235: loss = 0.177008\n",
      "Iteration 236: loss = 0.176772\n",
      "Iteration 237: loss = 0.176539\n",
      "Iteration 238: loss = 0.176306\n",
      "Iteration 239: loss = 0.176076\n",
      "Iteration 240: loss = 0.175846\n",
      "Iteration 241: loss = 0.175618\n",
      "Iteration 242: loss = 0.175392\n",
      "Iteration 243: loss = 0.175167\n",
      "Iteration 244: loss = 0.174943\n",
      "Iteration 245: loss = 0.174721\n",
      "Iteration 246: loss = 0.174500\n",
      "Iteration 247: loss = 0.174281\n",
      "Iteration 248: loss = 0.174063\n",
      "Iteration 249: loss = 0.173846\n",
      "Iteration 250: loss = 0.173631\n",
      "Iteration 251: loss = 0.173416\n",
      "Iteration 252: loss = 0.173203\n",
      "Iteration 253: loss = 0.172992\n",
      "Iteration 254: loss = 0.172781\n",
      "Iteration 255: loss = 0.172572\n",
      "Iteration 256: loss = 0.172364\n",
      "Iteration 257: loss = 0.172157\n",
      "Iteration 258: loss = 0.171951\n",
      "Iteration 259: loss = 0.171746\n",
      "Iteration 260: loss = 0.171543\n",
      "Iteration 261: loss = 0.171340\n",
      "Iteration 262: loss = 0.171139\n",
      "Iteration 263: loss = 0.170939\n",
      "Iteration 264: loss = 0.170740\n",
      "Iteration 265: loss = 0.170541\n",
      "Iteration 266: loss = 0.170344\n",
      "Iteration 267: loss = 0.170148\n",
      "Iteration 268: loss = 0.169953\n",
      "Iteration 269: loss = 0.169759\n",
      "Iteration 270: loss = 0.169565\n",
      "Iteration 271: loss = 0.169373\n",
      "Iteration 272: loss = 0.169181\n",
      "Iteration 273: loss = 0.168990\n",
      "Iteration 274: loss = 0.168800\n",
      "Iteration 275: loss = 0.168611\n",
      "Iteration 276: loss = 0.168423\n",
      "Iteration 277: loss = 0.168235\n",
      "Iteration 278: loss = 0.168048\n",
      "Iteration 279: loss = 0.167862\n",
      "Iteration 280: loss = 0.167677\n",
      "Iteration 281: loss = 0.167492\n",
      "Iteration 282: loss = 0.167308\n",
      "Iteration 283: loss = 0.167124\n",
      "Iteration 284: loss = 0.166942\n",
      "Iteration 285: loss = 0.166760\n",
      "Iteration 286: loss = 0.166578\n",
      "Iteration 287: loss = 0.166397\n",
      "Iteration 288: loss = 0.166217\n",
      "Iteration 289: loss = 0.166037\n",
      "Iteration 290: loss = 0.165858\n",
      "Iteration 291: loss = 0.165679\n",
      "Iteration 292: loss = 0.165501\n",
      "Iteration 293: loss = 0.165323\n",
      "Iteration 294: loss = 0.165146\n",
      "Iteration 295: loss = 0.164970\n",
      "Iteration 296: loss = 0.164794\n",
      "Iteration 297: loss = 0.164618\n",
      "Iteration 298: loss = 0.164443\n",
      "Iteration 299: loss = 0.164269\n",
      "Iteration 300: loss = 0.164094\n",
      "Iteration 301: loss = 0.163921\n",
      "Iteration 302: loss = 0.163748\n",
      "Iteration 303: loss = 0.163575\n",
      "Iteration 304: loss = 0.163402\n",
      "Iteration 305: loss = 0.163231\n",
      "Iteration 306: loss = 0.163059\n",
      "Iteration 307: loss = 0.162888\n",
      "Iteration 308: loss = 0.162718\n",
      "Iteration 309: loss = 0.162547\n",
      "Iteration 310: loss = 0.162378\n",
      "Iteration 311: loss = 0.162209\n",
      "Iteration 312: loss = 0.162040\n",
      "Iteration 313: loss = 0.161872\n",
      "Iteration 314: loss = 0.161704\n",
      "Iteration 315: loss = 0.161537\n",
      "Iteration 316: loss = 0.161371\n",
      "Iteration 317: loss = 0.161205\n",
      "Iteration 318: loss = 0.161040\n",
      "Iteration 319: loss = 0.160875\n",
      "Iteration 320: loss = 0.160711\n",
      "Iteration 321: loss = 0.160547\n",
      "Iteration 322: loss = 0.160384\n",
      "Iteration 323: loss = 0.160222\n",
      "Iteration 324: loss = 0.160060\n",
      "Iteration 325: loss = 0.159898\n",
      "Iteration 326: loss = 0.159737\n",
      "Iteration 327: loss = 0.159577\n",
      "Iteration 328: loss = 0.159417\n",
      "Iteration 329: loss = 0.159257\n",
      "Iteration 330: loss = 0.159098\n",
      "Iteration 174: loss = 0.192765\n",
      "Iteration 175: loss = 0.192343\n",
      "Iteration 176: loss = 0.191925\n",
      "Iteration 177: loss = 0.191512\n",
      "Iteration 178: loss = 0.191104\n",
      "Iteration 179: loss = 0.190699\n",
      "Iteration 180: loss = 0.190299\n",
      "Iteration 181: loss = 0.189903\n",
      "Iteration 182: loss = 0.189511\n",
      "Iteration 183: loss = 0.189123\n",
      "Iteration 184: loss = 0.188740\n",
      "Iteration 185: loss = 0.188360\n",
      "Iteration 186: loss = 0.187985\n",
      "Iteration 187: loss = 0.187614\n",
      "Iteration 188: loss = 0.187248\n",
      "Iteration 189: loss = 0.186885\n",
      "Iteration 190: loss = 0.186526\n",
      "Iteration 191: loss = 0.186172\n",
      "Iteration 192: loss = 0.185821\n",
      "Iteration 193: loss = 0.185474\n",
      "Iteration 194: loss = 0.185130\n",
      "Iteration 195: loss = 0.184790\n",
      "Iteration 196: loss = 0.184454\n",
      "Iteration 197: loss = 0.184120\n",
      "Iteration 198: loss = 0.183790\n",
      "Iteration 199: loss = 0.183463\n",
      "Iteration 200: loss = 0.183139\n",
      "Iteration 201: loss = 0.182817\n",
      "Iteration 202: loss = 0.182499\n",
      "Iteration 203: loss = 0.182183\n",
      "Iteration 204: loss = 0.181870\n",
      "Iteration 205: loss = 0.181559\n",
      "Iteration 206: loss = 0.181251\n",
      "Iteration 207: loss = 0.180946\n",
      "Iteration 208: loss = 0.180643\n",
      "Iteration 209: loss = 0.180343\n",
      "Iteration 210: loss = 0.180045\n",
      "Iteration 211: loss = 0.179750\n",
      "Iteration 212: loss = 0.179457\n",
      "Iteration 213: loss = 0.179167\n",
      "Iteration 214: loss = 0.178879\n",
      "Iteration 215: loss = 0.178594\n",
      "Iteration 216: loss = 0.178310\n",
      "Iteration 217: loss = 0.178029\n",
      "Iteration 218: loss = 0.177751\n",
      "Iteration 219: loss = 0.177474\n",
      "Iteration 220: loss = 0.177199\n",
      "Iteration 221: loss = 0.176926\n",
      "Iteration 222: loss = 0.176656\n",
      "Iteration 223: loss = 0.176387\n",
      "Iteration 224: loss = 0.176119\n",
      "Iteration 225: loss = 0.175853\n",
      "Iteration 226: loss = 0.175589\n",
      "Iteration 227: loss = 0.175326\n",
      "Iteration 228: loss = 0.175065\n",
      "Iteration 229: loss = 0.174805\n",
      "Iteration 230: loss = 0.174546\n",
      "Iteration 231: loss = 0.174289\n",
      "Iteration 232: loss = 0.174034\n",
      "Iteration 233: loss = 0.173780\n",
      "Iteration 234: loss = 0.173527\n",
      "Iteration 235: loss = 0.173277\n",
      "Iteration 236: loss = 0.173028\n",
      "Iteration 237: loss = 0.172781\n",
      "Iteration 238: loss = 0.172536\n",
      "Iteration 239: loss = 0.172292\n",
      "Iteration 240: loss = 0.172051\n",
      "Iteration 241: loss = 0.171811\n",
      "Iteration 242: loss = 0.171573\n",
      "Iteration 243: loss = 0.171337\n",
      "Iteration 244: loss = 0.171103\n",
      "Iteration 245: loss = 0.170871\n",
      "Iteration 246: loss = 0.170640\n",
      "Iteration 247: loss = 0.170411\n",
      "Iteration 248: loss = 0.170184\n",
      "Iteration 249: loss = 0.169958\n",
      "Iteration 250: loss = 0.169733\n",
      "Iteration 251: loss = 0.169510\n",
      "Iteration 252: loss = 0.169288\n",
      "Iteration 253: loss = 0.169068\n",
      "Iteration 254: loss = 0.168849\n",
      "Iteration 255: loss = 0.168631\n",
      "Iteration 256: loss = 0.168414\n",
      "Iteration 257: loss = 0.168199\n",
      "Iteration 258: loss = 0.167984\n",
      "Iteration 259: loss = 0.167771\n",
      "Iteration 260: loss = 0.167558\n",
      "Iteration 261: loss = 0.167347\n",
      "Iteration 262: loss = 0.167137\n",
      "Iteration 263: loss = 0.166927\n",
      "Iteration 264: loss = 0.166719\n",
      "Iteration 265: loss = 0.166512\n",
      "Iteration 266: loss = 0.166305\n",
      "Iteration 267: loss = 0.166100\n",
      "Iteration 268: loss = 0.165896\n",
      "Iteration 269: loss = 0.165693\n",
      "Iteration 270: loss = 0.165491\n",
      "Iteration 271: loss = 0.165291\n",
      "Iteration 272: loss = 0.165092\n",
      "Iteration 273: loss = 0.164894\n",
      "Iteration 274: loss = 0.164697\n",
      "Iteration 275: loss = 0.164502\n",
      "Iteration 276: loss = 0.164309\n",
      "Iteration 277: loss = 0.164117\n",
      "Iteration 278: loss = 0.163926\n",
      "Iteration 279: loss = 0.163737\n",
      "Iteration 280: loss = 0.163549\n",
      "Iteration 281: loss = 0.163363\n",
      "Iteration 282: loss = 0.163177\n",
      "Iteration 283: loss = 0.162993\n",
      "Iteration 284: loss = 0.162810\n",
      "Iteration 285: loss = 0.162628\n",
      "Iteration 286: loss = 0.162447\n",
      "Iteration 287: loss = 0.162267\n",
      "Iteration 288: loss = 0.162088\n",
      "Iteration 289: loss = 0.161910\n",
      "Iteration 290: loss = 0.161732\n",
      "Iteration 291: loss = 0.161556\n",
      "Iteration 292: loss = 0.161380\n",
      "Iteration 293: loss = 0.161205\n",
      "Iteration 294: loss = 0.161031\n",
      "Iteration 295: loss = 0.160857\n",
      "Iteration 296: loss = 0.160684\n",
      "Iteration 297: loss = 0.160511\n",
      "Iteration 298: loss = 0.160340\n",
      "Iteration 299: loss = 0.160168\n",
      "Iteration 300: loss = 0.159998\n",
      "Iteration 301: loss = 0.159828\n",
      "Iteration 302: loss = 0.159659\n",
      "Iteration 303: loss = 0.159490\n",
      "Iteration 304: loss = 0.159322\n",
      "Iteration 305: loss = 0.159154\n",
      "Iteration 306: loss = 0.158987\n",
      "Iteration 307: loss = 0.158821\n",
      "Iteration 308: loss = 0.158656\n",
      "Iteration 309: loss = 0.158490\n",
      "Iteration 310: loss = 0.158326\n",
      "Iteration 311: loss = 0.158162\n",
      "Iteration 312: loss = 0.157999\n",
      "Iteration 313: loss = 0.157836\n",
      "Iteration 314: loss = 0.157674\n",
      "Iteration 315: loss = 0.157512\n",
      "Iteration 316: loss = 0.157351\n",
      "Iteration 317: loss = 0.157190\n",
      "Iteration 318: loss = 0.157030\n",
      "Iteration 319: loss = 0.156870\n",
      "Iteration 320: loss = 0.156711\n",
      "Iteration 321: loss = 0.156552\n",
      "Iteration 322: loss = 0.156394\n",
      "Iteration 323: loss = 0.156236\n",
      "Iteration 324: loss = 0.156079\n",
      "Iteration 325: loss = 0.155922\n",
      "Iteration 326: loss = 0.155767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 141: loss = 0.217068\n",
      "Iteration 142: loss = 0.216391\n",
      "Iteration 143: loss = 0.215723\n",
      "Iteration 144: loss = 0.215065\n",
      "Iteration 145: loss = 0.214415\n",
      "Iteration 146: loss = 0.213774\n",
      "Iteration 147: loss = 0.213142\n",
      "Iteration 148: loss = 0.212518\n",
      "Iteration 149: loss = 0.211903\n",
      "Iteration 150: loss = 0.211296\n",
      "Iteration 151: loss = 0.210697\n",
      "Iteration 152: loss = 0.210107\n",
      "Iteration 153: loss = 0.209525\n",
      "Iteration 154: loss = 0.208951\n",
      "Iteration 155: loss = 0.208385\n",
      "Iteration 156: loss = 0.207826\n",
      "Iteration 157: loss = 0.207275\n",
      "Iteration 158: loss = 0.206731\n",
      "Iteration 159: loss = 0.206194\n",
      "Iteration 160: loss = 0.205664\n",
      "Iteration 161: loss = 0.205141\n",
      "Iteration 162: loss = 0.204625\n",
      "Iteration 163: loss = 0.204115\n",
      "Iteration 164: loss = 0.203611\n",
      "Iteration 165: loss = 0.203113\n",
      "Iteration 166: loss = 0.202621\n",
      "Iteration 167: loss = 0.202135\n",
      "Iteration 168: loss = 0.201654\n",
      "Iteration 169: loss = 0.201178\n",
      "Iteration 170: loss = 0.200708\n",
      "Iteration 171: loss = 0.200242\n",
      "Iteration 172: loss = 0.199782\n",
      "Iteration 173: loss = 0.199326\n",
      "Iteration 174: loss = 0.198875\n",
      "Iteration 175: loss = 0.198428\n",
      "Iteration 176: loss = 0.197986\n",
      "Iteration 177: loss = 0.197548\n",
      "Iteration 178: loss = 0.197115\n",
      "Iteration 179: loss = 0.196685\n",
      "Iteration 180: loss = 0.196260\n",
      "Iteration 181: loss = 0.195840\n",
      "Iteration 182: loss = 0.195423\n",
      "Iteration 183: loss = 0.195011\n",
      "Iteration 184: loss = 0.194602\n",
      "Iteration 185: loss = 0.194198\n",
      "Iteration 186: loss = 0.193798\n",
      "Iteration 187: loss = 0.193402\n",
      "Iteration 188: loss = 0.193011\n",
      "Iteration 189: loss = 0.192624\n",
      "Iteration 190: loss = 0.192241\n",
      "Iteration 191: loss = 0.191863\n",
      "Iteration 192: loss = 0.191489\n",
      "Iteration 193: loss = 0.191120\n",
      "Iteration 194: loss = 0.190754\n",
      "Iteration 195: loss = 0.190392\n",
      "Iteration 196: loss = 0.190034\n",
      "Iteration 197: loss = 0.189680\n",
      "Iteration 198: loss = 0.189329\n",
      "Iteration 199: loss = 0.188981\n",
      "Iteration 200: loss = 0.188636\n",
      "Iteration 201: loss = 0.188295\n",
      "Iteration 202: loss = 0.187956\n",
      "Iteration 203: loss = 0.187620\n",
      "Iteration 204: loss = 0.187287\n",
      "Iteration 205: loss = 0.186957\n",
      "Iteration 206: loss = 0.186629\n",
      "Iteration 207: loss = 0.186304\n",
      "Iteration 208: loss = 0.185982\n",
      "Iteration 209: loss = 0.185662\n",
      "Iteration 210: loss = 0.185344\n",
      "Iteration 211: loss = 0.185029\n",
      "Iteration 212: loss = 0.184717\n",
      "Iteration 213: loss = 0.184406\n",
      "Iteration 214: loss = 0.184099\n",
      "Iteration 215: loss = 0.183793\n",
      "Iteration 216: loss = 0.183490\n",
      "Iteration 217: loss = 0.183189\n",
      "Iteration 218: loss = 0.182890\n",
      "Iteration 219: loss = 0.182594\n",
      "Iteration 220: loss = 0.182299\n",
      "Iteration 221: loss = 0.182007\n",
      "Iteration 222: loss = 0.181717\n",
      "Iteration 223: loss = 0.181429\n",
      "Iteration 224: loss = 0.181142\n",
      "Iteration 225: loss = 0.180858\n",
      "Iteration 226: loss = 0.180576\n",
      "Iteration 227: loss = 0.180295\n",
      "Iteration 228: loss = 0.180017\n",
      "Iteration 229: loss = 0.179740\n",
      "Iteration 230: loss = 0.179466\n",
      "Iteration 231: loss = 0.179193\n",
      "Iteration 232: loss = 0.178922\n",
      "Iteration 233: loss = 0.178652\n",
      "Iteration 234: loss = 0.178385\n",
      "Iteration 235: loss = 0.178119\n",
      "Iteration 236: loss = 0.177854\n",
      "Iteration 237: loss = 0.177592\n",
      "Iteration 238: loss = 0.177331\n",
      "Iteration 239: loss = 0.177071\n",
      "Iteration 240: loss = 0.176813\n",
      "Iteration 241: loss = 0.176557\n",
      "Iteration 242: loss = 0.176303\n",
      "Iteration 243: loss = 0.176050\n",
      "Iteration 244: loss = 0.175798\n",
      "Iteration 245: loss = 0.175548\n",
      "Iteration 246: loss = 0.175300\n",
      "Iteration 247: loss = 0.175053\n",
      "Iteration 248: loss = 0.174808\n",
      "Iteration 249: loss = 0.174565\n",
      "Iteration 250: loss = 0.174322\n",
      "Iteration 251: loss = 0.174082\n",
      "Iteration 252: loss = 0.173843\n",
      "Iteration 253: loss = 0.173605\n",
      "Iteration 254: loss = 0.173369\n",
      "Iteration 255: loss = 0.173135\n",
      "Iteration 256: loss = 0.172902\n",
      "Iteration 257: loss = 0.172670\n",
      "Iteration 258: loss = 0.172440\n",
      "Iteration 259: loss = 0.172211\n",
      "Iteration 260: loss = 0.171984\n",
      "Iteration 261: loss = 0.171758\n",
      "Iteration 262: loss = 0.171533\n",
      "Iteration 263: loss = 0.171309\n",
      "Iteration 264: loss = 0.171087\n",
      "Iteration 265: loss = 0.170865\n",
      "Iteration 266: loss = 0.170645\n",
      "Iteration 267: loss = 0.170425\n",
      "Iteration 268: loss = 0.170207\n",
      "Iteration 269: loss = 0.169989\n",
      "Iteration 270: loss = 0.169773\n",
      "Iteration 271: loss = 0.169557\n",
      "Iteration 272: loss = 0.169342\n",
      "Iteration 273: loss = 0.169127\n",
      "Iteration 274: loss = 0.168914\n",
      "Iteration 275: loss = 0.168701\n",
      "Iteration 276: loss = 0.168490\n",
      "Iteration 277: loss = 0.168279\n",
      "Iteration 278: loss = 0.168068\n",
      "Iteration 279: loss = 0.167859\n",
      "Iteration 280: loss = 0.167650\n",
      "Iteration 281: loss = 0.167442\n",
      "Iteration 282: loss = 0.167235\n",
      "Iteration 283: loss = 0.167029\n",
      "Iteration 284: loss = 0.166823\n",
      "Iteration 285: loss = 0.166618\n",
      "Iteration 286: loss = 0.166414\n",
      "Iteration 287: loss = 0.166211\n",
      "Iteration 288: loss = 0.166008\n",
      "Iteration 289: loss = 0.165807\n",
      "Iteration 290: loss = 0.165606\n",
      "Iteration 291: loss = 0.165407\n",
      "Iteration 292: loss = 0.165208\n",
      "Iteration 293: loss = 0.165011\n",
      "Iteration 294: loss = 0.164815\n",
      "Iteration 295: loss = 0.164619\n",
      "Iteration 296: loss = 0.164425\n",
      "Iteration 297: loss = 0.164232\n",
      "Iteration 298: loss = 0.164040\n",
      "Iteration 299: loss = 0.163849\n",
      "Iteration 300: loss = 0.163659\n",
      "Iteration 301: loss = 0.163470\n",
      "Iteration 302: loss = 0.163282\n",
      "Iteration 303: loss = 0.163095\n",
      "Iteration 304: loss = 0.162910\n",
      "Iteration 305: loss = 0.162725\n",
      "Iteration 306: loss = 0.162540\n",
      "Iteration 307: loss = 0.162357\n",
      "Iteration 308: loss = 0.162175\n",
      "Iteration 309: loss = 0.161993\n",
      "Iteration 310: loss = 0.161812\n",
      "Iteration 311: loss = 0.161631\n",
      "Iteration 312: loss = 0.161451\n",
      "Iteration 313: loss = 0.161271\n",
      "Iteration 314: loss = 0.161092\n",
      "Iteration 315: loss = 0.160913\n",
      "Iteration 316: loss = 0.160735\n",
      "Iteration 317: loss = 0.160557\n",
      "Iteration 318: loss = 0.160379\n",
      "Iteration 319: loss = 0.160202\n",
      "Iteration 320: loss = 0.160025\n",
      "Iteration 321: loss = 0.159850\n",
      "Iteration 322: loss = 0.159674\n",
      "Iteration 323: loss = 0.159500\n",
      "Iteration 324: loss = 0.159326\n",
      "Iteration 325: loss = 0.159153\n",
      "Iteration 326: loss = 0.158981\n",
      "Iteration 327: loss = 0.158809\n",
      "Iteration 328: loss = 0.158638\n",
      "Iteration 329: loss = 0.158468\n",
      "Iteration 330: loss = 0.158299\n",
      "Iteration 331: loss = 0.158130\n",
      "Iteration 332: loss = 0.157962\n",
      "Iteration 333: loss = 0.157795\n",
      "Iteration 334: loss = 0.157628\n",
      "Iteration 335: loss = 0.157462\n",
      "Iteration 336: loss = 0.157297\n",
      "Iteration 337: loss = 0.157132\n",
      "Iteration 338: loss = 0.156968\n",
      "Iteration 339: loss = 0.156805\n",
      "Iteration 340: loss = 0.156642\n",
      "Iteration 341: loss = 0.156479\n",
      "Iteration 342: loss = 0.156318\n",
      "Iteration 343: loss = 0.156157\n",
      "Iteration 344: loss = 0.155996\n",
      "Iteration 345: loss = 0.155836\n",
      "Iteration 346: loss = 0.155677\n",
      "Iteration 347: loss = 0.155519\n",
      "Iteration 348: loss = 0.155361\n",
      "Iteration 349: loss = 0.155204\n",
      "Iteration 350: loss = 0.155048\n",
      "Iteration 351: loss = 0.154892\n",
      "Iteration 352: loss = 0.154738\n",
      "Iteration 353: loss = 0.154584\n",
      "Iteration 159: loss = 0.145030\n",
      "Iteration 160: loss = 0.144716\n",
      "Iteration 161: loss = 0.144406\n",
      "Iteration 162: loss = 0.144100\n",
      "Iteration 163: loss = 0.143798\n",
      "Iteration 164: loss = 0.143500\n",
      "Iteration 165: loss = 0.143206\n",
      "Iteration 166: loss = 0.142915\n",
      "Iteration 167: loss = 0.142628\n",
      "Iteration 168: loss = 0.142344\n",
      "Iteration 169: loss = 0.142063\n",
      "Iteration 170: loss = 0.141786\n",
      "Iteration 171: loss = 0.141513\n",
      "Iteration 172: loss = 0.141242\n",
      "Iteration 173: loss = 0.140974\n",
      "Iteration 174: loss = 0.140710\n",
      "Iteration 175: loss = 0.140448\n",
      "Iteration 176: loss = 0.140190\n",
      "Iteration 177: loss = 0.139934\n",
      "Iteration 178: loss = 0.139681\n",
      "Iteration 179: loss = 0.139431\n",
      "Iteration 180: loss = 0.139184\n",
      "Iteration 181: loss = 0.138939\n",
      "Iteration 182: loss = 0.138697\n",
      "Iteration 183: loss = 0.138458\n",
      "Iteration 184: loss = 0.138221\n",
      "Iteration 185: loss = 0.137986\n",
      "Iteration 186: loss = 0.137755\n",
      "Iteration 187: loss = 0.137525\n",
      "Iteration 188: loss = 0.137298\n",
      "Iteration 189: loss = 0.137073\n",
      "Iteration 190: loss = 0.136851\n",
      "Iteration 191: loss = 0.136630\n",
      "Iteration 192: loss = 0.136412\n",
      "Iteration 193: loss = 0.136196\n",
      "Iteration 194: loss = 0.135982\n",
      "Iteration 195: loss = 0.135770\n",
      "Iteration 196: loss = 0.135561\n",
      "Iteration 197: loss = 0.135353\n",
      "Iteration 198: loss = 0.135147\n",
      "Iteration 199: loss = 0.134943\n",
      "Iteration 200: loss = 0.134741\n",
      "Iteration 201: loss = 0.134540\n",
      "Iteration 202: loss = 0.134342\n",
      "Iteration 203: loss = 0.134145\n",
      "Iteration 204: loss = 0.133950\n",
      "Iteration 205: loss = 0.133757\n",
      "Iteration 206: loss = 0.133565\n",
      "Iteration 207: loss = 0.133375\n",
      "Iteration 208: loss = 0.133187\n",
      "Iteration 209: loss = 0.133000\n",
      "Iteration 210: loss = 0.132815\n",
      "Iteration 211: loss = 0.132631\n",
      "Iteration 212: loss = 0.132449\n",
      "Iteration 213: loss = 0.132268\n",
      "Iteration 214: loss = 0.132089\n",
      "Iteration 215: loss = 0.131911\n",
      "Iteration 216: loss = 0.131735\n",
      "Iteration 217: loss = 0.131560\n",
      "Iteration 218: loss = 0.131387\n",
      "Iteration 219: loss = 0.131214\n",
      "Iteration 220: loss = 0.131043\n",
      "Iteration 221: loss = 0.130874\n",
      "Iteration 222: loss = 0.130705\n",
      "Iteration 223: loss = 0.130538\n",
      "Iteration 224: loss = 0.130372\n",
      "Iteration 225: loss = 0.130207\n",
      "Iteration 226: loss = 0.130044\n",
      "Iteration 227: loss = 0.129881\n",
      "Iteration 228: loss = 0.129720\n",
      "Iteration 229: loss = 0.129560\n",
      "Iteration 230: loss = 0.129401\n",
      "Iteration 231: loss = 0.129243\n",
      "Iteration 232: loss = 0.129086\n",
      "Iteration 233: loss = 0.128930\n",
      "Iteration 234: loss = 0.128775\n",
      "Iteration 235: loss = 0.128621\n",
      "Iteration 236: loss = 0.128468\n",
      "Iteration 237: loss = 0.128317\n",
      "Iteration 238: loss = 0.128166\n",
      "Iteration 239: loss = 0.128016\n",
      "Iteration 240: loss = 0.127867\n",
      "Iteration 241: loss = 0.127720\n",
      "Iteration 242: loss = 0.127573\n",
      "Iteration 243: loss = 0.127427\n",
      "Iteration 244: loss = 0.127282\n",
      "Iteration 245: loss = 0.127137\n",
      "Iteration 246: loss = 0.126994\n",
      "Iteration 247: loss = 0.126852\n",
      "Iteration 248: loss = 0.126710\n",
      "Iteration 249: loss = 0.126570\n",
      "Iteration 250: loss = 0.126430\n",
      "Iteration 251: loss = 0.126291\n",
      "Iteration 252: loss = 0.126153\n",
      "Iteration 253: loss = 0.126016\n",
      "Iteration 254: loss = 0.125879\n",
      "Iteration 255: loss = 0.125744\n",
      "Iteration 256: loss = 0.125609\n",
      "Iteration 257: loss = 0.125475\n",
      "Iteration 258: loss = 0.125341\n",
      "Iteration 259: loss = 0.125209\n",
      "Iteration 260: loss = 0.125077\n",
      "Iteration 261: loss = 0.124946\n",
      "Iteration 262: loss = 0.124816\n",
      "Iteration 263: loss = 0.124686\n",
      "Iteration 264: loss = 0.124558\n",
      "Iteration 265: loss = 0.124429\n",
      "Iteration 266: loss = 0.124302\n",
      "Iteration 267: loss = 0.124175\n",
      "Iteration 268: loss = 0.124049\n",
      "Iteration 269: loss = 0.123924\n",
      "Iteration 270: loss = 0.123799\n",
      "Iteration 271: loss = 0.123675\n",
      "Iteration 272: loss = 0.123552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 216: loss = 0.136036\n",
      "Iteration 217: loss = 0.135860\n",
      "Iteration 218: loss = 0.135685\n",
      "Iteration 219: loss = 0.135511\n",
      "Iteration 220: loss = 0.135338\n",
      "Iteration 221: loss = 0.135167\n",
      "Iteration 222: loss = 0.134997\n",
      "Iteration 223: loss = 0.134828\n",
      "Iteration 224: loss = 0.134660\n",
      "Iteration 225: loss = 0.134494\n",
      "Iteration 226: loss = 0.134328\n",
      "Iteration 227: loss = 0.134164\n",
      "Iteration 228: loss = 0.134001\n",
      "Iteration 229: loss = 0.133839\n",
      "Iteration 230: loss = 0.133679\n",
      "Iteration 231: loss = 0.133519\n",
      "Iteration 232: loss = 0.133361\n",
      "Iteration 233: loss = 0.133203\n",
      "Iteration 234: loss = 0.133047\n",
      "Iteration 235: loss = 0.132892\n",
      "Iteration 236: loss = 0.132738\n",
      "Iteration 237: loss = 0.132585\n",
      "Iteration 238: loss = 0.132434\n",
      "Iteration 239: loss = 0.132283\n",
      "Iteration 240: loss = 0.132133\n",
      "Iteration 241: loss = 0.131985\n",
      "Iteration 242: loss = 0.131837\n",
      "Iteration 243: loss = 0.131690\n",
      "Iteration 244: loss = 0.131545\n",
      "Iteration 245: loss = 0.131400\n",
      "Iteration 246: loss = 0.131257\n",
      "Iteration 247: loss = 0.131114\n",
      "Iteration 248: loss = 0.130972\n",
      "Iteration 249: loss = 0.130832\n",
      "Iteration 250: loss = 0.130692\n",
      "Iteration 251: loss = 0.130553\n",
      "Iteration 252: loss = 0.130415\n",
      "Iteration 253: loss = 0.130278\n",
      "Iteration 254: loss = 0.130141\n",
      "Iteration 255: loss = 0.130006\n",
      "Iteration 256: loss = 0.129871\n",
      "Iteration 257: loss = 0.129738\n",
      "Iteration 258: loss = 0.129605\n",
      "Iteration 259: loss = 0.129473\n",
      "Iteration 260: loss = 0.129341\n",
      "Iteration 261: loss = 0.129211\n",
      "Iteration 262: loss = 0.129081\n",
      "Iteration 263: loss = 0.128952\n",
      "Iteration 264: loss = 0.128823\n",
      "Iteration 51: loss = 0.278250\n",
      "Iteration 52: loss = 0.274106\n",
      "Iteration 53: loss = 0.270122\n",
      "Iteration 54: loss = 0.266290\n",
      "Iteration 55: loss = 0.262602\n",
      "Iteration 56: loss = 0.259053\n",
      "Iteration 57: loss = 0.255635\n",
      "Iteration 58: loss = 0.252343\n",
      "Iteration 59: loss = 0.249172\n",
      "Iteration 60: loss = 0.246118\n",
      "Iteration 61: loss = 0.243177\n",
      "Iteration 62: loss = 0.240342\n",
      "Iteration 63: loss = 0.237608\n",
      "Iteration 64: loss = 0.234969\n",
      "Iteration 65: loss = 0.232421\n",
      "Iteration 66: loss = 0.229959\n",
      "Iteration 67: loss = 0.227578\n",
      "Iteration 68: loss = 0.225276\n",
      "Iteration 69: loss = 0.223049\n",
      "Iteration 70: loss = 0.220894\n",
      "Iteration 71: loss = 0.218807\n",
      "Iteration 72: loss = 0.216785\n",
      "Iteration 73: loss = 0.214826\n",
      "Iteration 74: loss = 0.212927\n",
      "Iteration 75: loss = 0.211086\n",
      "Iteration 76: loss = 0.209301\n",
      "Iteration 77: loss = 0.207568\n",
      "Iteration 78: loss = 0.205887\n",
      "Iteration 79: loss = 0.204255\n",
      "Iteration 80: loss = 0.202670\n",
      "Iteration 81: loss = 0.201131\n",
      "Iteration 82: loss = 0.199636\n",
      "Iteration 83: loss = 0.198182\n",
      "Iteration 84: loss = 0.196769\n",
      "Iteration 85: loss = 0.195395\n",
      "Iteration 86: loss = 0.194059\n",
      "Iteration 87: loss = 0.192758\n",
      "Iteration 88: loss = 0.191493\n",
      "Iteration 89: loss = 0.190261\n",
      "Iteration 90: loss = 0.189061\n",
      "Iteration 91: loss = 0.187893\n",
      "Iteration 92: loss = 0.186754\n",
      "Iteration 93: loss = 0.185645\n",
      "Iteration 94: loss = 0.184564\n",
      "Iteration 95: loss = 0.183510\n",
      "Iteration 96: loss = 0.182483\n",
      "Iteration 97: loss = 0.181480\n",
      "Iteration 98: loss = 0.180503\n",
      "Iteration 99: loss = 0.179549\n",
      "Iteration 100: loss = 0.178617\n",
      "Iteration 101: loss = 0.177709\n",
      "Iteration 102: loss = 0.176821\n",
      "Iteration 103: loss = 0.175954\n",
      "Iteration 104: loss = 0.175107\n",
      "Iteration 105: loss = 0.174280\n",
      "Iteration 106: loss = 0.173471\n",
      "Iteration 107: loss = 0.172680\n",
      "Iteration 108: loss = 0.171906\n",
      "Iteration 109: loss = 0.171150\n",
      "Iteration 110: loss = 0.170410\n",
      "Iteration 111: loss = 0.169685\n",
      "Iteration 112: loss = 0.168976\n",
      "Iteration 113: loss = 0.168282\n",
      "Iteration 114: loss = 0.167602\n",
      "Iteration 115: loss = 0.166937\n",
      "Iteration 116: loss = 0.166284\n",
      "Iteration 117: loss = 0.165645\n",
      "Iteration 118: loss = 0.165018\n",
      "Iteration 119: loss = 0.164403\n",
      "Iteration 120: loss = 0.163801\n",
      "Iteration 121: loss = 0.163209\n",
      "Iteration 122: loss = 0.162629\n",
      "Iteration 123: loss = 0.162060\n",
      "Iteration 124: loss = 0.161501\n",
      "Iteration 125: loss = 0.160952\n",
      "Iteration 126: loss = 0.160413\n",
      "Iteration 127: loss = 0.159883\n",
      "Iteration 128: loss = 0.159363\n",
      "Iteration 129: loss = 0.158852\n",
      "Iteration 130: loss = 0.158349\n",
      "Iteration 131: loss = 0.157854\n",
      "Iteration 132: loss = 0.157368\n",
      "Iteration 133: loss = 0.156890\n",
      "Iteration 134: loss = 0.156419\n",
      "Iteration 135: loss = 0.155956\n",
      "Iteration 136: loss = 0.155500\n",
      "Iteration 137: loss = 0.155051\n",
      "Iteration 138: loss = 0.154609\n",
      "Iteration 139: loss = 0.154173\n",
      "Iteration 140: loss = 0.153744\n",
      "Iteration 141: loss = 0.153321\n",
      "Iteration 142: loss = 0.152903\n",
      "Iteration 143: loss = 0.152492\n",
      "Iteration 144: loss = 0.152086\n",
      "Iteration 145: loss = 0.151686\n",
      "Iteration 146: loss = 0.151292\n",
      "Iteration 147: loss = 0.150902\n",
      "Iteration 148: loss = 0.150518\n",
      "Iteration 149: loss = 0.150139\n",
      "Iteration 150: loss = 0.149765\n",
      "Iteration 151: loss = 0.149396\n",
      "Iteration 152: loss = 0.149032\n",
      "Iteration 153: loss = 0.148672\n",
      "Iteration 154: loss = 0.148317\n",
      "Iteration 155: loss = 0.147967\n",
      "Iteration 156: loss = 0.147620\n",
      "Iteration 157: loss = 0.147279\n",
      "Iteration 158: loss = 0.146941\n",
      "Iteration 159: loss = 0.146607\n",
      "Iteration 160: loss = 0.146278\n",
      "Iteration 161: loss = 0.145953\n",
      "Iteration 162: loss = 0.145631\n",
      "Iteration 163: loss = 0.145314\n",
      "Iteration 164: loss = 0.145000\n",
      "Iteration 165: loss = 0.144690\n",
      "Iteration 166: loss = 0.144383\n",
      "Iteration 167: loss = 0.144081\n",
      "Iteration 168: loss = 0.143781\n",
      "Iteration 169: loss = 0.143486\n",
      "Iteration 170: loss = 0.143194\n",
      "Iteration 171: loss = 0.142905\n",
      "Iteration 172: loss = 0.142619\n",
      "Iteration 173: loss = 0.142337\n",
      "Iteration 174: loss = 0.142058\n",
      "Iteration 175: loss = 0.141782\n",
      "Iteration 176: loss = 0.141509\n",
      "Iteration 177: loss = 0.141240\n",
      "Iteration 178: loss = 0.140973\n",
      "Iteration 179: loss = 0.140709\n",
      "Iteration 180: loss = 0.140448\n",
      "Iteration 181: loss = 0.140190\n",
      "Iteration 182: loss = 0.139935\n",
      "Iteration 183: loss = 0.139682\n",
      "Iteration 184: loss = 0.139432\n",
      "Iteration 185: loss = 0.139185\n",
      "Iteration 186: loss = 0.138940\n",
      "Iteration 187: loss = 0.138698\n",
      "Iteration 188: loss = 0.138458\n",
      "Iteration 189: loss = 0.138220\n",
      "Iteration 190: loss = 0.137985\n",
      "Iteration 191: loss = 0.137752\n",
      "Iteration 192: loss = 0.137522\n",
      "Iteration 193: loss = 0.137293\n",
      "Iteration 194: loss = 0.137067\n",
      "Iteration 195: loss = 0.136843\n",
      "Iteration 196: loss = 0.136621\n",
      "Iteration 197: loss = 0.136400\n",
      "Iteration 198: loss = 0.136182\n",
      "Iteration 199: loss = 0.135966\n",
      "Iteration 200: loss = 0.135752\n",
      "Iteration 201: loss = 0.135540\n",
      "Iteration 202: loss = 0.135329\n",
      "Iteration 203: loss = 0.135120\n",
      "Iteration 204: loss = 0.134913\n",
      "Iteration 205: loss = 0.134708\n",
      "Iteration 206: loss = 0.134505\n",
      "Iteration 207: loss = 0.134303\n",
      "Iteration 208: loss = 0.134103\n",
      "Iteration 209: loss = 0.133905\n",
      "Iteration 210: loss = 0.133708\n",
      "Iteration 211: loss = 0.133513\n",
      "Iteration 212: loss = 0.133320\n",
      "Iteration 213: loss = 0.133128\n",
      "Iteration 214: loss = 0.132938\n",
      "Iteration 215: loss = 0.132750\n",
      "Iteration 216: loss = 0.132563\n",
      "Iteration 217: loss = 0.132378\n",
      "Iteration 218: loss = 0.132194\n",
      "Iteration 219: loss = 0.132012\n",
      "Iteration 220: loss = 0.131831\n",
      "Iteration 221: loss = 0.131652\n",
      "Iteration 222: loss = 0.131474\n",
      "Iteration 223: loss = 0.131297\n",
      "Iteration 224: loss = 0.131122\n",
      "Iteration 225: loss = 0.130949\n",
      "Iteration 226: loss = 0.130777\n",
      "Iteration 227: loss = 0.130606\n",
      "Iteration 228: loss = 0.130436\n",
      "Iteration 229: loss = 0.130268\n",
      "Iteration 230: loss = 0.130101\n",
      "Iteration 231: loss = 0.129935\n",
      "Iteration 232: loss = 0.129771\n",
      "Iteration 233: loss = 0.129608\n",
      "Iteration 234: loss = 0.129446\n",
      "Iteration 235: loss = 0.129285\n",
      "Iteration 236: loss = 0.129125\n",
      "Iteration 237: loss = 0.128967\n",
      "Iteration 238: loss = 0.128809\n",
      "Iteration 239: loss = 0.128653\n",
      "Iteration 240: loss = 0.128498\n",
      "Iteration 241: loss = 0.128343\n",
      "Iteration 242: loss = 0.128190\n",
      "Iteration 243: loss = 0.128038\n",
      "Iteration 244: loss = 0.127887\n",
      "Iteration 245: loss = 0.127737\n",
      "Iteration 246: loss = 0.127588\n",
      "Iteration 247: loss = 0.127439\n",
      "Iteration 248: loss = 0.127292\n",
      "Iteration 249: loss = 0.127146\n",
      "Iteration 250: loss = 0.127001\n",
      "Iteration 251: loss = 0.126856\n",
      "Iteration 252: loss = 0.126712\n",
      "Iteration 253: loss = 0.126570\n",
      "Iteration 254: loss = 0.126428\n",
      "Iteration 255: loss = 0.126287\n",
      "Iteration 256: loss = 0.126146\n",
      "Iteration 257: loss = 0.126007\n",
      "Iteration 258: loss = 0.125868\n",
      "Iteration 259: loss = 0.125730\n",
      "Iteration 260: loss = 0.125593\n",
      "Iteration 261: loss = 0.125457\n",
      "Iteration 262: loss = 0.125321\n",
      "Iteration 263: loss = 0.125186\n",
      "Iteration 264: loss = 0.125052\n",
      "Iteration 265: loss = 0.124919\n",
      "Iteration 266: loss = 0.124786\n",
      "Iteration 267: loss = 0.124654\n",
      "Iteration 268: loss = 0.124523\n",
      "Iteration 269: loss = 0.124392\n",
      "Iteration 270: loss = 0.124262\n",
      "Iteration 271: loss = 0.124132\n",
      "Iteration 272: loss = 0.124003\n",
      "Iteration 273: loss = 0.123875\n",
      "Iteration 274: loss = 0.123747\n",
      "Iteration 275: loss = 0.123619\n",
      "Iteration 276: loss = 0.123492\n",
      "Iteration 277: loss = 0.123366\n",
      "Iteration 278: loss = 0.123240\n",
      "Iteration 279: loss = 0.123115\n",
      "Iteration 280: loss = 0.122990\n",
      "Iteration 281: loss = 0.122866\n",
      "Iteration 282: loss = 0.122743\n",
      "Iteration 283: loss = 0.122620\n",
      "Iteration 284: loss = 0.122498\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a709d846",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
