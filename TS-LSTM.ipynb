{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#pip install tensorflow==2.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge cudatoolkit=11.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 10:39:57.165979: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Flow Version: 2.10.0\n",
      "Keras Version: 2.10.0\n",
      "\n",
      "Python 3.9.6 (v3.9.6:db3ff76da1, Jun 28 2021, 11:49:53) \n",
      "[Clang 6.0 (clang-600.0.57)]\n",
      "Pandas 1.5.0\n",
      "Scikit-Learn 1.1.2\n",
      "GPU is NOT AVAILABLE\n"
     ]
    }
   ],
   "source": [
    "# What version of Python do you have?\n",
    "import sys\n",
    "\n",
    "import tensorflow.keras\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tensorflow.keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1670796275067,
     "user": {
      "displayName": "Mojtaba Askarzadeh Farahani",
      "userId": "02005001605152753874"
     },
     "user_tz": 300
    },
    "id": "WI3EYR_be7ov"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "import sklearn.preprocessing\n",
    "\n",
    "import time\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from joblib import parallel_backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1670796275068,
     "user": {
      "displayName": "Mojtaba Askarzadeh Farahani",
      "userId": "02005001605152753874"
     },
     "user_tz": 300
    },
    "id": "nYPPgDj2e7oy",
    "outputId": "c71b2a21-7bf6-46ac-c958-794ccd9e5531"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53785, 749, 3)\n",
      "(53785, 749, 1)\n"
     ]
    }
   ],
   "source": [
    "# define a list of datasets\n",
    "datasets = [\"PHM2022_Multivar\", \"PHM2022_Univar_PDIN\"]\n",
    "datasets_path = \"../datasets\"\n",
    "\n",
    "for dataset in datasets:\n",
    "    Dataset_name = dataset + \"_Dataset\"\n",
    "    Dataset = np.load(datasets_path + \"/\" + Dataset_name + \".npy\")\n",
    "    print(Dataset.shape)\n",
    "    \n",
    "\n",
    "    Labels_name = dataset + \"_Labels\"\n",
    "    Labels = np.load(datasets_path + \"/\"  + Labels_name + \".npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The dataset shape is:(53785, 749, 3)\n",
      "\n",
      " The number of data samples (N) is:53785\n",
      "\n",
      " The number of TS length (T) is:749\n",
      "\n",
      " The number of TS dimention (M) is:3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 10:40:10.103886: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 749, 128)\n",
      "(None, 64)\n",
      "(64,)\n",
      "(64, 1)\n",
      "(64, 64)\n",
      "apply the attention mechanism\n",
      "(64, 64)\n",
      "(64,)\n",
      "(64, 1)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 749, 3)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 749, 128)     67584       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 64)           49408       ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " attention (attention)          (64,)                128         ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " tf.expand_dims (TFOpLambda)    (64, 1)              0           ['attention[0][0]']              \n",
      "                                                                                                  \n",
      " context_vector (Multiply)      (64, 64)             0           ['lstm_1[0][0]',                 \n",
      "                                                                  'tf.expand_dims[0][0]']         \n",
      "                                                                                                  \n",
      " context_vector_sum (Lambda)    (64,)                0           ['context_vector[0][0]']         \n",
      "                                                                                                  \n",
      " tf.expand_dims_1 (TFOpLambda)  (64, 1)              0           ['context_vector_sum[0][0]']     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (64, 128)            256         ['tf.expand_dims_1[0][0]']       \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (64, 12)             1548        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 118,924\n",
      "Trainable params: 118,924\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/500\n",
      "168/673 [======>.......................] - ETA: 12:23 - loss: 2.0431 - accuracy: 0.3549"
     ]
    }
   ],
   "source": [
    "# change this directory for your machine\n",
    "root_dir = './'\n",
    "\n",
    "# define the number of folds\n",
    "n_folds = 5\n",
    "\n",
    "# perform cross-validation for each dataset and algorithm combination\n",
    "for dataset in datasets:\n",
    "    Dataset_name = dataset + \"_Dataset\"\n",
    "    Dataset = np.load(datasets_path + \"/\" + Dataset_name + \".npy\")\n",
    "    \n",
    "\n",
    "    Labels_name = dataset + \"_Labels\"\n",
    "    Labels = np.load(datasets_path + \"/\"  + Labels_name + \".npy\")\n",
    "    \n",
    "\n",
    "\n",
    "    # Create a folder for results\n",
    "    results_path = root_dir + \"Results/\" + Dataset_name\n",
    "    if os.path.exists(results_path):\n",
    "        pass\n",
    "    else:\n",
    "        try:\n",
    "            os.makedirs(results_path)\n",
    "        except:\n",
    "            # in case another machine created the path meanwhile !:(\n",
    "            pass\n",
    "\n",
    "        \n",
    "    t_total = time.time() ##Start timing\n",
    "    \n",
    "\n",
    "\n",
    "    print(f\"\\n The dataset shape is:{Dataset.shape}\")\n",
    "    print(f\"\\n The number of data samples (N) is:{Dataset.shape[0]}\")\n",
    "    print(f\"\\n The number of TS length (T) is:{Dataset.shape[1]}\")\n",
    "    print(f\"\\n The number of TS dimention (M) is:{Dataset.shape[2]}\")\n",
    "\n",
    "    nb_classes = len(np.unique(Labels, axis=0))\n",
    "    \n",
    "    # transform the labels from integers to one hot vectors\n",
    "    enc = sklearn.preprocessing.OneHotEncoder(categories='auto')\n",
    "    Labels = enc.fit_transform(Labels.reshape(-1, 1)).toarray()\n",
    "\n",
    "\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True)\n",
    "    accuracy_scores = []\n",
    "    f1_scores = []\n",
    "    confusion_matrices = []\n",
    "    report_list = []\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(Dataset)):\n",
    "        # split the data into training and testing sets\n",
    "        X_train, X_test = Dataset[train_idx], Dataset[test_idx]\n",
    "        y_train, y_test = Labels[train_idx], Labels[test_idx]\n",
    "        \n",
    "        # save orignal y because later we will use binary\n",
    "        y_true = np.argmax(y_test, axis=1)\n",
    "        \n",
    "        input_shape = X_train.shape[1:]\n",
    "        \n",
    "        ## Create Classification module\n",
    "        from dl4tsc.classifiers import TSLSTM\n",
    "        classifier = TSLSTM.Classifier_TSLSTM(output_directory=results_path, input_shape=input_shape,\n",
    "                                        nb_classes=nb_classes, verbose=True,\n",
    "                                        lr=0.001, batch_size=64, epoch = 500)\n",
    "        \n",
    "        # fit the algorithm on the training data\n",
    "        classifier.fit(X_train, y_train, X_test, y_test, y_true)\n",
    "            \n",
    "        # calculate the evaluation metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(accuracy)\n",
    "\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        print(f1)\n",
    "\n",
    "        confusion = confusion_matrix(y_test, y_pred)\n",
    "        print(confusion)\n",
    "\n",
    "        accuracy_scores.append(accuracy)\n",
    "        f1_scores.append(f1)\n",
    "        confusion_matrices.append(confusion)\n",
    "\n",
    "        report = classification_report(y_test, y_pred, zero_division=1)\n",
    "        report_list.append(report)\n",
    "        print(report)\n",
    "        \n",
    "        print(f\" fold {fold+1} is Finished!\")\n",
    "        \n",
    "        # save the output to a text file\n",
    "        with open(f'{results_path}/dataset_{dataset_name}_BiLSTM_fold_{fold+1}.txt', 'w') as f:\n",
    "            f.write(f'Accuracy: {accuracy}\\n')\n",
    "            f.write(f'F1 Score: {f1}\\n')\n",
    "            f.write(f'Confusion Matrix:\\n{confusion}\\n\\n')\n",
    "            f.write(f'Classification report:\\n{report}\\n\\n')\n",
    "        \n",
    "    with open(f'{results_path}/dataset_{dataset_name}_BiLSTM.txt', 'w') as f:\n",
    "        f.write(\"Mean accuracy: {:.3f} (std={:.3f})\\n\".format(np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "        f.write(\"Mean F1 score: {:.3f} (std={:.3f})\\n\".format(np.mean(f1_scores), np.std(f1_scores)))\n",
    "        f.write(\"Mean confusion matrix:\\n{}\\n\".format(np.array2string(np.mean(confusion_matrices, axis=0))))\n",
    "        f.write(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "    print(\" Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1670797391439,
     "user": {
      "displayName": "Mojtaba Askarzadeh Farahani",
      "userId": "02005001605152753874"
     },
     "user_tz": 300
    },
    "id": "o0hfsmobe7o6"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, duration, y_true_val=None, y_pred_val=None):\n",
    "    res = pd.DataFrame(data=np.zeros((1, 4), dtype=np.float), index=[0],\n",
    "                       columns=['precision', 'accuracy', 'recall', 'duration'])\n",
    "    res['precision'] = precision_score(y_true, y_pred, average='macro')\n",
    "    res['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    if not y_true_val is None:\n",
    "        # this is useful when transfer learning is used with cross validation\n",
    "        res['accuracy_val'] = accuracy_score(y_true_val, y_pred_val)\n",
    "\n",
    "    res['recall'] = recall_score(y_true, y_pred, average='macro')\n",
    "    res['duration'] = duration\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = calculate_metrics(y_true, y_pred, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 99
    },
    "executionInfo": {
     "elapsed": 1221,
     "status": "ok",
     "timestamp": 1670797392656,
     "user": {
      "displayName": "Mojtaba Askarzadeh Farahani",
      "userId": "02005001605152753874"
     },
     "user_tz": 300
    },
    "id": "BIeCbOs6e7o6",
    "outputId": "973650ee-67f0-43ae-cf19-3b4bf69641fc"
   },
   "outputs": [],
   "source": [
    "classifier.predict(X_test, y_true,X_train,y_train,y_test,return_df_metrics = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "f = h5py.File('./Results/PHM2022_Multivar_Datasetbest_model.hdf5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
