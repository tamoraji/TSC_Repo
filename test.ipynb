{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "\n",
    "# LR model \n",
    "import sklearn\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53785, 749, 3)\n",
      "(53785, 749, 1)\n"
     ]
    }
   ],
   "source": [
    "# define a list of datasets\n",
    "datasets = [\"PHM2022_Multivar\", \"PHM2022_Univar_PDIN\"]\n",
    "datasets_path = \"./datasets\"\n",
    "\n",
    "for dataset in datasets:\n",
    "    Dataset_name = dataset + \"_Dataset\"\n",
    "    Dataset = np.load(datasets_path + \"/\" + Dataset_name + \".npy\")\n",
    "    print(Dataset.shape)\n",
    "    \n",
    "\n",
    "    Labels_name = dataset + \"_Labels\"\n",
    "    Labels = np.load(datasets_path + \"/\"  + Labels_name + \".npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The dataset shape is:(53785, 749, 3)\n",
      "\n",
      " The number of data samples (N) is:53785\n",
      "\n",
      " The number of TS length (T) is:749\n",
      "\n",
      " The number of TS dimention (M) is:3\n",
      "LR is not capable of doing classification on MTS. so it will be done on only the first dimension\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9263735242167891\n",
      "0.9259665397860946\n",
      "[[3732   14   20   25    7   23   17    9   23   21   11   15]\n",
      " [  53  548    0    9    0   34    0    4    2    1    6   18]\n",
      " [  35    0  657    0    0    0    0    0    0    0    0    0]\n",
      " [  43   11    0  547    0    4    0    2    1    0   14    8]\n",
      " [   6    0    0    0  624    0    0    2    1    0    0    2]\n",
      " [   8   17    0    3    0  560    0    0    0    0    0    9]\n",
      " [  16    0    0    1    0    0  539    1    0    0    0    0]\n",
      " [  14    0    0    3    0    0    0  586   17    1    0    0]\n",
      " [  26    1    0    1    0    3    0   16  517    1    3    6]\n",
      " [  11    0    0    1    2    4    1    0    0  625    3    1]\n",
      " [  48   16    0    6    0    1    0    0    0    0  451   26]\n",
      " [  38   14    0    5    0    5    0    0    1    1   20  579]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94      3917\n",
      "           1       0.88      0.81      0.85       675\n",
      "           2       0.97      0.95      0.96       692\n",
      "           3       0.91      0.87      0.89       630\n",
      "           4       0.99      0.98      0.98       635\n",
      "           5       0.88      0.94      0.91       597\n",
      "           6       0.97      0.97      0.97       557\n",
      "           7       0.95      0.94      0.94       621\n",
      "           8       0.92      0.90      0.91       574\n",
      "           9       0.96      0.96      0.96       648\n",
      "          10       0.89      0.82      0.85       548\n",
      "          11       0.87      0.87      0.87       663\n",
      "\n",
      "    accuracy                           0.93     10757\n",
      "   macro avg       0.93      0.91      0.92     10757\n",
      "weighted avg       0.93      0.93      0.93     10757\n",
      "\n",
      " fold 1 is Finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9236776052802826\n",
      "0.9236305546299287\n",
      "[[3561   20   20   34    6   33   19   14   27   14   30   28]\n",
      " [  38  556    0    3    0   19    0    3    0    0   10   11]\n",
      " [  20    0  673    0    1    0    0    0    0    1    0    0]\n",
      " [  39    9    0  552    1    0    0    2    2    1   12   11]\n",
      " [   5    0    0    0  616    0    0    1    0    1    0    0]\n",
      " [   4   32    0    5    0  573    0    0    0    1    0   11]\n",
      " [  13    0    0    0    0    0  612    0    0    1    0    0]\n",
      " [  18    0    0    5    0    0    0  603   15    0    0    0]\n",
      " [  45    0    0    1    0    0    0   18  571    2    3    3]\n",
      " [  21    2    0    3    0    1    0    5    1  583    6    2]\n",
      " [  43    5    0    4    0    3    0    0    2    0  471   27]\n",
      " [  42   14    0    7    0    4    0    0    1    0   16  565]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93      3806\n",
      "           1       0.87      0.87      0.87       640\n",
      "           2       0.97      0.97      0.97       695\n",
      "           3       0.90      0.88      0.89       629\n",
      "           4       0.99      0.99      0.99       623\n",
      "           5       0.91      0.92      0.91       626\n",
      "           6       0.97      0.98      0.97       626\n",
      "           7       0.93      0.94      0.94       641\n",
      "           8       0.92      0.89      0.90       643\n",
      "           9       0.97      0.93      0.95       624\n",
      "          10       0.86      0.85      0.85       555\n",
      "          11       0.86      0.87      0.86       649\n",
      "\n",
      "    accuracy                           0.92     10757\n",
      "   macro avg       0.92      0.92      0.92     10757\n",
      "weighted avg       0.92      0.92      0.92     10757\n",
      "\n",
      " fold 2 is Finished!\n",
      "0.9264664869387376\n",
      "0.9263238557002657\n",
      "[[3714   18   31   28    2   18   18   16   19   17   12   26]\n",
      " [  39  543    0    7    0   24    0    3    1    0   12   21]\n",
      " [  20    0  679    0    1    0    0    0    0    0    0    0]\n",
      " [  35   14    0  532    0    3    0    1    1    0    9    5]\n",
      " [   8    0    0    0  605    0    0    2    1    1    0    0]\n",
      " [   9   38    0    4    0  543    0    0    0    2    1    8]\n",
      " [  19    0    0    0    1    0  585    0    0    0    0    0]\n",
      " [  13    1    0    2    0    0    1  591   12    0    1    0]\n",
      " [  34    1    0    1    0    0    0   26  584    1    2    1]\n",
      " [  24    0    0    4    1    1    2    1    2  567    3    2]\n",
      " [  32   10    0    5    0    2    0    2    2    1  450   23]\n",
      " [  27   11    0    7    0    6    0    2    3    1   26  573]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94      3919\n",
      "           1       0.85      0.84      0.84       650\n",
      "           2       0.96      0.97      0.96       700\n",
      "           3       0.90      0.89      0.89       600\n",
      "           4       0.99      0.98      0.99       617\n",
      "           5       0.91      0.90      0.90       605\n",
      "           6       0.97      0.97      0.97       605\n",
      "           7       0.92      0.95      0.93       621\n",
      "           8       0.93      0.90      0.92       650\n",
      "           9       0.96      0.93      0.95       607\n",
      "          10       0.87      0.85      0.86       527\n",
      "          11       0.87      0.87      0.87       656\n",
      "\n",
      "    accuracy                           0.93     10757\n",
      "   macro avg       0.92      0.92      0.92     10757\n",
      "weighted avg       0.93      0.93      0.93     10757\n",
      "\n",
      " fold 3 is Finished!\n",
      "0.929162405875244\n",
      "0.928946296908831\n",
      "[[3736   22   23   21   10   30   20   25   20   26   15   21]\n",
      " [  21  533    0    4    1   29    0    1    3    1    5   16]\n",
      " [  22    0  689    0    1    0    0    0    0    0    0    0]\n",
      " [  41    6    0  550    0    1    0    1    1    0   10    2]\n",
      " [   3    0    0    0  598    0    0    1    1    0    0    1]\n",
      " [   5   25    0    2    0  557    0    1    1    1    2    9]\n",
      " [  11    0    0    1    0    0  619    0    0    0    0    0]\n",
      " [  16    0    0    0    0    0    0  583   15    0    0    0]\n",
      " [  32    0    0    4    0    1    0   18  586    3    0    1]\n",
      " [  18    0    0    2    0    1    0    4    0  538    0    0]\n",
      " [  48   18    0    2    0    2    0    1    3    0  454   24]\n",
      " [  38   14    0    4    0    9    0    1    3    0   17  552]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94      3969\n",
      "           1       0.86      0.87      0.87       614\n",
      "           2       0.97      0.97      0.97       712\n",
      "           3       0.93      0.90      0.92       612\n",
      "           4       0.98      0.99      0.99       604\n",
      "           5       0.88      0.92      0.90       603\n",
      "           6       0.97      0.98      0.97       631\n",
      "           7       0.92      0.95      0.93       614\n",
      "           8       0.93      0.91      0.92       645\n",
      "           9       0.95      0.96      0.95       563\n",
      "          10       0.90      0.82      0.86       552\n",
      "          11       0.88      0.87      0.87       638\n",
      "\n",
      "    accuracy                           0.93     10757\n",
      "   macro avg       0.93      0.92      0.92     10757\n",
      "weighted avg       0.93      0.93      0.93     10757\n",
      "\n",
      " fold 4 is Finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9257227851631495\n",
      "0.9255070112781602\n",
      "[[3735   15   27   27   11   28   20   20   25   19   22   20]\n",
      " [  32  520    0    7    0   20    0    3    1    0    7   14]\n",
      " [  21    0  671    0    1    0    0    0    0    0    0    0]\n",
      " [  48    9    0  513    0    0    0    4    3    0   13    9]\n",
      " [   5    0    0    0  614    0    0    2    0    0    0    1]\n",
      " [  10   27    0    8    0  578    0    0    1    1    4   14]\n",
      " [  14    0    0    0    0    0  623    0    0    2    0    0]\n",
      " [  16    0    0    0    0    0    0  626   14    0    1    0]\n",
      " [  30    2    0    1    0    0    0   22  543    2    2    5]\n",
      " [  23    1    0    1    0    0    0    0    0  588    0    2]\n",
      " [  38   14    0    4    0    1    0    1    2    1  422   26]\n",
      " [  45   10    0    3    0    3    0    0    2    0   12  525]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94      3969\n",
      "           1       0.87      0.86      0.87       604\n",
      "           2       0.96      0.97      0.96       693\n",
      "           3       0.91      0.86      0.88       599\n",
      "           4       0.98      0.99      0.98       622\n",
      "           5       0.92      0.90      0.91       643\n",
      "           6       0.97      0.97      0.97       639\n",
      "           7       0.92      0.95      0.94       657\n",
      "           8       0.92      0.89      0.91       607\n",
      "           9       0.96      0.96      0.96       615\n",
      "          10       0.87      0.83      0.85       509\n",
      "          11       0.85      0.88      0.86       600\n",
      "\n",
      "    accuracy                           0.93     10757\n",
      "   macro avg       0.92      0.92      0.92     10757\n",
      "weighted avg       0.93      0.93      0.93     10757\n",
      "\n",
      " fold 5 is Finished!\n",
      " Finished!\n",
      "Total time elapsed: 662.5753s\n",
      "\n",
      " The dataset shape is:(53785, 749, 3)\n",
      "\n",
      " The number of data samples (N) is:53785\n",
      "\n",
      " The number of TS length (T) is:749\n",
      "\n",
      " The number of TS dimention (M) is:3\n",
      "SVM is not capable of doing classification on MTS. so it will be done on only the first dimension\n",
      "0.9292553685971925\n",
      "0.9285249733100926\n",
      "[[3913    0   15    0    1    3   59    1    5    6    4    7]\n",
      " [  69  513    0    2    0   30    0    0    0    0    1   26]\n",
      " [  35    0  659    0    0    0    0    0    0    0    0    0]\n",
      " [  59    4    0  489    0    0    0    0    0    0    6   11]\n",
      " [   3    0    0    0  633    0    0    0    0    0    0    0]\n",
      " [  43   34    0    2    0  508    0    0    0    0    0   20]\n",
      " [  11    0    0    0    0    0  581    0    0    0    0    0]\n",
      " [  37    0    0    0    0    0    0  567   17    0    0    0]\n",
      " [  34    0    0    1    0    0    0   42  533    0    0    0]\n",
      " [   2    1    0    0    0    0    0    0    0  605    0    0]\n",
      " [  50    3    0    7    0    0    0    0    0    0  472   18]\n",
      " [  68    7    0    1    0   12    0    0    0    0    4  523]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.97      0.94      4014\n",
      "           1       0.91      0.80      0.85       641\n",
      "           2       0.98      0.95      0.96       694\n",
      "           3       0.97      0.86      0.91       569\n",
      "           4       1.00      1.00      1.00       636\n",
      "           5       0.92      0.84      0.88       607\n",
      "           6       0.91      0.98      0.94       592\n",
      "           7       0.93      0.91      0.92       621\n",
      "           8       0.96      0.87      0.92       610\n",
      "           9       0.99      1.00      0.99       608\n",
      "          10       0.97      0.86      0.91       550\n",
      "          11       0.86      0.85      0.86       615\n",
      "\n",
      "    accuracy                           0.93     10757\n",
      "   macro avg       0.94      0.91      0.92     10757\n",
      "weighted avg       0.93      0.93      0.93     10757\n",
      "\n",
      " fold 1 is Finished!\n",
      "0.932137212977596\n",
      "0.9315668913189997\n",
      "[[3777    0    5    1    2    1   53    4    5    3    1    4]\n",
      " [  64  529    0    2    0   19    0    0    0    0    0   24]\n",
      " [  39    0  663    0    0    0    0    0    0    0    0    0]\n",
      " [  57    7    0  543    0    0    0    0    0    0   14   13]\n",
      " [   2    0    0    0  631    0    0    0    0    0    0    0]\n",
      " [  40   37    0    2    0  524    0    0    0    0    0    8]\n",
      " [  22    0    0    0    0    0  602    0    0    0    0    0]\n",
      " [  39    0    0    0    0    0    0  579   15    0    0    0]\n",
      " [  33    0    0    0    0    0    0   33  587    0    0    0]\n",
      " [   4    0    0    0    0    0    0    0    0  594    0    0]\n",
      " [  64    6    0    6    0    0    0    0    0    0  429   25]\n",
      " [  62    6    0    1    0    7    0    0    0    0    0  569]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.94      3856\n",
      "           1       0.90      0.83      0.87       638\n",
      "           2       0.99      0.94      0.97       702\n",
      "           3       0.98      0.86      0.91       634\n",
      "           4       1.00      1.00      1.00       633\n",
      "           5       0.95      0.86      0.90       611\n",
      "           6       0.92      0.96      0.94       624\n",
      "           7       0.94      0.91      0.93       633\n",
      "           8       0.97      0.90      0.93       653\n",
      "           9       0.99      0.99      0.99       598\n",
      "          10       0.97      0.81      0.88       530\n",
      "          11       0.88      0.88      0.88       645\n",
      "\n",
      "    accuracy                           0.93     10757\n",
      "   macro avg       0.95      0.91      0.93     10757\n",
      "weighted avg       0.93      0.93      0.93     10757\n",
      "\n",
      " fold 2 is Finished!\n",
      "0.9339035046946175\n",
      "0.9333358502932386\n",
      "[[3785    0   11    2    1    3   73    4    2    1    2    2]\n",
      " [  60  552    0    3    0   29    0    0    0    0    2   20]\n",
      " [  37    0  647    0    0    0    0    0    0    0    0    0]\n",
      " [  55    3    0  539    0    0    0    0    1    0    9    7]\n",
      " [   1    0    0    0  626    0    0    0    0    0    0    0]\n",
      " [  44   48    0    0    0  507    0    0    0    0    0   10]\n",
      " [   7    0    0    0    0    0  624    0    0    0    0    0]\n",
      " [  30    0    0    0    0    0    0  582   13    0    0    0]\n",
      " [  39    0    0    1    0    0    0   37  529    0    0    0]\n",
      " [   4    0    0    0    0    1    0    0    0  643    0    0]\n",
      " [  49    1    0    9    0    0    0    0    0    0  440   17]\n",
      " [  63    2    0    2    0    6    0    0    0    0    0  572]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94      3886\n",
      "           1       0.91      0.83      0.87       666\n",
      "           2       0.98      0.95      0.96       684\n",
      "           3       0.97      0.88      0.92       614\n",
      "           4       1.00      1.00      1.00       627\n",
      "           5       0.93      0.83      0.88       609\n",
      "           6       0.90      0.99      0.94       631\n",
      "           7       0.93      0.93      0.93       625\n",
      "           8       0.97      0.87      0.92       606\n",
      "           9       1.00      0.99      1.00       648\n",
      "          10       0.97      0.85      0.91       516\n",
      "          11       0.91      0.89      0.90       645\n",
      "\n",
      "    accuracy                           0.93     10757\n",
      "   macro avg       0.95      0.92      0.93     10757\n",
      "weighted avg       0.94      0.93      0.93     10757\n",
      "\n",
      " fold 3 is Finished!\n",
      "0.9318583248117505\n",
      "0.9311338657147586\n",
      "[[3825    0    5    0    8    2   49    1    5    8    4    6]\n",
      " [  55  516    0    3    0   20    0    0    0    0    0   15]\n",
      " [  34    0  657    0    0    0    0    0    0    0    0    0]\n",
      " [  64    6    0  551    0    0    0    0    0    0    6    3]\n",
      " [   0    0    0    0  616    0    0    0    0    0    0    0]\n",
      " [  39   45    0    5    0  467    0    0    0    0    1   11]\n",
      " [  13    0    0    0    0    0  611    0    0    0    0    0]\n",
      " [  39    0    0    0    0    0    0  598   17    0    0    0]\n",
      " [  33    0    0    2    0    0    0   45  561    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0  595    0    0]\n",
      " [  56    2    0   10    0    0    0    0    0    0  462   30]\n",
      " [  71    6    0    4    0    8    0    0    0    0    2  565]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.94      3913\n",
      "           1       0.90      0.85      0.87       609\n",
      "           2       0.99      0.95      0.97       691\n",
      "           3       0.96      0.87      0.91       630\n",
      "           4       0.99      1.00      0.99       616\n",
      "           5       0.94      0.82      0.88       568\n",
      "           6       0.93      0.98      0.95       624\n",
      "           7       0.93      0.91      0.92       654\n",
      "           8       0.96      0.88      0.92       641\n",
      "           9       0.99      1.00      0.99       595\n",
      "          10       0.97      0.82      0.89       560\n",
      "          11       0.90      0.86      0.88       656\n",
      "\n",
      "    accuracy                           0.93     10757\n",
      "   macro avg       0.95      0.91      0.93     10757\n",
      "weighted avg       0.93      0.93      0.93     10757\n",
      "\n",
      " fold 4 is Finished!\n",
      "0.928604629543553\n",
      "0.9277554414978825\n",
      "[[3839    0    5    0    1    1   50    2    4    4    2    3]\n",
      " [  60  508    0    4    0   31    0    0    0    0    0   26]\n",
      " [  28    0  693    0    0    0    0    0    0    0    0    0]\n",
      " [  66    2    0  535    0    0    0    0    0    0   10   10]\n",
      " [   2    0    0    0  587    0    0    0    0    0    0    0]\n",
      " [  43   55    0    0    0  558    0    0    0    0    0   23]\n",
      " [  13    0    0    0    0    0  574    0    0    0    0    0]\n",
      " [  36    0    0    0    0    0    0  573   12    0    0    0]\n",
      " [  46    1    0    0    0    0    0   46  516    0    0    0]\n",
      " [   5    1    0    0    0    0    0    0    0  602    0    0]\n",
      " [  62    5    0    2    0    0    0    0    0    0  430   36]\n",
      " [  57    2    0    4    0    6    0    0    0    0    2  574]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.94      3911\n",
      "           1       0.89      0.81      0.84       629\n",
      "           2       0.99      0.96      0.98       721\n",
      "           3       0.98      0.86      0.92       623\n",
      "           4       1.00      1.00      1.00       589\n",
      "           5       0.94      0.82      0.88       679\n",
      "           6       0.92      0.98      0.95       587\n",
      "           7       0.92      0.92      0.92       621\n",
      "           8       0.97      0.85      0.90       609\n",
      "           9       0.99      0.99      0.99       608\n",
      "          10       0.97      0.80      0.88       535\n",
      "          11       0.85      0.89      0.87       645\n",
      "\n",
      "    accuracy                           0.93     10757\n",
      "   macro avg       0.94      0.90      0.92     10757\n",
      "weighted avg       0.93      0.93      0.93     10757\n",
      "\n",
      " fold 5 is Finished!\n",
      " Finished!\n",
      "Total time elapsed: 2013.5451s\n",
      "\n",
      " The dataset shape is:(53785, 749, 1)\n",
      "\n",
      " The number of data samples (N) is:53785\n",
      "\n",
      " The number of TS length (T) is:749\n",
      "\n",
      " The number of TS dimention (M) is:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.924886120665613\n",
      "0.9246086934805884\n",
      "[[3631   17   33   17    7   23   18   13   23    6   20   34]\n",
      " [  42  574    0    8    0   25    0    5    2    0    6   13]\n",
      " [  24    0  642    0    0    0    0    0    0    0    0    0]\n",
      " [  46   14    0  535    1    1    0    2    0    0   11   12]\n",
      " [   4    0    0    0  597    0    0    1    0    1    0    0]\n",
      " [   7   32    0    7    1  540    0    0    3    3    2    9]\n",
      " [  14    0    0    0    1    0  641    0    0    1    0    0]\n",
      " [  23    0    0    0    0    0    0  610    6    0    1    0]\n",
      " [  30    0    0    4    0    0    0   18  550    2    2    6]\n",
      " [  22    0    0    1    0    1    1    2    0  605    1    0]\n",
      " [  51   14    0    4    0    1    0    2    2    0  458   25]\n",
      " [  46    6    0    3    0    3    0    3    3    1   15  566]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93      3842\n",
      "           1       0.87      0.85      0.86       675\n",
      "           2       0.95      0.96      0.96       666\n",
      "           3       0.92      0.86      0.89       622\n",
      "           4       0.98      0.99      0.99       603\n",
      "           5       0.91      0.89      0.90       604\n",
      "           6       0.97      0.98      0.97       657\n",
      "           7       0.93      0.95      0.94       640\n",
      "           8       0.93      0.90      0.92       612\n",
      "           9       0.98      0.96      0.97       633\n",
      "          10       0.89      0.82      0.85       557\n",
      "          11       0.85      0.88      0.86       646\n",
      "\n",
      "    accuracy                           0.92     10757\n",
      "   macro avg       0.93      0.92      0.92     10757\n",
      "weighted avg       0.92      0.92      0.92     10757\n",
      "\n",
      " fold 1 is Finished!\n"
     ]
    }
   ],
   "source": [
    "# change this directory for your machine\n",
    "root_dir = './'\n",
    "\n",
    "\n",
    "#add the classifier path to the sys\n",
    "sys.path.append(\"./classifiers/\")\n",
    "\n",
    "# define a list of datasets\n",
    "datasets = [\"PHM2022_Multivar\", \"PHM2022_Univar_PDIN\"]\n",
    "datasets_path = \"./datasets\"\n",
    "\n",
    "# define a list of algorithms\n",
    "#algorithms = [LR_module]\n",
    "algorirhms_path = \"./classifiers\"\n",
    "\n",
    "sys.path.append(\"./classifiers\")\n",
    "from classifiers import LR_module\n",
    "from classifiers import SVM_module\n",
    "from classifiers import RF_module\n",
    "\n",
    "# define the number of folds\n",
    "n_folds = 5\n",
    "\n",
    "# perform cross-validation for each dataset and algorithm combination\n",
    "for dataset in datasets:\n",
    "    Dataset_name = dataset + \"_Dataset\"\n",
    "    Dataset = np.load(datasets_path + \"/\" + Dataset_name + \".npy\")\n",
    "    \n",
    "\n",
    "    Labels_name = dataset + \"_Labels\"\n",
    "    Labels = np.load(datasets_path + \"/\"  + Labels_name + \".npy\")\n",
    "\n",
    "    # Create a folder for results\n",
    "    results_path = root_dir + \"Results/\" + Dataset_name\n",
    "    if os.path.exists(results_path):\n",
    "        pass\n",
    "    else:\n",
    "        try:\n",
    "            os.makedirs(results_path)\n",
    "        except:\n",
    "            # in case another machine created the path meanwhile !:(\n",
    "            pass\n",
    "\n",
    "\n",
    "    #Run The Logistic Regression (LR) module\n",
    "    LR_module.LR(results_path, Dataset_name, Dataset, Labels, nb_folds= n_folds, C=10)\n",
    "\n",
    "    #Run The Support Vector Machine (SVM) Module\n",
    "    SVM_module.SVM(results_path, Dataset_name, Dataset, Labels, nb_folds= n_folds, C=1)\n",
    "\n",
    "    #Run The Random Forest (RF) Module\n",
    "    RF_module.RF(results_path, Dataset_name, Dataset, Labels, nb_folds= n_folds, n_trees=500)\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
