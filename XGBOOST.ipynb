{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.7.5-py3-none-macosx_10_15_x86_64.macosx_11_0_x86_64.macosx_12_0_x86_64.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from xgboost) (1.22.4)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from xgboost) (1.8.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.7.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import xgboost as xgb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 60, 1)\n",
      "(53785, 749, 3)\n",
      "(53785, 749, 1)\n"
     ]
    }
   ],
   "source": [
    "# define a list of datasets\n",
    "datasets = [\"Control_charts\", \"PHM2022_Multivar\", \"PHM2022_Univar_PDIN\"]\n",
    "datasets_path = \"../datasets\"\n",
    "\n",
    "for dataset in datasets:\n",
    "    Dataset_name = dataset + \"_Dataset\"\n",
    "    Dataset = np.load(datasets_path + \"/\" + Dataset_name + \".npy\")\n",
    "    print(Dataset.shape)\n",
    "    \n",
    "\n",
    "    Labels_name = dataset + \"_Labels\"\n",
    "    Labels = np.load(datasets_path + \"/\"  + Labels_name + \".npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The dataset shape is:(600, 60, 1)\n",
      "\n",
      " The number of data samples (N) is:600\n",
      "\n",
      " The number of TS length (T) is:60\n",
      "\n",
      " The number of TS dimention (M) is:1\n",
      "\n",
      " The classifier is fitted\n",
      "0.8666666666666667\n",
      "0.8677421412387426\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.82      0.80        17\n",
      "           1       0.95      1.00      0.98        20\n",
      "           2       0.95      0.95      0.95        20\n",
      "           3       0.68      0.81      0.74        16\n",
      "           4       0.86      0.86      0.86        14\n",
      "           5       0.93      0.79      0.85        33\n",
      "\n",
      "    accuracy                           0.87       120\n",
      "   macro avg       0.86      0.87      0.86       120\n",
      "weighted avg       0.87      0.87      0.87       120\n",
      "\n",
      " fold 1 is Finished!\n",
      "\n",
      " The classifier is fitted\n",
      "0.9\n",
      "0.9005594449530376\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.80      0.87        25\n",
      "           1       0.85      1.00      0.92        17\n",
      "           2       1.00      0.90      0.95        20\n",
      "           3       0.95      0.86      0.90        22\n",
      "           4       0.90      0.95      0.92        19\n",
      "           5       0.76      0.94      0.84        17\n",
      "\n",
      "    accuracy                           0.90       120\n",
      "   macro avg       0.90      0.91      0.90       120\n",
      "weighted avg       0.91      0.90      0.90       120\n",
      "\n",
      " fold 2 is Finished!\n",
      "\n",
      " The classifier is fitted\n",
      "0.8916666666666667\n",
      "0.8910756111843069\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88        25\n",
      "           1       1.00      1.00      1.00        21\n",
      "           2       0.76      0.81      0.79        16\n",
      "           3       1.00      0.94      0.97        17\n",
      "           4       0.82      0.75      0.78        24\n",
      "           5       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.89       120\n",
      "   macro avg       0.89      0.90      0.89       120\n",
      "weighted avg       0.89      0.89      0.89       120\n",
      "\n",
      " fold 3 is Finished!\n",
      "\n",
      " The classifier is fitted\n",
      "0.9333333333333333\n",
      "0.9342926588087879\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.93      0.84        14\n",
      "           1       0.96      0.96      0.96        23\n",
      "           2       1.00      0.85      0.92        20\n",
      "           3       0.96      1.00      0.98        27\n",
      "           4       0.89      0.89      0.89        18\n",
      "           5       1.00      0.94      0.97        18\n",
      "\n",
      "    accuracy                           0.93       120\n",
      "   macro avg       0.93      0.93      0.93       120\n",
      "weighted avg       0.94      0.93      0.93       120\n",
      "\n",
      " fold 4 is Finished!\n",
      "\n",
      " The classifier is fitted\n",
      "0.925\n",
      "0.9245553158285355\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.84      0.89        19\n",
      "           1       0.95      1.00      0.97        19\n",
      "           2       0.85      0.96      0.90        24\n",
      "           3       0.90      1.00      0.95        18\n",
      "           4       0.96      0.88      0.92        25\n",
      "           5       1.00      0.87      0.93        15\n",
      "\n",
      "    accuracy                           0.93       120\n",
      "   macro avg       0.93      0.92      0.93       120\n",
      "weighted avg       0.93      0.93      0.92       120\n",
      "\n",
      " fold 5 is Finished!\n",
      " Finished!\n",
      "Total time elapsed: 3.4533s\n",
      "\n",
      " The dataset shape is:(53785, 749, 3)\n",
      "\n",
      " The number of data samples (N) is:53785\n",
      "\n",
      " The number of TS length (T) is:749\n",
      "\n",
      " The number of TS dimention (M) is:3\n",
      "XGBoost is not capable of doing classification on MTS. so it will be done on only the first dimension\n"
     ]
    }
   ],
   "source": [
    "# change this directory for your machine\n",
    "root_dir = './'\n",
    "\n",
    "\n",
    "# define a list of algorithms\n",
    "algorirhms_path = \"./classifiers\"\n",
    "\n",
    "from classifiers import xgboost_module\n",
    "\n",
    "\n",
    "# define the number of folds\n",
    "n_folds = 5\n",
    "\n",
    "# perform cross-validation for each dataset and algorithm combination\n",
    "for dataset in datasets:\n",
    "    Dataset_name = dataset + \"_Dataset\"\n",
    "    Dataset = np.load(datasets_path + \"/\" + Dataset_name + \".npy\")\n",
    "    \n",
    "\n",
    "    Labels_name = dataset + \"_Labels\"\n",
    "    Labels = np.load(datasets_path + \"/\"  + Labels_name + \".npy\")\n",
    "\n",
    "    # Create a folder for results\n",
    "    results_path = root_dir + \"Results/\" + Dataset_name\n",
    "    if os.path.exists(results_path):\n",
    "        pass\n",
    "    else:\n",
    "        try:\n",
    "            os.makedirs(results_path)\n",
    "        except:\n",
    "            # in case another machine created the path meanwhile !:(\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "    #Run The XGboost Module\n",
    "    xgboost_module.XGBOOST(results_path, Dataset_name, Dataset, Labels, nb_folds=n_folds,\n",
    "                        learning_rate =0.3,\n",
    "                        n_estimators=100,\n",
    "                        max_depth=6,\n",
    "                        min_child_weight=1,\n",
    "                        gamma=0,\n",
    "                        subsample=1,\n",
    "                        colsample_bytree=1,\n",
    "                        objective='multi:softmax',\n",
    "                        n_jobs = 10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
