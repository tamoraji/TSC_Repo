{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559f4dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tsfresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eaebf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tsfresh import extract_features\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9167e0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5400, 1000, 3)\n",
      "(5400,)\n"
     ]
    }
   ],
   "source": [
    "datasets = [\n",
    "\"CWRU_12k_FE_multivar\"\n",
    "]\n",
    "\n",
    "datasets_path = \"../datasets\"\n",
    "\n",
    "for dataset in datasets:\n",
    "    Dataset_name = dataset + \"_Dataset\"\n",
    "    Dataset = np.load(datasets_path + \"/\" + Dataset_name + \".npy\")\n",
    "    Dataset = Dataset\n",
    "    print(Dataset.shape)\n",
    "    \n",
    "    Labels_name = dataset + \"_Labels\"\n",
    "    Labels = np.load(datasets_path + \"/\"  + Labels_name + \".npy\")\n",
    "    Labels = Labels.squeeze()\n",
    "    print(Labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9763a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n The dataset shape is:{Dataset.shape}\")\n",
    "N=Dataset.shape[0]\n",
    "print(f\"\\n The number of data samples (N) is:{N}\")\n",
    "\n",
    "T=Dataset.shape[1]\n",
    "print(f\"\\n The number of TS length (T) is:{T}\")\n",
    "\n",
    "M=Dataset.shape[2]\n",
    "print(f\"\\n The number of TS dimention (M) is:{M}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57959e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(Dataset.reshape(N*T, M), columns=[f'feature_{i}' for i in range(M)])\n",
    "df['id'] = np.repeat(np.arange(N), T)\n",
    "df['time'] = np.tile(np.arange(T), N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccada06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe39a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = (df['id'] == 0).sum()\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d6c719",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df['id'] == 0]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8347f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tsfresh.feature_extraction import EfficientFCParameters\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "\n",
    "\n",
    "# Compute features using the tsfresh extract_features function\n",
    "X_features = extract_features(df, column_id='id', column_sort='time',\n",
    "                     default_fc_parameters=EfficientFCParameters(),\n",
    "                     # we impute = remove all NaN features automatically\n",
    "                     impute_function=impute,\n",
    "                     n_jobs = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a015a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034e9200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the DataFrame to a pickle file\n",
    "X_features.to_pickle('FBL_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec72c64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = pd.read_pickle('FBL_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96d3baac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, Labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6c85820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty set of selected features and a list of candidate features\n",
    "selected_features = set()\n",
    "candidate_features = list(X_features.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3389c190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93feb0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2330\n",
      "Selected features: {'feature_1__partial_autocorrelation__lag_5'}\n",
      "Classification rate: 0.46190476190476193\n",
      "2329\n",
      "Selected features: {'feature_1__ar_coefficient__coeff_1__k_10', 'feature_1__partial_autocorrelation__lag_5'}\n",
      "Classification rate: 0.7277777777777777\n",
      "2328\n",
      "Selected features: {'feature_1__ar_coefficient__coeff_1__k_10', 'feature_2__ar_coefficient__coeff_3__k_10', 'feature_1__partial_autocorrelation__lag_5'}\n",
      "Classification rate: 0.908994708994709\n",
      "2327\n",
      "Selected features: {'feature_1__ar_coefficient__coeff_1__k_10', 'feature_2__ar_coefficient__coeff_8__k_10', 'feature_2__ar_coefficient__coeff_3__k_10', 'feature_1__partial_autocorrelation__lag_5'}\n",
      "Classification rate: 0.9708994708994709\n",
      "2326\n",
      "Selected features: {'feature_1__ar_coefficient__coeff_1__k_10', 'feature_2__ar_coefficient__coeff_8__k_10', 'feature_2__ar_coefficient__coeff_3__k_10', 'feature_1__partial_autocorrelation__lag_5', 'feature_0__percentage_of_reoccurring_datapoints_to_all_datapoints'}\n",
      "Classification rate: 0.9947089947089947\n",
      "2325\n",
      "Selected features: {'feature_1__ar_coefficient__coeff_1__k_10', 'feature_2__ar_coefficient__coeff_8__k_10', 'feature_2__ar_coefficient__coeff_3__k_10', 'feature_1__autocorrelation__lag_9', 'feature_1__partial_autocorrelation__lag_5', 'feature_0__percentage_of_reoccurring_datapoints_to_all_datapoints'}\n",
      "Classification rate: 0.9981481481481481\n"
     ]
    }
   ],
   "source": [
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "best_score = 0\n",
    "# Perform greedy forward feature selection\n",
    "while candidate_features:\n",
    "\n",
    "    # Evaluate each candidate feature by adding it to the set of selected features\n",
    "    # and training a logistic regression model on the resulting feature set\n",
    "    candidate_scores = []\n",
    "    for feature in candidate_features:\n",
    "        feature_set = selected_features.union({feature})\n",
    "        X_train_selected = X_train[list(feature_set)]\n",
    "        clf.fit(X_train_selected, y_train)\n",
    "        score = clf.score(X_train_selected, y_train)\n",
    "        candidate_scores.append((feature, score))\n",
    "    \n",
    "    # Select the candidate feature that results in the highest classification rate\n",
    "    current_feature, current_score = max(candidate_scores, key=lambda x: x[1])\n",
    "    \n",
    "    # Check if the improvement in the training set classification rate upon adding\n",
    "    # the current best feature is less than 3%, and terminate the loop if it is\n",
    "    if best_score is not None and (current_score - best_score) < 0.0003:\n",
    "        break\n",
    "    \n",
    "    # Set the best feature and score to the current feature and score if they are\n",
    "    # None, or if the current score is better than the best score\n",
    "    if best_score is None or current_score > best_score:\n",
    "        best_feature, best_score = current_feature, current_score\n",
    "    \n",
    "    # Add the best feature to the set of selected features\n",
    "    selected_features.add(best_feature)\n",
    "    \n",
    "    # Remove the best feature from the list of candidate features\n",
    "    candidate_features.remove(best_feature)\n",
    "    print(len(candidate_features))\n",
    "\n",
    "    \n",
    "    # Print the selected features and their classification rate on the training data\n",
    "    print(f\"Selected features: {selected_features}\")\n",
    "    print(f\"Classification rate: {best_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb8305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Initialize the best score to 0 and the current score to the score of the initial feature set\n",
    "best_score = 0\n",
    "current_score = 1\n",
    "\n",
    "# Perform greedy forward feature selection\n",
    "while candidate_features and (best_score < 1) and (current_score - best_score >= 0.03):\n",
    "\n",
    "    # Evaluate each candidate feature by adding it to the set of selected features\n",
    "    # and training a logistic regression model on the resulting feature set\n",
    "    candidate_scores = []\n",
    "    for feature in candidate_features:\n",
    "        feature_set = selected_features.union({feature})\n",
    "        X_train_selected = X_train[list(feature_set)]\n",
    "        score = clf.fit(X_train_selected, y_train).score(X_train_selected, y_train)\n",
    "        candidate_scores.append((feature, score))\n",
    "    \n",
    "    # Select the candidate feature that results in the highest classification rate\n",
    "    best_feature, best_score = max(candidate_scores, key=lambda x: x[1])\n",
    "    \n",
    "    # Add the best feature to the set of selected features\n",
    "    selected_features.add(best_feature)\n",
    "    \n",
    "    # Remove the best feature from the list of candidate features\n",
    "    candidate_features.remove(best_feature)\n",
    "    \n",
    "    # Update the current score to the score of the current feature set\n",
    "    current_score = clf.fit(X_train[list(selected_features)], y_train).score(X_train[list(selected_features)], y_train)\n",
    "    \n",
    "    # Print the selected features and their classification rate on the training data\n",
    "    print(f\"Selected features: {selected_features}\")\n",
    "    print(f\"Classification rate: {best_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef250b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7829fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Perform greedy forward feature selection\n",
    "while candidate_features:\n",
    "\n",
    "    # Evaluate each candidate feature by adding it to the set of selected features\n",
    "    # and training a logistic regression model on the resulting feature set\n",
    "    candidate_scores = []\n",
    "    for feature in candidate_features:\n",
    "        feature_set = selected_features.union({feature})\n",
    "        X_train_selected = X_train[list(feature_set)]\n",
    "        clf.fit(X_train_selected, y_train)\n",
    "        score = clf.score(X_train_selected, y_train)\n",
    "        candidate_scores.append((feature, score))\n",
    "    \n",
    "    # Select the candidate feature that results in the highest classification rate\n",
    "    best_feature, best_score = max(candidate_scores, key=lambda x: x[1])\n",
    "    \n",
    "    # Add the best feature to the set of selected features\n",
    "    selected_features.add(best_feature)\n",
    "    \n",
    "    # Remove the best feature from the list of candidate features\n",
    "    candidate_features.remove(best_feature)\n",
    "    print(len(candidate_features))\n",
    "    \n",
    "    # Print the selected features and their classification rate on the training data\n",
    "    print(f\"Selected features: {selected_features}\")\n",
    "    print(f\"Classification rate: {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0bf357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9856f57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f748160",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0165406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
